[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "End To End Data Science With R",
    "section": "",
    "text": "Preface\nWelcome to the first edition of “End to End Data Science with R”!\nAfter reading this book, you’ll have the tools to tackle a wide variety of data science challenges using the following skills:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "End To End Data Science With R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book isn’t just the product of the listed authors but is the result of many conversations (in person and online) that we’ve had with many people in the R community. We’re incredibly grateful for all the conversations we’ve had with y’all; Thank You So Much!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "The Data Science Universe",
    "section": "",
    "text": "How this book is organized\nBelow is a list of items we are going to cover in this book:",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "intro.html#how-this-book-is-organized",
    "href": "intro.html#how-this-book-is-organized",
    "title": "The Data Science Universe",
    "section": "",
    "text": "Introduction to Data Science and R\nData Exploration and Visualization\nSupervised Learning\nUnsupervised Learning\nBoosting and Random Forest\nNatural Language Processing\nImage Processing/Computer Vision\nReinforcement Learning\nBig Data & Cloud Computing",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "intro.html#why-learning-data-science",
    "href": "intro.html#why-learning-data-science",
    "title": "The Data Science Universe",
    "section": "Why Learning data science?",
    "text": "Why Learning data science?\nAs more and more businesses move towards digitalization, there is a growing demand for professionals who can analyze and make sense of the vast amounts of data that are being generated. This has created a significant shortage of skilled Data Scientists, making it a highly sought-after and well-compensated profession.\nAnother point is that, Data Science is a highly interdisciplinary field that combines knowledge and techniques from statistics, computer science, and domain-specific areas. This means that learning Data Science can enhance your critical thinking skills, improve your ability to solve complex problems, and provide you with a unique set of skills that are highly valued in the job market.",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "intro.html#the-role-of-r",
    "href": "intro.html#the-role-of-r",
    "title": "The Data Science Universe",
    "section": "The Role of R",
    "text": "The Role of R\nR is a programming language that is widely used in the field of Data Science. Its role in Data Science is multifaceted and can be summarized as follows:\n\nData Wrangling: R has a powerful set of libraries that allow you to manipulate and transform data, which is a critical step in any Data Science project.\nStatistical Analysis: R has a rich set of statistical libraries that allow you to perform a wide range of statistical analyses, including hypothesis testing, regression analysis, and time series analysis.\nData Visualization: R has an extensive set of libraries for creating high-quality data visualizations, such as plots, charts, and graphs, that enable you to communicate insights effectively.\nMachine Learning: R has a comprehensive set of libraries for building and deploying machine learning models, such as decision trees, random forests, and neural networks.\nReproducibility: R provides a framework for creating reproducible data analyses, which is essential for collaborating with others and ensuring that your work can be verified and replicated.\n\nOverall, R plays a critical role in the Data Science process by providing a powerful and flexible toolset for manipulating, analyzing, and visualizing data, building and deploying machine learning models, and ensuring reproducibility.",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Data Science and R",
    "section": "",
    "text": "In this first part of the book, our focus will be on the the three following chapters:\n\nIn 1  Understanding Data Science As a Career we start with an introduction to the world of Data Science and its importance in the our current world.\nIn 2  Understanding R Applications in Data Science we unpack the role of R in the field of Data Science.\nIn 3  R Objects And Variables we discuss some important data concepts that are frequently used in this course.",
    "crumbs": [
      "Data Science and R"
    ]
  },
  {
    "objectID": "understanding-ds.html",
    "href": "understanding-ds.html",
    "title": "1  Understanding Data Science As a Career",
    "section": "",
    "text": "1.1 State of Data Science\nData Science is an interdisciplinary field that involves using scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It involves collecting and processing large amounts of data, analyzing it to identify patterns and trends, and using those insights to make informed business decisions. A Data Scientist is a professional who has expertise in data analysis, machine learning, statistics, programming, and domain knowledge. They use various statistical and computational techniques to analyse large, complex datasets and derive valuable insights that help organisations make informed decisions. It can be used to optimize operations, improve customer experiences, and develop new products and services. By leveraging Data Science techniques, businesses can gain a competitive edge by making more informed decisions and quickly adapting to changing market conditions.\nData Science involves several stages, including data collection, data cleaning, data preprocessing, exploratory data analysis, modelling, and evaluation. Data Scientists work with various tools and technologies such as Python, R, SQL, Hadoop, Spark, and Tableau, to name a few.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding Data Science As a Career</span>"
    ]
  },
  {
    "objectID": "understanding-ds.html#difference-among-popular-data-careers",
    "href": "understanding-ds.html#difference-among-popular-data-careers",
    "title": "1  Understanding Data Science As a Career",
    "section": "\n1.2 Difference among popular data careers?",
    "text": "1.2 Difference among popular data careers?\nWhile Data Science is a broad field, there are several roles that one can pursue within it. Here are some of the key differences between Data Engineer, Data Analyst, BI Analyst, and Statistician:\n\n\n\n\n\n\n\n\nCareer\nJob Responsibilities\nRequired Skills\nTools/Technologies\n\n\n\nData Scientist\nDevelop and apply statistical or machine learning models to solve business problems; collect, clean, and analyze large datasets; communicate insights to stakeholders\nStrong analytical and problem-solving skills; knowledge of statistics, machine learning, and programming languages (Python, R, SQL); familiarity with data visualization techniques\nPython, R, SQL, Hadoop, Spark, Tableau, SAS\n\n\nData Engineer\nBuild and maintain data pipelines and infrastructure to support data analysis and machine learning; work with large datasets and distributed systems\nStrong programming skills; expertise in databases, data warehousing, and ETL (extract, transform, load) processes; knowledge of cloud computing\nHadoop, Spark, SQL, NoSQL databases, AWS, Azure\n\n\nData Analyst\nCollect and analyze data to identify trends, patterns, and insights; communicate findings to stakeholders\nStrong analytical and problem-solving skills; knowledge of statistics and data visualization techniques; proficiency in Excel\nExcel, SQL, Tableau, Power BI\n\n\nBI Analyst\nDesign and develop business intelligence solutions to support decision-making processes; gather and analyze data from multiple sources; create dashboards and reports\nStrong analytical and problem-solving skills; expertise in databases and data modeling; knowledge of data visualization techniques\nTableau, Power BI, SQL, Excel\n\n\nStatistician\nDesign and conduct experiments to collect and analyze data; develop statistical models to explain and predict phenomena; communicate findings to stakeholders\nStrong analytical and problem-solving skills; expertise in statistical theory and methods; knowledge of programming languages (Python, R, SAS)\nPython, R, SAS, SPSS\n\n\n\nWhile there is some overlap between these roles, each one requires a specific set of skills and expertise. Data Science as a field offers a wide range of career opportunities, and individuals can choose a role that aligns with their interests and strengths.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding Data Science As a Career</span>"
    ]
  },
  {
    "objectID": "understanding-ds.html#why-learning-data-science",
    "href": "understanding-ds.html#why-learning-data-science",
    "title": "1  Understanding Data Science As a Career",
    "section": "\n1.3 Why Learning data science?",
    "text": "1.3 Why Learning data science?\nAs more and more businesses move towards digitalization, there is a growing demand for professionals who can analyze and make sense of the vast amounts of data that are being generated. This has created a significant shortage of skilled Data Scientists, making it a highly sought-after and well-compensated profession.\nAnother point is that,Data Science is a highly interdisciplinary field that combines knowledge and techniques from statistics, computer science, and domain-specific areas. This means that learning Data Science can enhance your critical thinking skills, improve your ability to solve complex problems, and provide you with a unique set of skills that are highly valued in the job market.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding Data Science As a Career</span>"
    ]
  },
  {
    "objectID": "understanding-r-applications.html",
    "href": "understanding-r-applications.html",
    "title": "2  Understanding R Applications in Data Science",
    "section": "",
    "text": "2.1 The Role of R\nR is a programming language that is widely used in the field of Data Science. Its role in Data Science is multifaceted and can be summarized as follows:",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding R Applications in Data Science</span>"
    ]
  },
  {
    "objectID": "understanding-r-applications.html#the-role-of-r",
    "href": "understanding-r-applications.html#the-role-of-r",
    "title": "2  Understanding R Applications in Data Science",
    "section": "",
    "text": "Data Wrangling: R has a powerful set of libraries that allow you to manipulate and transform data, which is a critical step in any Data Science project.\nStatistical Analysis: R has a rich set of statistical libraries that allow you to perform a wide range of statistical analyses, including hypothesis testing, regression analysis, and time series analysis.\nData Visualization: R has an extensive set of libraries for creating high-quality data visualizations, such as plots, charts, and graphs, that enable you to communicate insights effectively.\nMachine Learning: R has a comprehensive set of libraries for building and deploying machine learning models, such as decision trees, random forests, and neural networks.\nReproducibility: R provides a framework for creating reproducible data analyses, which is essential for collaborating with others and ensuring that your work can be verified and replicated.\n\n\n2.1.1 R v.s. Python\n\n\n\n\n\n\n\nFigure 2.1: R vs. Python\n\n\n\n\nYou may wondering why we choose R over another popular language - Python, in this course. The short answer is the choice ultimately depends on the specific needs of the data scientist and the project at hand.\nAsk yourself: what kind of data scientist you want to become? R is hands down the best option when you focus on statistics and probabilities. It has a large community of statisticians that can answer your questions. But, if you want to develop applications that process enormous amounts of data, Python is your best option. It has a more extensive ecosystem of developers, and it’s easier to find people willing to collaborate with you.\nTechnical Differences 1. Syntax: R has a syntax that is tailored for statistical analysis and modelling, with many built-in functions and operators specifically designed for this purpose. Python, on the other hand, has a more general-purpose syntax that can be used for a wide range of tasks beyond statistical analysis.\n\nLibraries and Packages: Both R and Python have extensive libraries and packages for data science, but they differ in their focus and scope. R has a strong emphasis on statistical modelling and analysis, with packages like ggplot2, dplyr, and tidyr. Python, on the other hand, has a broader range of applications, including web development, scientific computing, and machine learning, with packages like NumPy, Pandas, and Scikit-learn.\nCommunity: Both R and Python have large and active communities, but they differ in their backgrounds and focus. R has historically been used more by statisticians and data analysts, while Python has been more popular among software engineers and developers.\nLearning Curve: R is generally considered to have a steeper learning curve than Python, especially for those who are new to programming. However, once you become familiar with R’s syntax and packages, it can be a very powerful and efficient tool for statistical analysis.\nVisualisation: R has a strong focus on Visualisation, with packages like ggplot2 and lattice that make it easy to create high-quality plots and charts. Python also has Visualisation packages like Matplotlib and Seaborn, but they may require more customisation and tweaking to get the desired output.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding R Applications in Data Science</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html",
    "href": "r-objects-and-variables.html",
    "title": "3  R Objects And Variables",
    "section": "",
    "text": "3.1 Basic R Objects\nIn this section, we will explore the fundamental building blocks of R programming, starting with the basic R objects. These objects serve as the foundation for data manipulation and analysis in R. We will delve into five key types of R objects: vectors, matrices, lists, data frames, and functions. Understanding these essential data structures is crucial for anyone looking to harness the power of R for data science, statistics, and programming tasks.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html#basic-r-objects",
    "href": "r-objects-and-variables.html#basic-r-objects",
    "title": "3  R Objects And Variables",
    "section": "",
    "text": "3.1.1 Vector\nSequence of data elements of the same type. Each element of the vector is also called a component, member, or value. Vectors are created in R using the c() function, which stands for combine, and coerces all of its arguments into a single type. The coercion will happen from simpler types into more complex types. That is, if we create a vector which contains logicals, numerics, and characters, as the following example shows, our resulting vector will only contain characters, which are the more complex of the three types. If we create a vector that contains logicals and numerics, our resulting vector will be numeric, again because it’s the more complex type.\n\nmy_vector &lt;- c(TRUE, FALSE, -1, 0, 1, \"A\", \"B\", NA, NULL, NaN, Inf)\nmy_vector\n#&gt;  [1] \"TRUE\"  \"FALSE\" \"-1\"    \"0\"     \"1\"     \"A\"     \"B\"     NA      \"NaN\"  \n#&gt; [10] \"Inf\"\n\n## Find the first element of `my_vector`\npaste(\"the first element of `my_vector` is:\", my_vector[1])\n#&gt; [1] \"the first element of `my_vector` is: TRUE\"\n\n## Find the 5th element of `my_vector`\npaste(\"the 5th element of `my_vector` is:\", my_vector[5])\n#&gt; [1] \"the 5th element of `my_vector` is: 1\"\n\n## Find the firt 3 elements\npaste(\"the firt 3 elements of `my_vector` are:\", my_vector[1:3])\n#&gt; [1] \"the firt 3 elements of `my_vector` are: TRUE\" \n#&gt; [2] \"the firt 3 elements of `my_vector` are: FALSE\"\n#&gt; [3] \"the firt 3 elements of `my_vector` are: -1\"\n\n\n3.1.2 Matrix\nMatrices are commonly used in mathematics and statistics, and much of R’s power comes from the various operations you can perform with them. In R, a matrix is a vector with two additional attributes, the number of rows and the number of columns. And, since matrices are vectors, they are constrained to a single data type.\nYou can use the matrix function to create matrices. You may pass it a vector of values, as well as the number of rows and columns the matrix should have. If you specify the vector of values and one of the dimensions, the other one will be calculated for you automatically to be the lowest number that makes sense for the vector you passed. However, you may specify both of them simultaneously if you prefer, which may produce different behavior depending on the vector you passed, as can be seen in the next example.\nBy default, matrices are constructed column-wise, meaning that the entries can be thought of as starting in the upper-left corner and running down the columns. However, if you prefer to construct it row-wise, you can send the byrow = TRUE parameter.\n\n# Creating a matrix\nmy_mat &lt;- matrix(1:6, nrow = 2, byrow = TRUE)\nmy_mat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    2    3\n#&gt; [2,]    4    5    6\n# Find the element in row 1 and column 2\nmy_mat[1, 2]\n#&gt; [1] 2\n# Find the elements in row 1 & 2 and column 3\nmy_mat[c(1, 2), 3]\n#&gt; [1] 3 6\n# Find all the elements in column 2\nmy_mat[, 2]\n#&gt; [1] 2 5\n\n\n3.1.3 List\nA list is an ordered collection of objects, like vectors, but lists can actually combine objects of different types. List elements can contain any type of object that exists in R, including data frames and functions. Lists play a central role in R due to their flexibility and they are the basis for data frames, object-oriented programming, and other constructs.\nUsing the function list() helps to explicitly a list. It takes an arbitrary number of arguments, and we can refer to each of those elements by both position, and, in case they are specified, also by names. If you want to reference list elements by names, you can use the $ notation.\n\n# Creating a list\nmy_list &lt;- list(A = 1, B = \"A\", C = TRUE, D = matrix(1:4, nrow = 2), \nZ = function(x) x^2)\n\n# Retrieve the class of each element in the list\nlapply(my_list, class)\n#&gt; $A\n#&gt; [1] \"numeric\"\n#&gt; \n#&gt; $B\n#&gt; [1] \"character\"\n#&gt; \n#&gt; $C\n#&gt; [1] \"logical\"\n#&gt; \n#&gt; $D\n#&gt; [1] \"matrix\" \"array\" \n#&gt; \n#&gt; $Z\n#&gt; [1] \"function\"\n\n# Perform calculation\nmy_list$Z(2)\n#&gt; [1] 4\n\n\n3.1.4 DataFrame\nA data frame is a natural way to represent such heterogeneous tabular data. Every element within a column must be of the same type, but different elements within a row may be of different types, that’s why we say that a data frame is a heterogeneous data structure.\nData frames are usually created by reading in a data using the read.table(), read.csv, or other similar data-loading functions. However, they can also be created explicitly with the data.frame function or they can be coerced from other types of objects such as lists. To create a data frame using the data.frame function, note that we send a vector (which, as we know, must contain elements of a single type) to each of the column names we want our data frame to have, which are A, B, and C in this case.\n\n# Creating a dataframe\nmy_dataframe &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  height = c(1.6, 1.8, 1.7)\n)\n        \n# Accessing a column of the dataframe\nmy_dataframe$name\n#&gt; [1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n\n3.1.5 Function\nA function is an object that takes other objects as inputs, called arguments, and returns an output object. Most functions are in the following form f(arg_1, arg_2, ...), where f is the name of the function and arg_1, arg_2 are the arguments to the function.\nWe can create our own function by using the function constructor and assign it to a symbol. It takes an arbitrary number of named arguments, which can be used within the body of the function.\nIn the following example, we create a function that calculates the Euclidian distance (https://en.wikipedia.org/wiki/Euclidean_distance) between two numeric vectors, and we show how the order of the arguments can be changed if we use named arguments. To realize this effect, we use the print function to make sure we can see in the console what R is receiving as the x and y vectors. When developing your own programs, using the print for debugging your function.\n\n# Creating l2_norm function\nl2_norm &lt;- function(x, y) {\n  print(\"value of x:\")\n  print(x)\n  print(\"value of y:\")\n  print(y)\n  num_diff &lt;- x - y\n  res &lt;- sum(num_diff^2)\n  return(res)\n}\n\na &lt;- 1:3\nb &lt;- 4:6\n\nl2_norm(a, b)\n#&gt; [1] \"value of x:\"\n#&gt; [1] 1 2 3\n#&gt; [1] \"value of y:\"\n#&gt; [1] 4 5 6\n#&gt; [1] 27",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html#type-of-data",
    "href": "r-objects-and-variables.html#type-of-data",
    "title": "3  R Objects And Variables",
    "section": "\n3.2 Type of Data",
    "text": "3.2 Type of Data\nA variable is a characteristic of the population (or sample) being studied, and it is possible to measure, count, and categorize it. The type of variable collected is crucial in the calculation of descriptive statistics and the graphical representation of results as well as in the selection of the statistical methods that will be used to analyze the data.\n\n3.2.1 Continuous Data\nIt refers to a type of numerical data that can take on any value within a specific range or interval. This type of data is measured on a continuous scale, meaning that there are no gaps or interruptions between values. Continuous data is often obtained through measurements or observations that are recorded as real numbers, such as weight, height, time, temperature, and distance.\n\n# Creating a vector of continuous data\nmy_data &lt;- c(1.2, 2.5, 3.1, 4.8, 5.0)\n\n# Calculating mean and standard deviation\nmean(my_data)\n#&gt; [1] 3.32\nsd(my_data)\n#&gt; [1] 1.599062\n\n\n3.2.2 Discrete Data\nUnlike Continuous Data, discrete data is numeric data that which can only take on certain values within a specific range. For example, the number of kids (or trees, or animals) has to be a whole integer.\nSuppose we have a dataset of the number of students in a class, where each value represents a count of a specific number of students:\n\nstudents &lt;- c(20, 25, 22, 18, 20, 23, 21, 19, 22, 20)\n\n# Calculating the frequency of each value\ntable(students)\n#&gt; students\n#&gt; 18 19 20 21 22 23 25 \n#&gt;  1  1  3  1  2  1  1\n# Calculate proportions\nprop.table(students)\n#&gt;  [1] 0.09523810 0.11904762 0.10476190 0.08571429 0.09523810 0.10952381\n#&gt;  [7] 0.10000000 0.09047619 0.10476190 0.09523810\n\n\n3.2.3 Categorical Data\nCategorical data, also known as nominal data, is a type of data that consists of categories or groups that cannot be ordered or ranked. In R, categorical data is typically represented as a factor variable.\nThe factor() function is used to convert the vector to a factor variable. The levels() function is used to view the categories or levels of the factor variable.\n\n# create a vector of categorical data\ngender &lt;- c(\"male\", \"female\", \"male\", \"male\", \"female\", \"female\")\n\n# convert the vector to a factor\ngender_factor &lt;- factor(gender)\n\n# view the levels of the factor\nlevels(gender_factor)\n#&gt; [1] \"female\" \"male\"\n\n\n3.2.4 Binary Data\nBinary data is categorical data where the only values are 0 and 1. It is often used in situations where a “hit” - an animal getting trapped, a customer clicking a link, etc. - is a 1, and no hit is a 0. In R, binary data can be represented using logical vectors.\nThe class() function is used to confirm that the vector is of logical class, which is the R data type used to represent binary data.\n\n# Create a vector of binary data\nbinary_data &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\n# Check the class of the vector\nclass(binary_data)\n#&gt; [1] \"logical\"\n# Output: \"logical\"\n\n\n3.2.5 Ordinal Data\nOrdinal data is a type of categorical data where each value is assigned a level or rank. It is useful with binned data, but also in graphing to rearrange the order categories are drawn. In R, it is referred to as factors.\n\n# Creating an ordered factor\nmy_factor &lt;- factor(c(\"small\", \"medium\", \"large\"), ordered = TRUE)\n\n# Sorting the levels of the factor\nmy_factor &lt;- factor(my_factor, levels = c(\"small\", \"medium\", \"large\"))\n\nprint(my_factor)\n#&gt; [1] small  medium large \n#&gt; Levels: small &lt; medium &lt; large",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html#data-distribution",
    "href": "r-objects-and-variables.html#data-distribution",
    "title": "3  R Objects And Variables",
    "section": "\n3.3 Data Distribution",
    "text": "3.3 Data Distribution\nData distribution in R refers to the pattern in which the values of a variable are spread across the range of the variable. In other words, it refers to how often every possible value occurs in a dataset. It is usually shown as a curved line on a graph, or a histogram.\n\n3.3.1 Normal Distribution\nNormal distribution is data where the mean equals the median, 2/3 of the data are within one standard deviation of the mean, 95% of the data are within two standard deviations, and 97% are within three. Many statistical analyses assume your data are normally distributed. However, many datasets - especially in nature - aren’t.\n\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n# Creating a dataset with normal distribution\nmy_val &lt;- rnorm(100000, mean = 0, sd = 1)\nvec &lt;- c(1:100000)\nmy_data &lt;- data.frame(vec, my_val)\n# Plotting the histogram\np &lt;- ggplot(my_data) + aes(x = my_val) +\n      geom_histogram(aes(y = ..density..), bins = 30L, \n                     fill = \"white\", colour = 1) +\n      theme_linedraw() + geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25)\nggplotly(p)\n#&gt; Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(density)` instead.\n#&gt; ℹ The deprecated feature was likely used in the ggplot2 package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/ggplot2/issues&gt;.\n\n\n\n\n\n\n3.3.2 Skewed Distribution\nSkewed distribution is data where the median does not equal the mean. A left-skewed distribution has a long tail on the left side of the graph, while a right-skewed distribution has a long tail to the right. It is named after the tail and not the peak of the graph, as values in that tail occur more often than would be expected with a normal distribution.\n\nlibrary(ggplot2)\nlibrary(plotly)\n# Creating a dataset with normal distribution\nmy_val &lt;- rexp(100000, rate = 0.5)\nvec &lt;- c(1:100000)\nmy_data &lt;- data.frame(vec, my_val)\n# Plotting the histogram\np &lt;- ggplot(my_data) + aes(x = my_val) +\n      geom_histogram(aes(y = ..density..), bins = 30L, \n                     fill = \"white\", colour = 1) +\n      theme_linedraw() + geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25)\nggplotly(p)",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "data-exploration-vis.html",
    "href": "data-exploration-vis.html",
    "title": "Data Exploration And Visualisation",
    "section": "",
    "text": "In this second part of the book, our focus will be on the the five following chapters:\n\nIn 4  Basics Of Statistics Using R we will discuss some of the statistical terms commonly used in data science and provide code examples in R.\nIn 5  Data Manipulation Using R we will pay special attention to two of the most used R libraries: dplyr and data.table.\nIn 6  Feature Engineering for Tabular Data we will understand how to perform feature engineering using R.\nIn 7  Handling Data Issues we will understand how to handle various data problems using R.\nIn 8  Data Visualisation Using ggplot2 we will look into the awesome R package ggplot2 for creating awesome graphics.",
    "crumbs": [
      "Data Exploration And Visualisation"
    ]
  },
  {
    "objectID": "basic-stats-r.html",
    "href": "basic-stats-r.html",
    "title": "4  Basics Of Statistics Using R",
    "section": "",
    "text": "4.1 Hypothesis Testing\nHypothesis testing is a statistical tool used to compare the null hypothesis to an alternative hypothesis. The null hypothesis typically states that two quantities are equivalent, while the alternative hypothesis in a two-tailed test is that the quantities are different. In contrast, the alternative hypothesis in a one-tailed test is that one quantity is larger or smaller than the other.\nIn business, hypothesis testing is almost never used to determine if one variable causes another but rather if one variable can predict the other. To conduct a hypothesis test in R, we can use the t.test() function:.\n# Create two sample datasets\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(6, 7, 8, 9, 10)\n\n# Conduct a two-sample t-test\nt.test(x, y)\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x and y\n#&gt; t = -5, df = 8, p-value = 0.001053\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -7.306004 -2.693996\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;         3         8",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics Of Statistics Using R</span>"
    ]
  },
  {
    "objectID": "basic-stats-r.html#p-value",
    "href": "basic-stats-r.html#p-value",
    "title": "4  Basics Of Statistics Using R",
    "section": "\n4.2 P-value",
    "text": "4.2 P-value\nThe p-value is the probability of observing an effect of the same size as our results given a random model. High p-values often mean that our independent variables are irrelevant. However, low p-values don’t necessarily mean they’re important, and examining the effect size and importance is crucial to determine significance.\nThe traditional significance threshold is a p-value of 0.05, but this is an arbitrary cut-off point. There’s no reason to set a line in the sand for significance. A p-value of 0.05 means that there’s a 1 in 20 probability your result could be random chance, and a p-value of 0.056 means it’s 1 in 18. Those are almost identical odds. Some journals have banned their use altogether, while others still only accept significant results.\nTo calculate the p-value in R, we can use the t.test() function.\n\n# Create two sample datasets\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(6, 7, 8, 9, 10)\n\n# Conduct a two-sample t-test\nt.test(x, y)$p.value\n#&gt; [1] 0.001052826",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics Of Statistics Using R</span>"
    ]
  },
  {
    "objectID": "basic-stats-r.html#estimates-and-statistics",
    "href": "basic-stats-r.html#estimates-and-statistics",
    "title": "4  Basics Of Statistics Using R",
    "section": "\n4.3 Estimates and Statistics",
    "text": "4.3 Estimates and Statistics\nn\nn refers to the number of observations in a dataset or level of a categorical variable. In R, nrow(dataframe) can be used to calculate the number of rows in a dataframe or length(Vector) to calculate the length of a vector. To calculate the number of observations by group, the count(Data, GroupingVariable) function can be used. For example, nrow(iris), length(iris$Sepal.Length), and count(iris, Species) can be used to find the number of observations in the iris dataset.\nMean\nmean refers to the average of a dataset, defined as the sum of all observations divided by the number of observations. In R, mean(Vector) can be used to calculate the mean of a vector. For example, mean(iris$Sepal.Length) can be used to find the mean sepal length of the iris dataset.\nTrimmed Mean\nTrimmed Mean refers to the mean of a dataset with a certain proportion of data not included. The highest and lowest values are trimmed - for instance, the 10% trimmed mean will use the middle 80% of your data. To calculate the trimmed mean in R, mean(Vector, trim = 0.##) can be used. For example, mean(iris$Sepal.Length, trim = 0.10) can be used to find the 10% trimmed mean sepal length of the iris dataset.\nVariance\nVariance refers to a measure of the spread of your data. In R, var(Vector) can be used to calculate the variance of a vector. For example, var(iris$Sepal.Length) can be used to find the variance of the sepal length in the iris dataset.\nStandard Deviation\nStandard Deviation refers to the amount any observation can be expected to differ from the mean. In R, sd(Vector) can be used to calculate the standard deviation of a vector. For example, sd(iris$Sepal.Length) can be used to find the standard deviation of the sepal length in the iris dataset.\nStandard Error\nStandard Error refers to the error associated with a point estimate (e.g. the mean) of the sample. If you’re reporting the mean of a sample variable, use the SD. If you’re putting error bars around means on a graph, use the SE. In R, there is no native function to calculate the standard error. However, it can be calculated using sd(Vector)/sqrt(length(Vector)). For example, sd(iris$Sepal.Length)/sqrt(length(iris$Sepal.Length)) can be used to find the standard error of the sepal length in the iris dataset.\nMedian\nMedian refers to a robust estimate of the center of the data. In R, median(Vector) can be used to calculate the median of a vector. For example, median(iris$Sepal.Length) can be used to find median of the data.\nMinimum\nThe min() function in R calculates the smallest value in a vector.\nMaximum\nThe max() function in R calculates the largest value in a vector.\nRange\nThe range of a dataset is the difference between the maximum and minimum values.\nQuantile\nA quantile is a point in a distribution that divides the data into equally sized subsets. The quantile() function in R can be used to calculate specific quantiles, such as quartiles (the 0.25, 0.5, and 0.75 quantiles). Note that quantiles range from 0 to 1. To convert to percentiles, multiply by 100.\nInterquartile Range\nThe interquartile range (IQR) is a measure of spread that represents the range of the middle 50% of the data. It can be calculated using the IQR() function in R\nSkew\nSkewness measures the asymmetry of a distribution. A skewness value of 0 indicates a symmetrical distribution, while positive and negative skewness values indicate a right-skewed and left-skewed distribution, respectively. The skew() function is not included in base R, but it can be found in various packages such as moments or psych.\nKurtosis\nKurtosis measures the “peakedness” of a distribution. A kurtosis value of 0 indicates a normal distribution, while positive and negative kurtosis values indicate a more peaked and flatter distribution, respectively. The kurtosis() function is not included in base R, but it can be found in various packages such as moments or psych. Values much different from 0 indicate non-normal distributions.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics Of Statistics Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html",
    "href": "data-manipulation.html",
    "title": "5  Data Manipulation Using R",
    "section": "",
    "text": "5.1 Overview\nData manipulation in data science refers to the process of transforming and modifying raw data to make it more suitable for analysis, modeling, and decision-making. It involves various operations such as filtering, sorting, aggregating, joining, reshaping, and creating derived variables. The goal of data manipulation is to organize, clean, and prepare data in a format that facilitates extracting meaningful insights and patterns.\nData manipulation is a critical step in the data science workflow for several reasons:\nAs a data scientist, you will find most of your time will be spent on exploring your data.\nFigure 5.1: The role of a data scientist and why we need them (www.raconteur.net/the-role-of-a-data-scientist-and-why-we-need-them)",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#overview",
    "href": "data-manipulation.html#overview",
    "title": "5  Data Manipulation Using R",
    "section": "",
    "text": "Data cleaning: Raw data often contains errors, missing values, outliers, or inconsistencies. Data manipulation allows for identifying and addressing these issues through techniques like data validation, imputation, removal of duplicates, and handling missing values. Clean data is essential for accurate analysis and modeling.\nFeature engineering: Data manipulation enables the creation of new variables (features) based on existing data, which can capture important patterns and relationships. Feature engineering involves operations such as transforming variables, generating interaction terms, extracting time-based features, and encoding categorical variables. Well-engineered features can significantly enhance the performance of machine learning models.\nData integration: Data manipulation facilitates combining data from multiple sources or tables through operations like merging, joining, or concatenating. This is particularly useful when working with relational databases or when integrating data from different systems. By combining disparate data sources, analysts can gain a comprehensive view and uncover insights that may not be apparent when examining individual datasets.\nData aggregation: Aggregating data involves summarizing or grouping data to derive higher-level insights. Data manipulation allows for aggregating data by factors such as time periods, geographic regions, or customer segments. Aggregating data can provide valuable summary statistics, identify trends, and support decision-making processes.\nData reshaping: Data manipulation allows for transforming data from one format to another, such as pivoting data from a wide format to a long format or vice versa. Reshaping data is useful for various analytical tasks, including data visualization, time series analysis, and data modeling. It enables data to be structured in a way that is most suitable for the analysis at hand.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#data-manipulation-with-dplyr",
    "href": "data-manipulation.html#data-manipulation-with-dplyr",
    "title": "5  Data Manipulation Using R",
    "section": "\n5.2 Data Manipulation with dplyr\n",
    "text": "5.2 Data Manipulation with dplyr\n\nThe R package dplyr is a powerful and popular package for data manipulation and transformation. It provides a set of functions that offer a consistent and intuitive syntax to perform common data manipulation tasks. dplyr focuses on data frames as the primary data structure and aims to make data manipulation more efficient and user-friendly.\nHere’s a brief overview of how to use dplyr:\n\nCreating a data frame: You can create a data frame or load one from external sources (e.g., CSV files) using the data.frame() function or read.csv() function, respectively.\nData manipulation: dplyr provides a set of core functions that enable efficient and readable data manipulation. Some commonly used functions include:\n\n\nSelecting columns: select(df, col1, col2)\n\nFiltering rows: filter(df, condition)\n\nAdding or modifying columns: mutate(df, new_col = expression)\n\nArranging rows: arrange(df, col)\n\nGrouping data: group_by(df, group_col)\n\nSummarizing data: summarize(df, new_col = function(col))\n\nJoining data: inner_join(df1, df2, by = \"key_col\")\n\n\n\nPiping %&gt;% operator: dplyr utilizes the %&gt;% operator from the magrittr package, allowing you to chain multiple operations together in a clear and readable manner. This “pipe” operator enhances code readability and reduces the need for intermediate variables.\nEfficiency and performance: dplyr is designed to be efficient, leveraging optimized C++ code and lazy evaluation. It also integrates well with other packages like dbplyr for connecting to databases and working with large datasets.\n\nHere is an example:\n\n# Load dplyr package\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Create a data frame\ndf &lt;- data.frame(\n  CustomerID = c(1, 1, 2, 2, 3),\n  Date = as.Date(c(\"2023-01-01\", \"2023-01-05\", \"2023-01-02\", \"2023-01-06\", \"2023-01-03\")),\n  Product = c(\"A\", \"B\", \"A\", \"C\", \"B\"),\n  Quantity = c(10, 5, 7, 3, 8)\n)\n\n# Convert data frame to tibble (optional)\ndf &lt;- as_tibble(df)\n\n# Subset data and calculate total quantity by product\ndf_subset &lt;- df %&gt;%\n  filter(Date &gt;= as.Date(\"2023-01-02\"), Date &lt;= as.Date(\"2023-01-05\")) %&gt;%\n  select(-CustomerID)  # Exclude CustomerID column from output\ndf_summary &lt;- df_subset %&gt;%\n  group_by(Product) %&gt;%\n  summarize(TotalQuantity = sum(Quantity))\n\nhead(df_summary, 4)\n#&gt; # A tibble: 2 × 2\n#&gt;   Product TotalQuantity\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 A                   7\n#&gt; 2 B                  13",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#data-manipulation-with-data.table",
    "href": "data-manipulation.html#data-manipulation-with-data.table",
    "title": "5  Data Manipulation Using R",
    "section": "\n5.3 Data Manipulation with data.table\n",
    "text": "5.3 Data Manipulation with data.table\n\nThe R package data.table is an efficient and powerful extension of the base R data.frame. It provides a high-performance data manipulation tool with syntax and functionality optimized for large datasets. The primary goal of data.table is to offer fast and memory-efficient operations for handling substantial amounts of data.\nHere’s a brief overview of how to use data.table:\n\nCreating a data.table: You can create a data.table from a data.frame or by directly specifying the data.\nData manipulation: data.table provides concise and fast syntax for various data manipulation tasks, including filtering, sorting, aggregating, joining, and updating data. Some commonly used operations include:\n\n\nSubset rows using conditions: dt[condition]\n\nSelect columns: dt[, c(\"col1\", \"col2\")]\n\nModify or create columns: dt[, new_col := expression]\n\nSort data: dt[order(col)]\n\nAggregate data: dt[, .(mean_col = mean(col)), by = group_col]\n\nJoin data: dt1[dt2, on = \"key_col\"]\n\n\n\nEfficiency: data.table is designed to handle large datasets efficiently. It uses memory-mapped files and optimized algorithms to minimize memory usage and improve performance. Additionally, it provides parallel processing capabilities, allowing you to make use of multiple cores for faster computations.\n\ndata.table is especially beneficial in scenarios where you’re working with large datasets and need to perform complex data manipulations quickly. It shines in the following scenarios:\n\nBig data analysis: When dealing with datasets that are too large to fit in memory, data.table provides an efficient solution by minimizing memory usage and optimizing performance.\nSpeed optimization: data.table is specifically engineered for fast and scalable operations. It outperforms base R and other packages for tasks like subsetting, aggregating, and merging data.\nTime-series analysis: data.table offers powerful functionality for working with time-series data, such as rolling joins and efficient grouping and aggregation.\nData cleaning and preprocessing: data.table provides concise and efficient syntax for filtering, transforming, and reshaping data, making it ideal for data cleaning and preprocessing tasks.\n\nHere is an example\n\n# Load data.table package\nlibrary(data.table)\n#&gt; \n#&gt; Attaching package: 'data.table'\n#&gt; The following objects are masked from 'package:dplyr':\n#&gt; \n#&gt;     between, first, last\n\n# Create a data.table\ndt &lt;- data.table(\n  CustomerID = c(1, 1, 2, 2, 3),\n  Date = as.Date(c(\"2023-01-01\", \"2023-01-05\", \"2023-01-02\", \"2023-01-06\", \"2023-01-03\")),\n  Product = c(\"A\", \"B\", \"A\", \"C\", \"B\"),\n  Quantity = c(10, 5, 7, 3, 8)\n)\n\n# Subset data and calculate total quantity by product\ndt_subset &lt;- dt[Date &gt;= as.Date(\"2023-01-02\") & Date &lt;= as.Date(\"2023-01-05\")]\ndt_summary &lt;- dt_subset[, .(TotalQuantity = sum(Quantity)), by = Product]\n\nhead(dt_summary, 4)\n#&gt;    Product TotalQuantity\n#&gt;     &lt;char&gt;         &lt;num&gt;\n#&gt; 1:       B            13\n#&gt; 2:       A             7",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#dplyr-vs.-data.table",
    "href": "data-manipulation.html#dplyr-vs.-data.table",
    "title": "5  Data Manipulation Using R",
    "section": "\n5.4 dplyr vs. data.table\n",
    "text": "5.4 dplyr vs. data.table\n\nWhen deciding between data.table and dplyr for data manipulation in R, several factors should be considered. Both packages offer powerful functionality, but they have different design philosophies and performance characteristics. Let’s compare them in terms of syntax, performance, memory usage, functionality, and use cases:\n\n\nSyntax:\n\n\ndplyr has a more intuitive and expressive syntax that closely resembles natural language. Its function names and the %&gt;% pipe operator contribute to readable and concise code.\n\ndata.table has a terser syntax designed for efficiency and speed. It uses square brackets [ ] for subsetting and assignment operations, which can take some time to get used to.\n\n\n\nPerformance:\n\n\ndata.table is specifically optimized for fast data manipulation and performs exceptionally well on large datasets. It uses memory-mapped files, efficient indexing, and parallel processing to achieve high performance.\n\ndplyr performs well for smaller datasets but may face performance limitations when dealing with very large datasets due to its use of in-memory operations.\n\n\n\nMemory usage:\n\n\ndata.table is memory efficient and optimized for handling large datasets by minimizing memory allocations. It uses a “by-reference” approach, which reduces memory duplication and can be useful for memory-constrained environments.\n\ndplyr is more memory intensive, as it generally creates new copies of data frames during each operation. This can be a disadvantage when working with very large datasets that exceed available memory.\n\n\n\nFunctionality:\n\nBoth packages offer similar functionality for data manipulation tasks, including subsetting, filtering, aggregating, and joining data. However, data.table provides additional features like fast grouping, updating columns by reference, and rolling joins, which may not be available or as efficient in dplyr.\n\ndplyr has a broader ecosystem and integrates well with other tidyverse packages, such as tidyr andggplot2`, making it convenient for end-to-end data analysis workflows.\n\n\n\nUse cases:\n\nIf you primarily work with large datasets that require efficient and high-performance operations, data.table is a strong choice. It excels in scenarios involving big data, time-series analysis, and situations where speed is crucial.\nIf you prioritize code readability, prefer a more intuitive and user-friendly syntax, and work with smaller to medium-sized datasets, dplyr is a good fit. It is well-suited for interactive data exploration, data cleaning, and general data analysis tasks.\n\n\n\nIt’s worth noting that the choice between data.table and dplyr is not mutually exclusive. Both packages can coexist in the same R environment, allowing you to leverage the strengths of each when appropriate. You can even convert between data.table and dplyr objects using functions like as.data.table() and as_tibble().",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "feature-eng.html",
    "href": "feature-eng.html",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "",
    "text": "6.1 Overview\nFeature engineering is the process of preparing the data for the learning algorithms.\nFeature engineering encompasses a variety of tasks, including:\nIt is essential to recognize that different learning algorithms have unique feature engineering requirements. A technique that is appropriate for a specific learning algorithm and context may be unnecessary or even detrimental for others. Consequently, there is no universal formula or a single correct approach to feature engineering. As such, it is recommended to experiment with various feature engineering techniques and allow the data to guide decision-making. This approach acknowledges that effective feature engineering requires a deep understanding of the data and the problem at hand. By leveraging data-driven insights, analysts can optimise feature engineering efforts and ultimately improve the performance of machine learning models.\nIn the following sections will describe several feature engineering techniques for tabular data. However, keep in mind that a key aspect of feature engineering is to come up with informative features. That depends on the context, rather than on specific techniques.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#overview",
    "href": "feature-eng.html#overview",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "",
    "text": "Extracting features from raw data;\nConstructing informative features based on domain knowledge;\nProcessing the data into the format required by different learning algorithms;\nProcessing the predictors in ways that help the learning algorithm to build better models;\nImputing missing values.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#feature-scaling-on-numerical-features",
    "href": "feature-eng.html#feature-scaling-on-numerical-features",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "\n6.2 Feature scaling on numerical features",
    "text": "6.2 Feature scaling on numerical features\nMost learning algorithms require the inputs to be on the same scale and sometimes be centred around zero. It is a critical preprocessing step in machine learning, especially for algorithms that are sensitive to the scale of input features. Feature scaling can help to improve model performance by ensuring that all features are on a similar scale and preventing some features from dominating others. Common methods for feature scaling include min-max scaling, z-score normalisation, and log scaling.\nHere’s an example in R using the iris dataset:\n\n# Load required package\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Load iris dataset\ndata(iris)\n\n# Select numerical features\nnumerical_features &lt;- iris %&gt;% select_if(is.numeric)\n\n# Perform feature scaling using the scale() function\nscaled_features &lt;- scale(numerical_features)\n\n# Convert scaled features back to a data frame\nscaled_df &lt;- as.data.frame(scaled_features)\n\n# Print the first few rows of the scaled dataset\nhead(scaled_df)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; 1   -0.8976739  1.01560199    -1.335752   -1.311052\n#&gt; 2   -1.1392005 -0.13153881    -1.335752   -1.311052\n#&gt; 3   -1.3807271  0.32731751    -1.392399   -1.311052\n#&gt; 4   -1.5014904  0.09788935    -1.279104   -1.311052\n#&gt; 5   -1.0184372  1.24503015    -1.335752   -1.311052\n#&gt; 6   -0.5353840  1.93331463    -1.165809   -1.048667\n\nIn this example, we use the iris dataset and select the numerical features using select_if(is.numeric). The scale() function is then applied to the numerical features to perform feature scaling. It standardizes each feature by subtracting the mean and dividing by the standard deviation. The resulting scaled features are stored in the scaled_features object.\nTo convert the scaled features back to a data frame, we use as.data.frame(scaled_features). Finally, we print the first few rows of the scaled dataset using head(scaled_df).\nAfter applying feature scaling, each numerical feature will have a mean of zero and a standard deviation of one, resulting in comparable scales across all features. This is particularly useful for algorithms that are sensitive to differences in feature scales, such as distance-based algorithms (e.g., k-means clustering) or regularization-based models (e.g., linear regression with L1 or L2 regularization).",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#categorical-features",
    "href": "feature-eng.html#categorical-features",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "\n6.3 Categorical features",
    "text": "6.3 Categorical features\nOne-hot encoding\nOne-hot encoding is a popular approach for encoding nominal variables is one-hot encoding. This technique involves constructing a binary indicator for each category of the nominal variable. For instance, if the nominal variable is colour ∈ {blue, yellow, red}, we can represent it as a feature vector with binary indicators for each possible value: * x = [1,0,0] if blue, * x = [0,1,0] if yellow, * x = [0,0,1] if red.\nBy encoding nominal variables in this manner, analysts can use them as input features for machine learning algorithms that require numerical input.\nDummy encoding\nDummy encoding is a technique used to convert nominal variables into numerical features, similar to one-hot encoding. We start with one-hot encoding but delete one arbitrary feature to avoid perfect multicollinearity.If we have a predictor gender ∈ {male, female}, we encode it as * x = 0 if male, * x = 1 if female.\nSparse category\nIn categorical analysis, some categories may have low counts, which can reduce the statistical power of the analysis or complicate interpretation. To address this issue, analysts may choose to merge all categories with a count below a certain minimum into a more general “other” category. This approach is useful for simplifying the analysis and improving interpretability by reducing the number of categories. By aggregating low-count categories into a broader category, analysts can improve the accuracy and reliability of statistical models.\nHigh cardinality\nIn statistical analysis, the term “cardinality” refers to the number of unique values in a variable. High cardinality indicates that there is a large number of categories, which can present challenges in data analysis. To address this issue, several techniques are commonly used: * Merging sparse categories: categories with low frequency can be merged into a more general category, improving the statistical power of the analysis. * Merging similar categories: categories that are similar in meaning can be combined to simplify the analysis and improve interpretability. * Hash encoding: this technique involves hashing the categorical values to a fixed number of bins, reducing the dimensionality of the data while retaining some of the information in the original categories. * Target encoding: this technique involves encoding categorical variables as the average value of the target variable within each category, which can improve the accuracy of predictive models. * Using specialised algorithms: some machine learning algorithms, such as CatBoost, are designed specifically to handle categorical features efficiently and accurately.\nBy employing these techniques, data scientists can effectively handle high-cardinality categorical variables and improve the accuracy and interpretability of statistical models.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(caret)\n#&gt; Loading required package: ggplot2\n#&gt; Loading required package: lattice\nlibrary(Matrix)\n\n# Create a sample data frame\ndataf &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"D\", \"C\"),\n  label = c(1, 0, 1, 0, 1, 0, 1)\n)\n\n# One-hot encoding using caret package\ndmy &lt;- caret::dummyVars(\" ~ .\", data = dataf)\none_hot_encoded &lt;- data.frame(predict(dmy, newdata = dataf))\nprint(one_hot_encoded)\n#&gt;   categoryA categoryB categoryC categoryD label\n#&gt; 1         1         0         0         0     1\n#&gt; 2         0         1         0         0     0\n#&gt; 3         0         0         1         0     1\n#&gt; 4         1         0         0         0     0\n#&gt; 5         0         1         0         0     1\n#&gt; 6         0         0         0         1     0\n#&gt; 7         0         0         1         0     1\n\n# Dummy encoding using dplyr package\ndummy_encoded &lt;- dataf %&gt;%\n  mutate(category = as.factor(category)) %&gt;%\n  bind_cols(model.matrix(~ category - 1, data = .))\n\n# Print the dummy encoded features\nprint(dummy_encoded)\n#&gt;   category label categoryA categoryB categoryC categoryD\n#&gt; 1        A     1         1         0         0         0\n#&gt; 2        B     0         0         1         0         0\n#&gt; 3        C     1         0         0         1         0\n#&gt; 4        A     0         1         0         0         0\n#&gt; 5        B     1         0         1         0         0\n#&gt; 6        D     0         0         0         0         1\n#&gt; 7        C     1         0         0         1         0\n\n# Handling sparse categories using Matrix package\nsparse_categories &lt;- dataf %&gt;%\n  mutate(category = as.factor(category)) %&gt;%\n  mutate(category = as.character(category))\n\n# Create a sparse matrix of the categories\nsparse_matrix &lt;- sparse.model.matrix(~ category - 1, data = sparse_categories)\n\n# Convert the sparse matrix to a regular matrix\nsparse_encoded &lt;- as.data.frame(as.matrix(sparse_matrix))\n\n# Print the sparse encoded features\nprint(sparse_encoded)\n#&gt;   categoryA categoryB categoryC categoryD\n#&gt; 1         1         0         0         0\n#&gt; 2         0         1         0         0\n#&gt; 3         0         0         1         0\n#&gt; 4         1         0         0         0\n#&gt; 5         0         1         0         0\n#&gt; 6         0         0         0         1\n#&gt; 7         0         0         1         0",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#mixed-features",
    "href": "feature-eng.html#mixed-features",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "\n6.4 Mixed features",
    "text": "6.4 Mixed features\nDifferent machine learning algorithms have varying abilities to handle mixed data types. Tree-based methods, for example, can naturally handle mixed data types, while methods like k-Nearest Neighbours (kNN) work best when all predictors are directly comparable. However, one challenge that arises when working with mixed data types is ensuring that all features are on the same scale. While there are natural ways to scale continuous variables, there is no straightforward way to scale categorical variables, which are often converted to numerical features using techniques such as one-hot encoding or dummy encoding. Despite this issue, in practice, analysts often treat numerical features constructed from categorical predictors in the same way as any other numerical predictor. While some methods may require all features to be on the same scale, this issue is frequently ignored when working with mixed data types.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#interaction-features",
    "href": "feature-eng.html#interaction-features",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "\n6.5 Interaction features",
    "text": "6.5 Interaction features\nAn interaction effect refers to a situation where the relationship between the response variable and a predictor variable is dependent on one or more other predictor variables. In other words, the effect of a predictor on the response varies based on the level or values of other predictors.\nThe model for the illustration is \\(f(x)\\) = \\(\\beta_0\\) + \\(\\beta_1\\) × income + \\(\\beta_2\\) × student + \\(\\beta_3\\) × income × student, and student = 1 if the observed individual is student and student = 0 otherwise. \\[\n\\begin{cases}\n    \\text{student} = 1 \\text{ if the observed individual is student}\\\\\n    \\text{student} = 0 \\text{ otherwise}\n\\end{cases}\n\\]\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  feature1 = c(\"A\", \"B\", \"C\"),\n  feature2 = c(\"X\", \"Y\", \"Z\"),\n  label = c(1, 0, 1)\n)\n\n# Create interaction feature\ndata$interaction_feature &lt;- interaction(data$feature1, data$feature2, drop = TRUE)\n\n# Print the data frame with the interaction feature\nprint(data)\n#&gt;   feature1 feature2 label interaction_feature\n#&gt; 1        A        X     1                 A.X\n#&gt; 2        B        Y     0                 B.Y\n#&gt; 3        C        Z     1                 C.Z",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#curse-of-dimensionality",
    "href": "feature-eng.html#curse-of-dimensionality",
    "title": "6  Feature Engineering for Tabular Data",
    "section": "\n6.6 Curse of Dimensionality",
    "text": "6.6 Curse of Dimensionality\nThe curse of dimensionality refers to the difficulties that arise when working with high-dimensional data, especially in cases where the number of features (i.e., dimensions) greatly exceeds the number of observations. When dealing with interaction features, which are constructed by combining multiple predictors, the number of features can grow rapidly, exacerbating the problems associated with the curse of dimensionality. For instance:\n\nWhen \\(p=2\\) predictors, there is one possible interaction: \\(f_1(x_1, x_2)\\).\nWhen \\(p = 3\\), there are four: \\(f_1(x_1, x_2),\\text{ }f_2(x_1, x_3),\\text{ }f_3(x_2, x_3)\\), and \\(f_4(x_1, x_2, x_3)\\).\nWhen \\(p = 4\\), there are 11 combinations.\nWhen \\(p = 10\\), there are 1013 combinations.\nWhen \\(p = 50\\), there are there are 1,125,899,906,842,573 combinations.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "handling-data-issues.html",
    "href": "handling-data-issues.html",
    "title": "7  Handling Data Issues",
    "section": "",
    "text": "7.1 Missing values\nIn dealing with missing values, several methods can be employed: * Removing rows that contain missing values should be avoided since it could lead to the loss of valuable information. * Similarly, removing columns that have too many missing values should also be avoided. * Simple imputation methods such as replacing missing values with the mean, median, or mode can be used, although this may result in biased estimates. * More sophisticated imputation methods such as building a model to predict missing values can be employed. * Another approach is to create a dummy variable that indicates the presence of missing values. In some cases, the fact that data is missing may carry valuable information. * Lastly, some learning algorithms can handle missing values directly, making it unnecessary to perform any imputation.\n# Load required package\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Create a sample data frame with missing values\ndata &lt;- data.frame(\n  feature1 = c(1, 2, NA, 4),\n  feature2 = c(\"A\", NA, \"C\", \"D\"),\n  feature3 = c(5, 6, 7, NA),\n  label = c(1, 0, 1, 0)\n)\n\n# Removing rows with missing values\ndata_without_missing_rows &lt;- na.omit(data)\n\n# Removing columns with too many missing values\ndata_without_missing_cols &lt;- data %&gt;% \n  select_if(function(x) sum(is.na(x)) &lt; 0.5 * nrow(data))\n\n# Imputation using mean, median, or mode\nimputed_data &lt;- data %&gt;%\n  mutate(\n    feature1_imputed = ifelse(is.na(feature1), mean(feature1, na.rm = TRUE), feature1),\n    feature2_imputed = ifelse(is.na(feature2), median(feature2, na.rm = TRUE), feature2),\n    feature3_imputed = ifelse(is.na(feature3), as.character(mode(feature3)), feature3)\n  )\n\n# Imputation using model-based approach\nlibrary(mice)\n#&gt; \n#&gt; Attaching package: 'mice'\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     cbind, rbind\nimputed_data_model &lt;- complete(mice(data))\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  feature1\n#&gt;   1   2  feature1\n#&gt;   1   3  feature1\n#&gt;   1   4  feature1\n#&gt;   1   5  feature1\n#&gt;   2   1  feature1\n#&gt;   2   2  feature1\n#&gt;   2   3  feature1\n#&gt;   2   4  feature1\n#&gt;   2   5  feature1\n#&gt;   3   1  feature1\n#&gt;   3   2  feature1\n#&gt;   3   3  feature1\n#&gt;   3   4  feature1\n#&gt;   3   5  feature1\n#&gt;   4   1  feature1\n#&gt;   4   2  feature1\n#&gt;   4   3  feature1\n#&gt;   4   4  feature1\n#&gt;   4   5  feature1\n#&gt;   5   1  feature1\n#&gt;   5   2  feature1\n#&gt;   5   3  feature1\n#&gt;   5   4  feature1\n#&gt;   5   5  feature1\n#&gt; Warning: Number of logged events: 2\n\n# Creating a dummy variable for missing values\ndata_with_dummy &lt;- data %&gt;%\n  mutate(\n    feature1_missing = ifelse(is.na(feature1), 1, 0),\n    feature2_missing = ifelse(is.na(feature2), 1, 0),\n    feature3_missing = ifelse(is.na(feature3), 1, 0)\n  )\n\n# Learning algorithms handling missing values\nlibrary(randomForest)\n#&gt; randomForest 4.7-1.1\n#&gt; Type rfNews() to see new features/changes/bug fixes.\n#&gt; \n#&gt; Attaching package: 'randomForest'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\nrf_model &lt;- randomForest(label ~ ., data = data, na.action = na.exclude)\n#&gt; Warning in randomForest.default(m, y, ...): The response has five or fewer\n#&gt; unique values.  Are you sure you want to do regression?\n\n# Print the modified data frames\nprint(data_without_missing_rows)\n#&gt;   feature1 feature2 feature3 label\n#&gt; 1        1        A        5     1\nprint(data_without_missing_cols)\n#&gt;   feature1 feature2 feature3 label\n#&gt; 1        1        A        5     1\n#&gt; 2        2     &lt;NA&gt;        6     0\n#&gt; 3       NA        C        7     1\n#&gt; 4        4        D       NA     0\nprint(imputed_data)\n#&gt;   feature1 feature2 feature3 label feature1_imputed feature2_imputed\n#&gt; 1        1        A        5     1         1.000000                A\n#&gt; 2        2     &lt;NA&gt;        6     0         2.000000                C\n#&gt; 3       NA        C        7     1         2.333333                C\n#&gt; 4        4        D       NA     0         4.000000                D\n#&gt;   feature3_imputed\n#&gt; 1                5\n#&gt; 2                6\n#&gt; 3                7\n#&gt; 4          numeric\nprint(imputed_data_model)\n#&gt;   feature1 feature2 feature3 label\n#&gt; 1        1        A        5     1\n#&gt; 2        2     &lt;NA&gt;        6     0\n#&gt; 3        4        C        7     1\n#&gt; 4        4        D       NA     0\nprint(data_with_dummy)\n#&gt;   feature1 feature2 feature3 label feature1_missing feature2_missing\n#&gt; 1        1        A        5     1                0                0\n#&gt; 2        2     &lt;NA&gt;        6     0                0                1\n#&gt; 3       NA        C        7     1                1                0\n#&gt; 4        4        D       NA     0                0                0\n#&gt;   feature3_missing\n#&gt; 1                0\n#&gt; 2                0\n#&gt; 3                0\n#&gt; 4                1",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling Data Issues</span>"
    ]
  },
  {
    "objectID": "handling-data-issues.html#outliers",
    "href": "handling-data-issues.html#outliers",
    "title": "7  Handling Data Issues",
    "section": "\n7.2 Outliers",
    "text": "7.2 Outliers\nOutliers are values that are significantly different from the other values in a dataset. They can have a large effect on statistical estimates and machine learning models. Here are some common approaches for dealing with outliers: * Avoid deleting outliers unless they are clearly due to measurement or data entry errors. Outliers may contain valuable information about the data or the underlying process generating the data. * If an outlier is due to an error, try to fix the error if possible. If there is no way to correct the error, treat the outlier as a missing value. * Censoring involves setting a threshold value for a variable and any values beyond the threshold are set to the threshold value. This can be useful when extreme values are unlikely but still possible, and they can be safely considered as equivalent to the threshold. * Transforming the variable can sometimes make the data more amenable to analysis or modelling. For example, taking the logarithm of a skewed variable can help to make it more symmetric. * Creating a dummy variable to indicate outliers can be useful when outliers are expected to have a different effect on the response variable than other values. For example, a dummy variable could indicate whether a data point is an outlier or not, and this variable could be used as a predictor in a regression model. * Using a learning algorithm that is robust to outliers can also be effective. For example, robust regression methods can downright the influence of outliers on the model fit.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(car)\n#&gt; Loading required package: carData\n#&gt; \n#&gt; Attaching package: 'car'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     recode\nlibrary(MASS)\n#&gt; \n#&gt; Attaching package: 'MASS'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     select\n\n# Create a sample data frame with outliers\ndata &lt;- data.frame(\n  variable = c(1, 2, 3, 100, 5, 6, 200),\n  label = c(1, 0, 1, 0, 1, 0, 1)\n)\n\n# Avoid deleting outliers\n# Outliers may contain valuable information, so we don't remove them\n\n# Fix errors or treat outliers as missing values\ndata$variable_fixed &lt;- ifelse(data$variable &gt; 100, NA, data$variable)\n\n# Censoring outliers\nthreshold &lt;- 100\ndata$variable_censored &lt;- ifelse(data$variable &gt; threshold, threshold, data$variable)\n\n# Transforming the variable\ndata$variable_log &lt;- log(data$variable)\n\n# Creating a dummy variable to indicate outliers\ndata$outlier_dummy &lt;- ifelse(data$variable &gt; 100, 1, 0)\n\n# Using a robust learning algorithm\nmodel &lt;- rlm(formula = label ~ variable, data = data, method = \"MM\")\n\n# Print the modified data frame\nprint(data)\n#&gt;   variable label variable_fixed variable_censored variable_log outlier_dummy\n#&gt; 1        1     1              1                 1    0.0000000             0\n#&gt; 2        2     0              2                 2    0.6931472             0\n#&gt; 3        3     1              3                 3    1.0986123             0\n#&gt; 4      100     0            100               100    4.6051702             0\n#&gt; 5        5     1              5                 5    1.6094379             0\n#&gt; 6        6     0              6                 6    1.7917595             0\n#&gt; 7      200     1             NA               100    5.2983174             1",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling Data Issues</span>"
    ]
  },
  {
    "objectID": "handling-data-issues.html#data-leakage",
    "href": "handling-data-issues.html#data-leakage",
    "title": "7  Handling Data Issues",
    "section": "\n7.3 Data leakage",
    "text": "7.3 Data leakage\nLeakage in statistical learning refers to a situation where a model is trained using information that would not be available during prediction in real-world scenarios. Leakage can lead to over-optimistic estimates of the model’s performance, as the model may be relying on information that it would not have access to during deployment. To avoid leakage, it is essential to carefully inspect the variables used in the model and ensure that they are not revealing information that would not be available in a production environment. By avoiding leakage, we can create more accurate and reliable machine learning models.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Handling Data Issues</span>"
    ]
  },
  {
    "objectID": "data-visualisation.html",
    "href": "data-visualisation.html",
    "title": "8  Data Visualisation Using ggplot2",
    "section": "",
    "text": "8.1 Introduction and Basics\nThe utilization of ggplot2 for data visualization offers several advantages that make it a preferred choice among data analysts and data scientists.\nOne significant benefit is its adherence to the grammar of graphics, a conceptual framework that provides a consistent and structured approach to creating visualizations. The grammar of graphics breaks down a plot into its fundamental components: data, aesthetics, geometries, scales, and facets. This modular approach allows for greater flexibility and customization. For instance, with ggplot2, users can easily map variables to aesthetics, such as color or size, to reveal patterns and relationships in the data.\nAdditionally, ggplot2 simplifies the process of creating complex plots by providing a wide range of geometries, including points, lines, bars, and areas, which can be tailored to suit specific data types and analysis requirements. By understanding and leveraging the grammar of graphics, users can create visually compelling and informative plots that effectively communicate insights from their data.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation.html#customization-and-styling",
    "href": "data-visualisation.html#customization-and-styling",
    "title": "8  Data Visualisation Using ggplot2",
    "section": "\n8.2 Customization and Styling",
    "text": "8.2 Customization and Styling\nggplot2 is a powerful and versatile package for creating customizable and visually appealing plots. It provides a wide range of options to customize plots, allowing users to tailor their visualizations to meet specific requirements. With ggplot2, you can easily customize plot titles, axis labels, and legends to provide clear and descriptive information.\nFor example, to add a title to a plot, you can use the labs() function:\n\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n# Create a scatter plot\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Sepal Length vs. Sepal Width\",\n       x = \"Sepal Length\", y = \"Sepal Width\")\nplotly::ggplotly(p)\n\n\n\n\n\nMoreover, ggplot2 allows you to modify colors, shapes, and sizes of plot elements to enhance visual clarity and emphasize important aspects.\nFor instance, you can change the color of points in a scatter plot using the color argument:\n\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  labs(title = \"Scatter Plot with Colored Points\",\n       x = \"Sepal Length\", y = \"Sepal Width\")\nplotly::ggplotly(p)\n\n\n\n\n\nAdjusting axis scales and limits is another crucial aspect of plot customization. You can control the range of values displayed on the x-axis and y-axis using functions such as xlim() and ylim(). Here’s an example of adjusting the axis limits in a scatter plot:\n\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title = \"Scatter Plot with Adjusted Axis Limits\",\n       x = \"Sepal Length\", y = \"Sepal Width\") +\n  xlim(4, 8) +\n  ylim(2, 4.5)\nggplotly(p)\n\n\n\n\n\nGrouping and faceting in ggplot2 allow you to visualize subsets of data or create multiple plots based on categorical variables. By using the facet_wrap() or facet_grid() functions, you can split the data into panels based on specific grouping variables.\nHere’s an example that demonstrates faceting in a scatter plot by species:\n\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title = \"Scatter Plot with Grouping and Faceting\",\n       x = \"Sepal Length\", y = \"Sepal Width\") +\n  facet_wrap(~ Species)\nggplotly(p)\n\n\n\n\n\nWorking with multiple data sources is also facilitated by ggplot2. You can combine and overlay different datasets to create layered and composite visualizations.\nFor instance, you can combine a scatter plot and a line plot by using the + operator:\n\ndf1 &lt;- data.frame(x = 1:10, y = 1:10)\ndf2 &lt;- data.frame(x = 1:10, y = 10:1)\n\np &lt;- ggplot() +\n  geom_point(data = df1, aes(x, y)) +\n  geom_line(data = df2, aes(x, y)) +\n  labs(title = \"Overlaying Multiple Data Sources\")\nggplotly(p)",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation.html#creating-advanced-plot-types",
    "href": "data-visualisation.html#creating-advanced-plot-types",
    "title": "8  Data Visualisation Using ggplot2",
    "section": "\n8.3 Creating Advanced Plot Types",
    "text": "8.3 Creating Advanced Plot Types\nR ggplot2 is a versatile and powerful package for data visualization, allowing users to create advanced plot types and customize their visual appearance. One key aspect of ggplot2 is the ability to create advanced plot types, beyond basic scatter plots or bar charts, to effectively represent complex data patterns. This can be achieved by incorporating additional geometries, statistical transformations, and layering multiple visual elements. For example, users can create visually appealing heatmaps, boxplots, violin plots, or density plots, among others, using the extensive set of geometries and statistical functions provided by ggplot2.\nTo ensure consistent plot styling across different visualizations, ggplot2 offers themes and templates that allow users to define and apply predefined sets of formatting rules. By applying a theme, users can easily modify the appearance of various plot elements such as axes, titles, legends, background colors, and fonts. This helps to maintain a cohesive visual style throughout multiple plots in a project or presentation. Additionally, ggplot2 provides options to customize themes or create custom templates to suit specific design preferences or conform to brand guidelines.\nHere’s an example that demonstrates creating an advanced plot type (a violin plot) and applying a custom theme for consistent plot styling:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Create a dataset\ndata &lt;- data.frame(\n  Group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  Value = rnorm(300)\n)\n\n# Create a violin plot\np &lt;- ggplot(data, aes(x = Group, y = Value)) +\n  geom_violin(fill = \"#FF6666\", color = \"#990000\", alpha = 0.8) +\n  theme_minimal()  # Apply a minimal theme\n\n# Display the plot\nggplotly(p)\n\n\n\n\n\nIn the above example, we create a dataset with three groups (A, B, C) and corresponding values. We use the geom_violin() function to create a violin plot, specifying the fill color, border color, and transparency. Finally, we apply the theme_minimal() function to apply a minimal theme to the plot, which removes unnecessary background elements and provides a clean and focused visualization.\nBy exploring the various advanced plot types available in ggplot2 and leveraging themes and templates, users can create visually striking and consistent plots that effectively communicate complex data patterns.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "supervised-learning.html",
    "href": "supervised-learning.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this third part of the book, our focus will be on the the five following chapters:\n\nIn ?sec-basics-supervised-learning we will get to understand the basics of supervised learning from a theoretical and practical standpoint.\nIn 10  Linear Regression we will dive into linear regression from a theoretical and practical standpoint.\nIn 11  K-Nearest Neighbour Regression we will understand the k-Nearest Neighbours (kNN) regression algorithm as a classic method for nonlinear data..\nIn 12  Classification Using Logistic Model we will dive into classification models.",
    "crumbs": [
      "Supervised Learning"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html",
    "href": "basic-supervised-learning.html",
    "title": "9  Basics of Supervised Learning",
    "section": "",
    "text": "9.1 Overview\nIn this section, we will introduce the fundamentals of supervised learning algorithms. It is a widely used approach to machine learning, and it has numerous practical applications in various industries. Here are some real-life examples of supervised learning:\nThese are just a few examples of the many applications of supervised learning. With the increasing availability of labeled data and the development of advanced algorithms, the potential applications of supervised learning are constantly expanding.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#overview",
    "href": "basic-supervised-learning.html#overview",
    "title": "9  Basics of Supervised Learning",
    "section": "",
    "text": "Image Classification: Supervised learning algorithms can be trained to classify images into different categories, such as recognising faces, identifying objects in a scene, and detecting anomalies in medical images.\nFraud Detection: Supervised learning algorithms can be trained on labeled data to detect fraudulent transactions, identify unusual patterns in financial data, and prevent financial crimes.\nText and Sentiment Analysis: Supervised learning can be used for sentiment analysis, where algorithms are trained to classify text as positive or negative based on a labeled dataset. This can be used in customer feedback analysis, social media monitoring, and market research.\nMedical Diagnosis: Supervised learning algorithms can be used to diagnose diseases based on medical images, clinical data, and genetic data. Examples include the diagnosis of cancer, Alzheimer’s disease, and heart disease.\nSpeech Recognition: Supervised learning can be used to train speech recognition systems to transcribe spoken language into text, identify speakers based on their voice, and improve automatic translation systems.\nAutonomous Driving: Supervised learning algorithms can be used to train self-driving cars to recognise and respond to different traffic situations, road signs, and road conditions.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#fundamentals-of-supervised-learning-models",
    "href": "basic-supervised-learning.html#fundamentals-of-supervised-learning-models",
    "title": "9  Basics of Supervised Learning",
    "section": "\n9.2 Fundamentals of supervised learning models",
    "text": "9.2 Fundamentals of supervised learning models\nWe provide an overview of the key concepts and techniques required to build effective supervised learning models.\nWe’ll use the notation \\(Y\\) for the output variable and \\(X_1,\\ldots, X_p\\) for the input variables.  For example, suppose we want to predict the sale price of residential properties. In this case, the output variable is the price. and the input variables are the characteristics of the house such as size, number of bedrooms, number of bathrooms, and location.\nTo build a statistical learning model, we need training data. We represent the output data as a vector: \\[\\begin{equation*}       \\boldsymbol y=\\begin{pmatrix}             y_{1} \\\\             y_{2}\\\\             \\vdots\\\\             y_{n} \\end{pmatrix},\\,\\,\\,\\,\\, \\end{equation*} \\]\nwhere \\(y_i\\) refers to the observed value of the output variable for observation i. We represent the input data as the matrix: \\[\\begin{equation*}   \\boldsymbol X=\\begin{bmatrix}               \\begin{array}{cccc}                 x_{11} & x_{12} &  \\ldots & x_{1p} \\\\                 x_{21} & x_{22} &  \\ldots & x_{2p} \\\\                 \\vdots & \\vdots  & \\ddots & \\vdots \\\\                 x_{n1} & x_{n2} &  \\ldots & x_{np} \\\\               \\end{array}    \\end{bmatrix},       \\end{equation*} \\]\nwhere \\(x_{ij}\\) denotes the value of input j for observation i. We refer to this matrix as the design matrix.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#generalisation",
    "href": "basic-supervised-learning.html#generalisation",
    "title": "9  Basics of Supervised Learning",
    "section": "\n9.3 Generalisation",
    "text": "9.3 Generalisation\nThe primary objective of supervised learning is to develop a model that can accurately predict new data. To evaluate the performance of a model, we need to test its ability to generalise to data it has not seen before. This ability to generalise is referred to as the model’s generalisation capability. Generalisation is a crucial concept in machine learning, particularly in the context of building models that perform well on live data in business production systems. While it is essential to train a model using existing data, the ultimate goal is to ensure that the model can generalise well to new data. To test the generalisation capability of a model, we use a separate dataset known as test data. This dataset consists of examples that are distinct from the training data and are used to evaluate the model’s performance. Test data may be pre-existing data that we set aside for evaluation purposes or future data that the model will encounter in the real world.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#training-and-validation-sets",
    "href": "basic-supervised-learning.html#training-and-validation-sets",
    "title": "9  Basics of Supervised Learning",
    "section": "\n9.4 Training and validation sets",
    "text": "9.4 Training and validation sets\nSupervised learning is a practical approach that involves using data to inform modelling decisions and iteratively improving the models. One common practice in supervised learning is to randomly split the training data into two subsets: a training set and a validation set. The training set is used to build machine learning models. In contrast, the validation set is used to measure the performance of different models and select the best one. This technique is known as cross-validation and is a critical step in the model building process. By splitting the data into training and validation sets, we can train the model on one subset and use the other subset to evaluate its performance. This approach allows us to estimate the generalisation error of the model and assess how well it will perform on new data.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#metrics",
    "href": "basic-supervised-learning.html#metrics",
    "title": "9  Basics of Supervised Learning",
    "section": "\n9.5 Metrics",
    "text": "9.5 Metrics\nAt the beginning of a supervised learning project, it is crucial to determine the appropriate method for evaluating the predictive performance of the machine learning models. One common approach is to use a metric, which is a function that assesses the quality of a model’s predictions on a given dataset, such as the validation set.\nThe most common metric for regression is the mean squared error (MSE), \\[\\textsf{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\widehat{y}_i)^2}, \\] where \\(y_i\\) and \\(\\widehat{y}\\) are the actual and predicted values respectively. We commonly report the root mean squared error (RMSE) \\[\\textsf{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\widehat{y}_i)^2}, \\] instead of the mean squared error as it’s on the same scale as the output variable. The prediction \\(\\text{R}^2\\) is \\[\\textsf{Prediction R}^2=1-\\frac{\\sum_{i=1}^{n}(y_i-\\widehat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\overline{y})^2}\\]\nis also derived from the MSE, but does not depend on the scale of the response variable. The mean absolute error (MAE) is \\[\\textsf{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}\\vert y_i-\\widehat{y}_i\\vert. \\]\nThe MAE is less sensitive to large prediction errors than the MSE.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "linear-reg.html",
    "href": "linear-reg.html",
    "title": "10  Linear Regression",
    "section": "",
    "text": "10.1 The mathematics of Linear Regression\nIn the linear regression method, we use a linear function for prediction: \\[f(\\boldsymbol x; \\boldsymbol \\beta)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\ldots+\\beta_px_p, \\]\nwhere \\(\\boldsymbol x\\) is a vector of input values and \\(\\boldsymbol \\beta\\) is a vector of parameters.\nWe train the linear regression model by minimising the training mean squared error, \\[\\begin{align*} \\widehat{\\boldsymbol \\beta}=\\underset{\\boldsymbol \\beta}{\\operatorname{argmin}}\\,\\, \\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2\\right\\}, \\end{align*} \\]\nwhere \\(\\widehat{\\boldsymbol \\beta}\\) denotes the estimated parameters. This is known as the ordinary least squares (OLS) method for estimating a linear regression. To get the solution, let the design matrix be: \\[\\begin{equation*}    \\boldsymbol X=\\begin{bmatrix}               \\begin{array}{ccccc}                 1 & x_{11} & x_{12} &  \\ldots & x_{1p} \\\\                 1 & x_{21} & x_{22} &  \\ldots & x_{2p} \\\\                 \\vdots & \\vdots & \\vdots  & \\ddots & \\vdots \\\\                 1 & x_{n1} & x_{n2} &  \\ldots & x_{np} \\\\               \\end{array}    \\end{bmatrix},   \\end{equation*} \\] noting that we added a column of ones at the beginning of the design matrix.\nAs before, the output vector is:\n\\[\\begin{equation*}       \\boldsymbol y=\\begin{pmatrix}             y_{1} \\\\             y_{2}\\\\             \\vdots\\\\             y_{n} \\end{pmatrix}.  \\end{equation*} \\]\nWe can show that the OLS problem \\[\\begin{align*} \\widehat{\\boldsymbol \\beta}=\\underset{\\boldsymbol \\beta}{\\operatorname{argmin}}\\,\\, \\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2\\right\\}, \\end{align*} \\]\nhas solution \\[\\widehat{\\boldsymbol \\beta} = (\\boldsymbol X^\\top\\boldsymbol X)^{-1} \\boldsymbol X^\\top\\boldsymbol y, \\] under the assumption that the columns of \\(\\boldsymbol X\\) are linearly independent.\nUpon observing a new input \\(\\boldsymbol x_0\\), we make the prediction: \\[\\widehat{f}(\\boldsymbol x_0)=\\widehat{\\beta}_0+\\sum_{j=1}^p\\widehat{\\beta}_j x_{0j}. \\]",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-reg.html#classical-assumptions-of-the-linear-regression-model",
    "href": "linear-reg.html#classical-assumptions-of-the-linear-regression-model",
    "title": "10  Linear Regression",
    "section": "\n11.1 Classical assumptions of the linear regression model",
    "text": "11.1 Classical assumptions of the linear regression model\nIn the statistical approach to linear regression, the classical linear regression model is based on the following assumptions: * Linearity: if \\(X=\\boldsymbol x\\), then \\(Y=\\beta_0+\\beta_1x_1+\\ldots+\\beta_px_p+\\varepsilon\\) for some population parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) and a random error . * The errors have zero conditional mean: \\(\\mathbb E(\\varepsilon |X=\\boldsymbol x)=0\\) for all \\(\\boldsymbol x\\). * Constant error variance: \\(\\mathbb V(\\varepsilon|X=\\boldsymbol x)=\\sigma^2\\) for all \\(\\boldsymbol x\\). * Independent errors: all the errors are independent of each other. * No perfect multicollinearity. * Gaussian errors (optional): that \\(\\varepsilon \\sim N(0,\\sigma^2)\\). The linear regression algorithm for supervised learning only requires the assumption of no perfect multicollinearity, which is necessary for the least squares solution to be defined. The twist is that the classical assumptions are still desirable because they represent the ideal situation for using a linear regression model trained by OLS.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-reg.html#potential-problems",
    "href": "linear-reg.html#potential-problems",
    "title": "10  Linear Regression",
    "section": "\n11.2 Potential problems",
    "text": "11.2 Potential problems\nIn regression analysis, there are several potential issues that can arise and affect the accuracy and validity of the results. These issues include nonlinearity, non-constant error variance, dependent errors, perfect and imperfect multicollinearity, non-Gaussian errors, and outliers and leverage points.\n\nNonlinearity: One potential issue in regression analysis is nonlinearity. This occurs when the relationship between the predictor variables and the response variable is not linear. In other words, the relationship between the variables is not a straight line, but instead is curved or has other shapes. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nNon-constant Error Variance: Another potential issue is non-constant error variance, also known as heteroscedasticity. This occurs when the variability of the errors is not constant across the range of the predictor variables. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nDependent Errors: also known as autocorrelation, occur when the errors in the regression model are correlated with each other. This violates the assumption of independence of the errors, which can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nPerfect Multicollinearity: when one or more of the predictor variables are perfectly correlated with each other. This can lead to an inability to estimate the regression coefficients and affect the accuracy of the predictions.\nImperfect Multicollinearity: when there is high correlation between predictor variables, but not perfect correlation. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nNon-Gaussian Errors: the errors in the regression model do not follow a normal distribution. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nOutliers and Leverage Points: data points that are significantly different from the other data points in the dataset. Outliers can affect the estimates of the regression coefficients, while leverage points can have a large influence on the regression line. Both can affect the accuracy of the predictions.\n\nResponse transformation\nThe response variable is often highly skewed in business data. In this case, a log transformation of the output variable can:\n\nAccount for some forms of nonlinearity.\nStabilise the variance of the errors.\nReduce skewness in the errors.\n\nIn the example below, we load the mtcars dataset, which contains information about various car models. We then perform a linear regression analysis to predict the miles per gallon (mpg) based on the weight (wt) of the cars. The lm() function is used to fit the linear regression model, with mpg ~ wt specifying the dependent and independent variables.\nNext, we print the summary of the regression model using summary(model). This provides important information such as the coefficients, standard errors, p-values, and goodness-of-fit measures.\nHere is an R example to illustrate the concept:\n\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\n# read dataset\ndf = mtcars\n\n# create multiple linear model\nlm_fit &lt;- lm(mpg ~ wt, data=df)\nsummary(lm_fit)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = mpg ~ wt, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.5432 -2.3647 -0.1252  1.4096  6.8727 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\n#&gt; wt           -5.3445     0.5591  -9.559 1.29e-10 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.046 on 30 degrees of freedom\n#&gt; Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 \n#&gt; F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# this is predicted line comparing only chosen variables\np &lt;- ggplot(data = df, aes(x = wt, y = mpg)) + \n  geom_point(color='blue') +\n  geom_abline(slope = coef(lm_fit)[[\"wt\"]], \n              intercept = coef(lm_fit)[[\"(Intercept)\"]], \n              color = 'red') + \n  labs(title=\"Miles per gallon\",\n       x=\"Weight (lb/1000)\", y = \"Miles/(US) gallon\") + theme_linedraw()\nggplotly(p)",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "k-nearest-neighbour-reg.html",
    "href": "k-nearest-neighbour-reg.html",
    "title": "11  K-Nearest Neighbour Regression",
    "section": "",
    "text": "11.1 Overview\nThe kNN method keeps all training examples in memory to make predictions. When a new input comes in, kNN finds the training examples that are more similar to that new input. The prediction is the average value of the output variable for these nearest neighbours.\nIn mathematical notation, the kNN prediction is \\[\\begin{align*} \\widehat{f}(\\boldsymbol x)&=\\text{Average}\\Big[\\,y_i \\,\\Big|\\, i \\;\\text{is in}\\; \\mathcal{N}_k(\\boldsymbol x,\\mathcal{D}) \\, \\Big]\\\\ &=\\frac{1}{k}\\sum_{i \\in \\mathcal{N}_k(\\boldsymbol x,\\mathcal{D})}y_i \\end{align*}\\]\nwhere \\(\\mathcal{N}_k(\\boldsymbol x,\\mathcal{D})\\) is the set of indexes for the closest \\(k\\) data points to \\(x\\) in \\(\\mathcal{D}\\) according to some distance function \\(\\text{dist}(\\boldsymbol x,\\boldsymbol x_i)\\).\nThe kNN method is highly flexible: in principle, it can approximate any “reasonable” regression function assuming what it may look like.\nUnfortunately, the price to pay for this flexibility is that kNN performs poorly when the number of inputs increases. This problem is known as the curse of dimensionality. We’ll explain it in more detail later in the unit.\nHyperparameters A hyperparameter is a parameter of a learning algorithm that is set before the training process begins and is not directly learnable from the data. The choice of hyperparameters is crucial for many learning algorithms, as they determine their behaviour.\nThe KNN method has two hyperparameters: the number of neighbours (K) and the distance function.\nAbove, you saw how the choice of the number of neighbours has an important effect on the learned predictive function. For now, try to reach your own intuitive conclusions regarding how the number of neighbours affects the KNN method.\nA common choice of distance function is the Euclidean distance: \\[\\begin{align*} \\textsf{dist}(\\boldsymbol x_i,\\boldsymbol x_l)&=\\Vert \\boldsymbol x_i-\\boldsymbol x_l\\Vert_2\\\\[3pt] &=\\sqrt{\\sum_{j=1}^{p}(x_{ij}-x_{lj})^2}, \\end{align*}\\]\nThere are many other distance functions available. The Mahalonobis distance is \\[\\textsf{dist}(\\boldsymbol x_i,\\boldsymbol x_l)=\\sqrt{(\\boldsymbol x_i-\\boldsymbol x_l)^\\top\\boldsymbol S^{-1}(\\boldsymbol x_i-\\boldsymbol x_l)},\\]\nwhere \\(\\boldsymbol S\\) is the sample covariance matrix of the predictors. In addition to automatic scaling, this approach accounts for the correlations between the predictors.\nAn advanced approach is metric learning, which aims to learn a task-specific distance metric from data. For example, we can use metric learning metrics to choose the \\(\\boldsymbol S\\) matrix in the Mahalanobis distance. Metric learning can significantly improve KNN, but is computationally expensive.\nHere’s an example in base R to illustrate the concept of k-nearest neighbor (KNN):\n# Create a sample dataset\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- sin(x) + rnorm(length(x), mean = 0, sd = 0.3)\ndata &lt;- data.frame(x, y)\n\n# Define a function for KNN regression\nknn_regression &lt;- function(train_data, test_point, k) {\n  distances &lt;- sqrt((train_data$x - test_point$x)^2)\n  sorted_indices &lt;- order(distances)\n  neighbors &lt;- train_data$y[sorted_indices[1:k]]\n  predicted_value &lt;- mean(neighbors)\n  return(predicted_value)\n}\n\n# Predict the value using KNN regression\ntest_point &lt;- data.frame(x = 8.5)\nk &lt;- 3\npredicted_value &lt;- knn_regression(data, test_point, k)\n\n# Print the predicted value\nprint(predicted_value)\n#&gt; [1] 0.5460616\nIn this example, we create a sample dataset with two variables x and y. The x variable represents the input feature, and the y variable represents the target variable we want to predict.\nWe define a function knn_regression that takes three arguments: train_data (the training dataset), test_point (the test point for which we want to make a prediction), and k (the number of nearest neighbors to consider).\nInside the knn_regression function, we calculate the Euclidean distance between the x values of the training data and the x value of the test point. Then, we sort the distances and select the k nearest neighbors based on the sorted indices. We extract the corresponding y values of the neighbors and calculate the mean as the predicted value.\nWe predict the value for a test point with x = 8.5 and k = 3 by calling the knn_regression function with the sample dataset, test point, and k value. Finally, we print the predicted value.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>K-Nearest Neighbour Regression</span>"
    ]
  },
  {
    "objectID": "class-log-reg.html",
    "href": "class-log-reg.html",
    "title": "12  Classification Using Logistic Model",
    "section": "",
    "text": "12.1 Overview\nThe linear models that we have constructed so far have proven to be effective in forecasting numerical outcomes such as the housing price or the number of customers who may click on a hyperlink. These models, known as regression models, are suited for such applications. Nonetheless, there are situations where we require the ability to predict events or categories, rather than just a numerical quantity. For instance, we may want to ascertain the species of a flower, whether a person is likely to click on an emailed link, or the likelihood of a tree dying in a given year.\nFor these tasks, we need to use a classification model. To walk through how these work, let’s start using a new dataset included in base R - mtcars which contains information on different car models, including their fuel efficiency (mpg), number of cylinders (cyl), engine displacement (disp), horsepower (hp), weight (wt), and other features. To obtain more information on the dataset, you can type ?mtcars in the console.\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n# Load mtcars dataset\ndata(mtcars)\n\np &lt;- ggplot(mtcars, aes(mpg, am)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F)\nggplotly(p)\n#&gt; `geom_smooth()` using formula = 'y ~ x'\nThere are a few reasons based on statistics why using a simple straight line is not appropriate for classification problems. When the data has heavy tails, meaning that there are data points that have a high probability (such as 95%) of belonging to one group, the linear formula becomes less effective. Similarly, when predictors are non-linear, where high and low values of a predictor tend to make belonging to group 0 more likely and middle values tend to be 1s, the linear model performs poorly. Instead, a logistic model is preferred, which produces a line that looks more like this:\np &lt;- ggplot(mtcars, aes(mpg, am)) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F)\n\nggplotly(p)\n#&gt; `geom_smooth()` using formula = 'y ~ x'\nLogistic Regression vs. Linear Regression Logistic regression works similarly to linear regression, where data points with probabilities higher than 50% are assigned to the class “1” and so on. Comparing the two, the logistic model demonstrates slightly better classification performance. For example, using the same model formula, logistic regression achieved 81% accuracy, outperforming the linear model.\nWhile some experts, such as certain professors, argue that the simplicity of linear models makes them the best choice for the majority of datasets, logistic models are generally as easy to implement and understand. Moreover, logistic regression usually provides better classification results. As such, logistic models are commonly relied upon for classifying datasets.\nCreating Logistic Regression Models The process of creating logistic regression models is quite similar to that of linear regression models. Instead of using the lm() function, the glm() function is employed, as logistic regression belongs to the family of algorithms known as generalized linear models. The formula and data arguments still need to be supplied, just as with lm(). The main difference when using glm() is the additional requirement to specify the argument family. In this case, setting family = binomial will calculate the logistic model.\nWe can build a logistic regression model using the following line of code:\nglm(am ~ mpg, data = mtcars, family = binomial)\n#&gt; \n#&gt; Call:  glm(formula = am ~ mpg, family = binomial, data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          mpg  \n#&gt;      -6.604        0.307  \n#&gt; \n#&gt; Degrees of Freedom: 31 Total (i.e. Null);  30 Residual\n#&gt; Null Deviance:       43.23 \n#&gt; Residual Deviance: 29.68     AIC: 33.68\nThe model summary can be viewed just like we did with lm():\nlogmod &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nsummary(logmod)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = am ~ mpg, family = binomial, data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept)  -6.6035     2.3514  -2.808  0.00498 **\n#&gt; mpg           0.3070     0.1148   2.673  0.00751 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 43.230  on 31  degrees of freedom\n#&gt; Residual deviance: 29.675  on 30  degrees of freedom\n#&gt; AIC: 33.675\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\nThe output presents various metrics that help to understand and interpret the logistic regression model:\nWe can further refine our model by adding more variables and changing the formula, just as we did with linear models:\nsummary(glm(am ~ hp + wt, data = mtcars, family=\"binomial\"))\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = am ~ hp + wt, family = \"binomial\", data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept) 18.86630    7.44356   2.535  0.01126 * \n#&gt; hp           0.03626    0.01773   2.044  0.04091 * \n#&gt; wt          -8.08348    3.06868  -2.634  0.00843 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 43.230  on 31  degrees of freedom\n#&gt; Residual deviance: 10.059  on 29  degrees of freedom\n#&gt; AIC: 16.059\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 8\nWe can see the AIC decreased significantly after adding an extra feature.\nPredicting with Logistic Regression Models In fact, logistic models created with the glm() function work similarly to linear models made with lm(). However, there are some key differences to consider. For instance, when using the model to make predictions, let’s assign the model to mtmod:\nmtmod &lt;- glm(am ~ hp + wt, data = mtcars, family=\"binomial\")\nRecall how we generated predictions from our models using predict(model, data). Here is what happens if we do something similar with logistic regression:\nhead(predict(mtmod, mtcars))\n#&gt;         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive \n#&gt;         1.6757093        -0.3855769         3.4844067        -3.1339584 \n#&gt; Hornet Sportabout           Valiant \n#&gt;        -2.5961266        -5.2956878\nThis does not produce the expected probabilities. The reason is that R is attempting to use the logistic model as if it were a linear model. To fix this, we must specify type = \"response\" in our predict() call:\nhead(predict(mtmod, mtcars, type = \"response\"))\n#&gt;         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive \n#&gt;       0.842335537       0.404782533       0.970240822       0.041728035 \n#&gt; Hornet Sportabout           Valiant \n#&gt;       0.069388122       0.004988159\nWith this adjustment, we can generate the correct probabilities.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Classification Using Logistic Model</span>"
    ]
  },
  {
    "objectID": "class-log-reg.html#overview",
    "href": "class-log-reg.html#overview",
    "title": "12  Classification Using Logistic Model",
    "section": "",
    "text": "Deviance Residuals: These are a measure of the difference between the observed values and the values predicted by the model. The minimum, 1st quartile, median, 3rd quartile, and maximum deviance residuals are shown. Smaller values indicate better model fit.\nCoefficients: These are the estimated coefficients for the model predictors (intercept and mpg in this case). The Estimate column shows the estimated value of each coefficient, while the Std. Error column presents the standard error for the estimate. The z value is the test statistic (estimate divided by standard error), and Pr(&gt;|z|) is the p-value, which indicates the significance of each predictor in the model. Significance codes are provided to help interpret the p-value (e.g., *** for p &lt; 0.001, ** for p &lt; 0.01, and * for p &lt; 0.05).\nDispersion parameter: For the binomial family, the dispersion parameter is taken to be 1. This indicates that the model assumes the variance of the response variable is equal to its mean.\nNull deviance: This is the deviance of the null model (a model with no predictors). In this case, the null deviance is 43.230, calculated based on 31 degrees of freedom.\nResidual deviance: This is the deviance of the fitted model, which measures the goodness-of-fit. The residual deviance is 29.675, calculated based on 30 degrees of freedom. A lower residual deviance compared to the null deviance indicates a better model fit.\nAIC (Akaike Information Criterion): This is a measure of the model’s quality in terms of the trade-off between goodness-of-fit and model complexity. A lower AIC value suggests a better model.\nNumber of Fisher Scoring iterations: This is the number of iterations taken by the Fisher Scoring algorithm to converge. In this case, the algorithm converged in 5 iterations.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Classification Using Logistic Model</span>"
    ]
  }
]