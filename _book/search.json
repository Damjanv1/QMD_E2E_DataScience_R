[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "End To End Data Science With R",
    "section": "",
    "text": "Preface\nWelcome to the first edition of ‚ÄúEnd to End Data Science with R‚Äù!\nAfter reading this book, you‚Äôll have the tools to tackle a wide variety of data science challenges using the following skills:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#funding-support",
    "href": "index.html#funding-support",
    "title": "End To End Data Science With R",
    "section": "Funding & Support",
    "text": "Funding & Support\nThis book was primarily a volunteer effort that took thousands of hours to create üòÉ",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "End To End Data Science With R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book isn‚Äôt just the product of the listed authors but is the result of many conversations (in person and online) that we‚Äôve had with many people in the R community. We‚Äôre incredibly grateful for all the conversations we‚Äôve had with y‚Äôall; Thank You So Much!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contribution",
    "href": "index.html#contribution",
    "title": "End To End Data Science With R",
    "section": "Contribution",
    "text": "Contribution\n\n\n\nIf you would like to make a content contribution, please contact with us first via Github issues or by email: [franckess48@gmail.com]. We are implementing a schedule for updates and are creating a contributor guide.\nPlease note that this book project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "The Data Science Universe",
    "section": "",
    "text": "How this book is organized\nBelow is a list of items we are going to cover in this book:",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "intro.html#how-this-book-is-organized",
    "href": "intro.html#how-this-book-is-organized",
    "title": "The Data Science Universe",
    "section": "",
    "text": "Data Science and R\nData Exploration and Visualization\nSupervised Learning\nUnsupervised Learning\nBoosting and Random Forest\nNatural Language Processing\nImage Processing/Computer Vision\nReinforcement Learning\nBig Data & Cloud Computing",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "intro.html#why-learning-data-science",
    "href": "intro.html#why-learning-data-science",
    "title": "The Data Science Universe",
    "section": "Why Learning data science?",
    "text": "Why Learning data science?\nAs more and more businesses move towards digitalization, there is a growing demand for professionals who can analyze and make sense of the vast amounts of data that are being generated. This has created a significant shortage of skilled Data Scientists, making it a highly sought-after and well-compensated profession.\nAnother point is that, Data Science is a highly interdisciplinary field that combines knowledge and techniques from statistics, computer science, and domain-specific areas. This means that learning Data Science can enhance your critical thinking skills, improve your ability to solve complex problems, and provide you with a unique set of skills that are highly valued in the job market.",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "intro.html#the-role-of-r",
    "href": "intro.html#the-role-of-r",
    "title": "The Data Science Universe",
    "section": "The Role of R",
    "text": "The Role of R\nR is a programming language that is widely used in the field of Data Science. Its role in Data Science is multifaceted and can be summarized as follows:\n\nData Wrangling: R has a powerful set of libraries that allow you to manipulate and transform data, which is a critical step in any Data Science project.\nStatistical Analysis: R has a rich set of statistical libraries that allow you to perform a wide range of statistical analyses, including hypothesis testing, regression analysis, and time series analysis.\nData Visualization: R has an extensive set of libraries for creating high-quality data visualizations, such as plots, charts, and graphs, that enable you to communicate insights effectively.\nMachine Learning: R has a comprehensive set of libraries for building and deploying machine learning models, such as decision trees, random forests, and neural networks.\nReproducibility: R provides a framework for creating reproducible data analyses, which is essential for collaborating with others and ensuring that your work can be verified and replicated.\n\nOverall, R plays a critical role in the Data Science process by providing a powerful and flexible toolset for manipulating, analyzing, and visualizing data, building and deploying machine learning models, and ensuring reproducibility.",
    "crumbs": [
      "The Data Science Universe"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Data Science and R",
    "section": "",
    "text": "In this first part of the book, our focus will be on the the three following chapters:\n\nIn 1¬† Understanding Data Science As a Career we start with an introduction to the world of Data Science and its importance in the our current world.\nIn 2¬† Understanding R Applications in Data Science we unpack the role of R in the field of Data Science.\nIn 3¬† R Objects And Variables we discuss some important data concepts that are frequently used in this course.",
    "crumbs": [
      "Data Science and R"
    ]
  },
  {
    "objectID": "understanding-ds.html",
    "href": "understanding-ds.html",
    "title": "1¬† Understanding Data Science As a Career",
    "section": "",
    "text": "1.1 State of Data Science\nData Science is an interdisciplinary field that involves using scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It involves collecting and processing large amounts of data, analyzing it to identify patterns and trends, and using those insights to make informed business decisions. A Data Scientist is a professional who has expertise in data analysis, machine learning, statistics, programming, and domain knowledge. They use various statistical and computational techniques to analyse large, complex datasets and derive valuable insights that help organisations make informed decisions. It can be used to optimize operations, improve customer experiences, and develop new products and services. By leveraging Data Science techniques, businesses can gain a competitive edge by making more informed decisions and quickly adapting to changing market conditions.\nData Science involves several stages, including data collection, data cleaning, data preprocessing, exploratory data analysis, modelling, and evaluation. Data Scientists work with various tools and technologies such as Python, R, SQL, Hadoop, Spark, and Tableau, to name a few.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Understanding Data Science As a Career</span>"
    ]
  },
  {
    "objectID": "understanding-ds.html#difference-among-popular-data-careers",
    "href": "understanding-ds.html#difference-among-popular-data-careers",
    "title": "1¬† Understanding Data Science As a Career",
    "section": "\n1.2 Difference among popular data careers?",
    "text": "1.2 Difference among popular data careers?\nWhile Data Science is a broad field, there are several roles that one can pursue within it. Here are some of the key differences between Data Engineer, Data Analyst, BI Analyst, and Statistician:\n\n\n\n\n\n\n\n\nCareer\nJob Responsibilities\nRequired Skills\nTools/Technologies\n\n\n\nData Scientist\nDevelop and apply statistical or machine learning models to solve business problems; collect, clean, and analyze large datasets; communicate insights to stakeholders\nStrong analytical and problem-solving skills; knowledge of statistics, machine learning, and programming languages (Python, R, SQL); familiarity with data visualization techniques\nPython, R, SQL, Hadoop, Spark, Tableau, SAS\n\n\nData Engineer\nBuild and maintain data pipelines and infrastructure to support data analysis and machine learning; work with large datasets and distributed systems\nStrong programming skills; expertise in databases, data warehousing, and ETL (extract, transform, load) processes; knowledge of cloud computing\nHadoop, Spark, SQL, NoSQL databases, AWS, Azure\n\n\nData Analyst\nCollect and analyze data to identify trends, patterns, and insights; communicate findings to stakeholders\nStrong analytical and problem-solving skills; knowledge of statistics and data visualization techniques; proficiency in Excel\nExcel, SQL, Tableau, Power BI\n\n\nBI Analyst\nDesign and develop business intelligence solutions to support decision-making processes; gather and analyze data from multiple sources; create dashboards and reports\nStrong analytical and problem-solving skills; expertise in databases and data modeling; knowledge of data visualization techniques\nTableau, Power BI, SQL, Excel\n\n\nStatistician\nDesign and conduct experiments to collect and analyze data; develop statistical models to explain and predict phenomena; communicate findings to stakeholders\nStrong analytical and problem-solving skills; expertise in statistical theory and methods; knowledge of programming languages (Python, R, SAS)\nPython, R, SAS, SPSS\n\n\n\nWhile there is some overlap between these roles, each one requires a specific set of skills and expertise. Data Science as a field offers a wide range of career opportunities, and individuals can choose a role that aligns with their interests and strengths.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Understanding Data Science As a Career</span>"
    ]
  },
  {
    "objectID": "understanding-ds.html#why-learning-data-science",
    "href": "understanding-ds.html#why-learning-data-science",
    "title": "1¬† Understanding Data Science As a Career",
    "section": "\n1.3 Why Learning data science?",
    "text": "1.3 Why Learning data science?\nAs more and more businesses move towards digitalization, there is a growing demand for professionals who can analyze and make sense of the vast amounts of data that are being generated. This has created a significant shortage of skilled Data Scientists, making it a highly sought-after and well-compensated profession.\nAnother point is that,Data Science is a highly interdisciplinary field that combines knowledge and techniques from statistics, computer science, and domain-specific areas. This means that learning Data Science can enhance your critical thinking skills, improve your ability to solve complex problems, and provide you with a unique set of skills that are highly valued in the job market.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Understanding Data Science As a Career</span>"
    ]
  },
  {
    "objectID": "understanding-r-applications.html",
    "href": "understanding-r-applications.html",
    "title": "2¬† Understanding R Applications in Data Science",
    "section": "",
    "text": "2.1 The Role of R\nR is a programming language that is widely used in the field of Data Science. Its role in Data Science is multifaceted and can be summarized as follows:",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Understanding R Applications in Data Science</span>"
    ]
  },
  {
    "objectID": "understanding-r-applications.html#the-role-of-r",
    "href": "understanding-r-applications.html#the-role-of-r",
    "title": "2¬† Understanding R Applications in Data Science",
    "section": "",
    "text": "Data Wrangling: R has a powerful set of libraries that allow you to manipulate and transform data, which is a critical step in any Data Science project.\nStatistical Analysis: R has a rich set of statistical libraries that allow you to perform a wide range of statistical analyses, including hypothesis testing, regression analysis, and time series analysis.\nData Visualization: R has an extensive set of libraries for creating high-quality data visualizations, such as plots, charts, and graphs, that enable you to communicate insights effectively.\nMachine Learning: R has a comprehensive set of libraries for building and deploying machine learning models, such as decision trees, random forests, and neural networks.\nReproducibility: R provides a framework for creating reproducible data analyses, which is essential for collaborating with others and ensuring that your work can be verified and replicated.\n\n\n2.1.1 R v.s. Python\n\n\n\n\n\n\n\nFigure¬†2.1: R vs.¬†Python\n\n\n\n\nYou may wondering why we choose R over another popular language - Python, in this course. The short answer is the choice ultimately depends on the specific needs of the data scientist and the project at hand.\nAsk yourself: what kind of data scientist you want to become? R is hands down the best option when you focus on statistics and probabilities. It has a large community of statisticians that can answer your questions. But, if you want to develop applications that process enormous amounts of data, Python is your best option. It has a more extensive ecosystem of developers, and it‚Äôs easier to find people willing to collaborate with you.\nTechnical Differences 1. Syntax: R has a syntax that is tailored for statistical analysis and modelling, with many built-in functions and operators specifically designed for this purpose. Python, on the other hand, has a more general-purpose syntax that can be used for a wide range of tasks beyond statistical analysis.\n\nLibraries and Packages: Both R and Python have extensive libraries and packages for data science, but they differ in their focus and scope. R has a strong emphasis on statistical modelling and analysis, with packages like ggplot2, dplyr, and tidyr. Python, on the other hand, has a broader range of applications, including web development, scientific computing, and machine learning, with packages like NumPy, Pandas, and Scikit-learn.\nCommunity: Both R and Python have large and active communities, but they differ in their backgrounds and focus. R has historically been used more by statisticians and data analysts, while Python has been more popular among software engineers and developers.\nLearning Curve: R is generally considered to have a steeper learning curve than Python, especially for those who are new to programming. However, once you become familiar with R‚Äôs syntax and packages, it can be a very powerful and efficient tool for statistical analysis.\nVisualisation: R has a strong focus on Visualisation, with packages like ggplot2 and lattice that make it easy to create high-quality plots and charts. Python also has Visualisation packages like Matplotlib and Seaborn, but they may require more customisation and tweaking to get the desired output.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Understanding R Applications in Data Science</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html",
    "href": "r-objects-and-variables.html",
    "title": "3¬† R Objects And Variables",
    "section": "",
    "text": "3.1 Basic R Objects\nIn this section, we will explore the fundamental building blocks of R programming, starting with the basic R objects. These objects serve as the foundation for data manipulation and analysis in R. We will delve into five key types of R objects: vectors, matrices, lists, data frames, and functions. Understanding these essential data structures is crucial for anyone looking to harness the power of R for data science, statistics, and programming tasks.",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html#basic-r-objects",
    "href": "r-objects-and-variables.html#basic-r-objects",
    "title": "3¬† R Objects And Variables",
    "section": "",
    "text": "3.1.1 Vector\nSequence of data elements of the same type. Each element of the vector is also called a component, member, or value. Vectors are created in R using the c() function, which stands for combine, and coerces all of its arguments into a single type. The coercion will happen from simpler types into more complex types. That is, if we create a vector which contains logicals, numerics, and characters, as the following example shows, our resulting vector will only contain characters, which are the more complex of the three types. If we create a vector that contains logicals and numerics, our resulting vector will be numeric, again because it‚Äôs the more complex type.\n\nmy_vector &lt;- c(TRUE, FALSE, -1, 0, 1, \"A\", \"B\", NA, NULL, NaN, Inf)\nmy_vector\n#&gt;  [1] \"TRUE\"  \"FALSE\" \"-1\"    \"0\"     \"1\"     \"A\"     \"B\"     NA      \"NaN\"  \n#&gt; [10] \"Inf\"\n\n## Find the first element of `my_vector`\npaste(\"the first element of `my_vector` is:\", my_vector[1])\n#&gt; [1] \"the first element of `my_vector` is: TRUE\"\n\n## Find the 5th element of `my_vector`\npaste(\"the 5th element of `my_vector` is:\", my_vector[5])\n#&gt; [1] \"the 5th element of `my_vector` is: 1\"\n\n## Find the firt 3 elements\npaste(\"the firt 3 elements of `my_vector` are:\", my_vector[1:3])\n#&gt; [1] \"the firt 3 elements of `my_vector` are: TRUE\" \n#&gt; [2] \"the firt 3 elements of `my_vector` are: FALSE\"\n#&gt; [3] \"the firt 3 elements of `my_vector` are: -1\"\n\n\n3.1.2 Matrix\nMatrices are commonly used in mathematics and statistics, and much of R‚Äôs power comes from the various operations you can perform with them. In R, a matrix is a vector with two additional attributes, the number of rows and the number of columns. And, since matrices are vectors, they are constrained to a single data type.\nYou can use the matrix function to create matrices. You may pass it a vector of values, as well as the number of rows and columns the matrix should have. If you specify the vector of values and one of the dimensions, the other one will be calculated for you automatically to be the lowest number that makes sense for the vector you passed. However, you may specify both of them simultaneously if you prefer, which may produce different behavior depending on the vector you passed, as can be seen in the next example.\nBy default, matrices are constructed column-wise, meaning that the entries can be thought of as starting in the upper-left corner and running down the columns. However, if you prefer to construct it row-wise, you can send the byrow = TRUE parameter.\n\n# Creating a matrix\nmy_mat &lt;- matrix(1:6, nrow = 2, byrow = TRUE)\nmy_mat\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    2    3\n#&gt; [2,]    4    5    6\n# Find the element in row 1 and column 2\nmy_mat[1, 2]\n#&gt; [1] 2\n# Find the elements in row 1 & 2 and column 3\nmy_mat[c(1, 2), 3]\n#&gt; [1] 3 6\n# Find all the elements in column 2\nmy_mat[, 2]\n#&gt; [1] 2 5\n\n\n3.1.3 List\nA list is an ordered collection of objects, like vectors, but lists can actually combine objects of different types. List elements can contain any type of object that exists in R, including data frames and functions. Lists play a central role in R due to their flexibility and they are the basis for data frames, object-oriented programming, and other constructs.\nUsing the function list() helps to explicitly a list. It takes an arbitrary number of arguments, and we can refer to each of those elements by both position, and, in case they are specified, also by names. If you want to reference list elements by names, you can use the $ notation.\n\n# Creating a list\nmy_list &lt;- list(A = 1, B = \"A\", C = TRUE, D = matrix(1:4, nrow = 2), \nZ = function(x) x^2)\n\n# Retrieve the class of each element in the list\nlapply(my_list, class)\n#&gt; $A\n#&gt; [1] \"numeric\"\n#&gt; \n#&gt; $B\n#&gt; [1] \"character\"\n#&gt; \n#&gt; $C\n#&gt; [1] \"logical\"\n#&gt; \n#&gt; $D\n#&gt; [1] \"matrix\" \"array\" \n#&gt; \n#&gt; $Z\n#&gt; [1] \"function\"\n\n# Perform calculation\nmy_list$Z(2)\n#&gt; [1] 4\n\n\n3.1.4 DataFrame\nA data frame is a natural way to represent such heterogeneous tabular data. Every element within a column must be of the same type, but different elements within a row may be of different types, that‚Äôs why we say that a data frame is a heterogeneous data structure.\nData frames are usually created by reading in a data using the read.table(), read.csv, or other similar data-loading functions. However, they can also be created explicitly with the data.frame function or they can be coerced from other types of objects such as lists. To create a data frame using the data.frame function, note that we send a vector (which, as we know, must contain elements of a single type) to each of the column names we want our data frame to have, which are A, B, and C in this case.\n\n# Creating a dataframe\nmy_dataframe &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  height = c(1.6, 1.8, 1.7)\n)\n        \n# Accessing a column of the dataframe\nmy_dataframe$name\n#&gt; [1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n\n3.1.5 Function\nA function is an object that takes other objects as inputs, called arguments, and returns an output object. Most functions are in the following form f(arg_1, arg_2, ...), where f is the name of the function and arg_1, arg_2 are the arguments to the function.\nWe can create our own function by using the function constructor and assign it to a symbol. It takes an arbitrary number of named arguments, which can be used within the body of the function.\nIn the following example, we create a function that calculates the Euclidian distance (https://en.wikipedia.org/wiki/Euclidean_distance) between two numeric vectors, and we show how the order of the arguments can be changed if we use named arguments. To realize this effect, we use the print function to make sure we can see in the console what R is receiving as the x and y vectors. When developing your own programs, using the print for debugging your function.\n\n# Creating l2_norm function\nl2_norm &lt;- function(x, y) {\n  print(\"value of x:\")\n  print(x)\n  print(\"value of y:\")\n  print(y)\n  num_diff &lt;- x - y\n  res &lt;- sum(num_diff^2)\n  return(res)\n}\n\na &lt;- 1:3\nb &lt;- 4:6\n\nl2_norm(a, b)\n#&gt; [1] \"value of x:\"\n#&gt; [1] 1 2 3\n#&gt; [1] \"value of y:\"\n#&gt; [1] 4 5 6\n#&gt; [1] 27",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html#type-of-data",
    "href": "r-objects-and-variables.html#type-of-data",
    "title": "3¬† R Objects And Variables",
    "section": "\n3.2 Type of Data",
    "text": "3.2 Type of Data\nA variable is a characteristic of the population (or sample) being studied, and it is possible to measure, count, and categorize it. The type of variable collected is crucial in the calculation of descriptive statistics and the graphical representation of results as well as in the selection of the statistical methods that will be used to analyze the data.\n\n3.2.1 Continuous Data\nIt refers to a type of numerical data that can take on any value within a specific range or interval. This type of data is measured on a continuous scale, meaning that there are no gaps or interruptions between values. Continuous data is often obtained through measurements or observations that are recorded as real numbers, such as weight, height, time, temperature, and distance.\n\n# Creating a vector of continuous data\nmy_data &lt;- c(1.2, 2.5, 3.1, 4.8, 5.0)\n\n# Calculating mean and standard deviation\nmean(my_data)\n#&gt; [1] 3.32\nsd(my_data)\n#&gt; [1] 1.599062\n\n\n3.2.2 Discrete Data\nUnlike Continuous Data, discrete data is numeric data that which can only take on certain values within a specific range. For example, the number of kids (or trees, or animals) has to be a whole integer.\nSuppose we have a dataset of the number of students in a class, where each value represents a count of a specific number of students:\n\nstudents &lt;- c(20, 25, 22, 18, 20, 23, 21, 19, 22, 20)\n\n# Calculating the frequency of each value\ntable(students)\n#&gt; students\n#&gt; 18 19 20 21 22 23 25 \n#&gt;  1  1  3  1  2  1  1\n# Calculate proportions\nprop.table(students)\n#&gt;  [1] 0.09523810 0.11904762 0.10476190 0.08571429 0.09523810 0.10952381\n#&gt;  [7] 0.10000000 0.09047619 0.10476190 0.09523810\n\n\n3.2.3 Categorical Data\nCategorical data, also known as nominal data, is a type of data that consists of categories or groups that cannot be ordered or ranked. In R, categorical data is typically represented as a factor variable.\nThe factor() function is used to convert the vector to a factor variable. The levels() function is used to view the categories or levels of the factor variable.\n\n# create a vector of categorical data\ngender &lt;- c(\"male\", \"female\", \"male\", \"male\", \"female\", \"female\")\n\n# convert the vector to a factor\ngender_factor &lt;- factor(gender)\n\n# view the levels of the factor\nlevels(gender_factor)\n#&gt; [1] \"female\" \"male\"\n\n\n3.2.4 Binary Data\nBinary data is categorical data where the only values are 0 and 1. It is often used in situations where a ‚Äúhit‚Äù - an animal getting trapped, a customer clicking a link, etc. - is a 1, and no hit is a 0. In R, binary data can be represented using logical vectors.\nThe class() function is used to confirm that the vector is of logical class, which is the R data type used to represent binary data.\n\n# Create a vector of binary data\nbinary_data &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\n# Check the class of the vector\nclass(binary_data)\n#&gt; [1] \"logical\"\n# Output: \"logical\"\n\n\n3.2.5 Ordinal Data\nOrdinal data is a type of categorical data where each value is assigned a level or rank. It is useful with binned data, but also in graphing to rearrange the order categories are drawn. In R, it is referred to as factors.\n\n# Creating an ordered factor\nmy_factor &lt;- factor(c(\"small\", \"medium\", \"large\"), ordered = TRUE)\n\n# Sorting the levels of the factor\nmy_factor &lt;- factor(my_factor, levels = c(\"small\", \"medium\", \"large\"))\n\nprint(my_factor)\n#&gt; [1] small  medium large \n#&gt; Levels: small &lt; medium &lt; large",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "r-objects-and-variables.html#data-distribution",
    "href": "r-objects-and-variables.html#data-distribution",
    "title": "3¬† R Objects And Variables",
    "section": "\n3.3 Data Distribution",
    "text": "3.3 Data Distribution\nData distribution in R refers to the pattern in which the values of a variable are spread across the range of the variable. In other words, it refers to how often every possible value occurs in a dataset. It is usually shown as a curved line on a graph, or a histogram.\n\n3.3.1 Normal Distribution\nNormal distribution is data where the mean equals the median, 2/3 of the data are within one standard deviation of the mean, 95% of the data are within two standard deviations, and 97% are within three. Many statistical analyses assume your data are normally distributed. However, many datasets - especially in nature - aren‚Äôt.\n\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n# Creating a dataset with normal distribution\nmy_val &lt;- rnorm(100000, mean = 0, sd = 1)\nvec &lt;- c(1:100000)\nmy_data &lt;- data.frame(vec, my_val)\n# Plotting the histogram\np &lt;- ggplot(my_data) + aes(x = my_val) +\n      geom_histogram(aes(y = ..density..), bins = 30L, \n                     fill = \"white\", colour = 1) +\n      theme_linedraw() + geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25)\nggplotly(p)\n#&gt; Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n#&gt; ‚Ñπ Please use `after_stat(density)` instead.\n#&gt; ‚Ñπ The deprecated feature was likely used in the ggplot2 package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/ggplot2/issues&gt;.\n\n\n\n\n\n\n3.3.2 Skewed Distribution\nSkewed distribution is data where the median does not equal the mean. A left-skewed distribution has a long tail on the left side of the graph, while a right-skewed distribution has a long tail to the right. It is named after the tail and not the peak of the graph, as values in that tail occur more often than would be expected with a normal distribution.\n\nlibrary(ggplot2)\nlibrary(plotly)\n# Creating a dataset with normal distribution\nmy_val &lt;- rexp(100000, rate = 0.5)\nvec &lt;- c(1:100000)\nmy_data &lt;- data.frame(vec, my_val)\n# Plotting the histogram\np &lt;- ggplot(my_data) + aes(x = my_val) +\n      geom_histogram(aes(y = ..density..), bins = 30L, \n                     fill = \"white\", colour = 1) +\n      theme_linedraw() + geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25)\nggplotly(p)",
    "crumbs": [
      "Data Science and R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R Objects And Variables</span>"
    ]
  },
  {
    "objectID": "data-exploration-vis.html",
    "href": "data-exploration-vis.html",
    "title": "Data Exploration And Visualisation",
    "section": "",
    "text": "In this second part of the book, our focus will be on the the five following chapters:\n\nIn 4¬† Basics Of Statistics Using R we will discuss some of the statistical terms commonly used in data science and provide code examples in R.\nIn 5¬† Data Manipulation Using R we will pay special attention to two of the most used R libraries: dplyr and data.table.\nIn 6¬† Feature Engineering for Tabular Data we will understand how to perform feature engineering using R.\nIn 7¬† Handling Data Issues we will understand how to handle various data problems using R.\nIn 8¬† Data Visualisation Using ggplot2 we will look into the awesome R package ggplot2 for creating awesome graphics.",
    "crumbs": [
      "Data Exploration And Visualisation"
    ]
  },
  {
    "objectID": "basic-stats-r.html",
    "href": "basic-stats-r.html",
    "title": "4¬† Basics Of Statistics Using R",
    "section": "",
    "text": "4.1 Hypothesis Testing\nHypothesis testing is a statistical tool used to compare the null hypothesis to an alternative hypothesis. The null hypothesis typically states that two quantities are equivalent, while the alternative hypothesis in a two-tailed test is that the quantities are different. In contrast, the alternative hypothesis in a one-tailed test is that one quantity is larger or smaller than the other.\nIn business, hypothesis testing is almost never used to determine if one variable causes another but rather if one variable can predict the other. To conduct a hypothesis test in R, we can use the t.test() function:.\n# Create two sample datasets\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(6, 7, 8, 9, 10)\n\n# Conduct a two-sample t-test\nt.test(x, y)\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x and y\n#&gt; t = -5, df = 8, p-value = 0.001053\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -7.306004 -2.693996\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;         3         8",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Basics Of Statistics Using R</span>"
    ]
  },
  {
    "objectID": "basic-stats-r.html#p-value",
    "href": "basic-stats-r.html#p-value",
    "title": "4¬† Basics Of Statistics Using R",
    "section": "\n4.2 P-value",
    "text": "4.2 P-value\nThe p-value is the probability of observing an effect of the same size as our results given a random model. High p-values often mean that our independent variables are irrelevant. However, low p-values don‚Äôt necessarily mean they‚Äôre important, and examining the effect size and importance is crucial to determine significance.\nThe traditional significance threshold is a p-value of 0.05, but this is an arbitrary cut-off point. There‚Äôs no reason to set a line in the sand for significance. A p-value of 0.05 means that there‚Äôs a 1 in 20 probability your result could be random chance, and a p-value of 0.056 means it‚Äôs 1 in 18. Those are almost identical odds. Some journals have banned their use altogether, while others still only accept significant results.\nTo calculate the p-value in R, we can use the t.test() function.\n\n# Create two sample datasets\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(6, 7, 8, 9, 10)\n\n# Conduct a two-sample t-test\nt.test(x, y)$p.value\n#&gt; [1] 0.001052826",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Basics Of Statistics Using R</span>"
    ]
  },
  {
    "objectID": "basic-stats-r.html#estimates-and-statistics",
    "href": "basic-stats-r.html#estimates-and-statistics",
    "title": "4¬† Basics Of Statistics Using R",
    "section": "\n4.3 Estimates and Statistics",
    "text": "4.3 Estimates and Statistics\nn\nn refers to the number of observations in a dataset or level of a categorical variable. In R, nrow(dataframe) can be used to calculate the number of rows in a dataframe or length(Vector) to calculate the length of a vector. To calculate the number of observations by group, the count(Data, GroupingVariable) function can be used. For example, nrow(iris), length(iris$Sepal.Length), and count(iris, Species) can be used to find the number of observations in the iris dataset.\nMean\nmean refers to the average of a dataset, defined as the sum of all observations divided by the number of observations. In R, mean(Vector) can be used to calculate the mean of a vector. For example, mean(iris$Sepal.Length) can be used to find the mean sepal length of the iris dataset.\nTrimmed Mean\nTrimmed Mean refers to the mean of a dataset with a certain proportion of data not included. The highest and lowest values are trimmed - for instance, the 10% trimmed mean will use the middle 80% of your data. To calculate the trimmed mean in R, mean(Vector, trim = 0.##) can be used. For example, mean(iris$Sepal.Length, trim = 0.10) can be used to find the 10% trimmed mean sepal length of the iris dataset.\nVariance\nVariance refers to a measure of the spread of your data. In R, var(Vector) can be used to calculate the variance of a vector. For example, var(iris$Sepal.Length) can be used to find the variance of the sepal length in the iris dataset.\nStandard Deviation\nStandard Deviation refers to the amount any observation can be expected to differ from the mean. In R, sd(Vector) can be used to calculate the standard deviation of a vector. For example, sd(iris$Sepal.Length) can be used to find the standard deviation of the sepal length in the iris dataset.\nStandard Error\nStandard Error refers to the error associated with a point estimate (e.g.¬†the mean) of the sample. If you‚Äôre reporting the mean of a sample variable, use the SD. If you‚Äôre putting error bars around means on a graph, use the SE. In R, there is no native function to calculate the standard error. However, it can be calculated using sd(Vector)/sqrt(length(Vector)). For example, sd(iris$Sepal.Length)/sqrt(length(iris$Sepal.Length)) can be used to find the standard error of the sepal length in the iris dataset.\nMedian\nMedian refers to a robust estimate of the center of the data. In R, median(Vector) can be used to calculate the median of a vector. For example, median(iris$Sepal.Length) can be used to find median of the data.\nMinimum\nThe min() function in R calculates the smallest value in a vector.\nMaximum\nThe max() function in R calculates the largest value in a vector.\nRange\nThe range of a dataset is the difference between the maximum and minimum values.\nQuantile\nA quantile is a point in a distribution that divides the data into equally sized subsets. The quantile() function in R can be used to calculate specific quantiles, such as quartiles (the 0.25, 0.5, and 0.75 quantiles). Note that quantiles range from 0 to 1. To convert to percentiles, multiply by 100.\nInterquartile Range\nThe interquartile range (IQR) is a measure of spread that represents the range of the middle 50% of the data. It can be calculated using the IQR() function in R\nSkew\nSkewness measures the asymmetry of a distribution. A skewness value of 0 indicates a symmetrical distribution, while positive and negative skewness values indicate a right-skewed and left-skewed distribution, respectively. The skew() function is not included in base R, but it can be found in various packages such as moments or psych.\nKurtosis\nKurtosis measures the ‚Äúpeakedness‚Äù of a distribution. A kurtosis value of 0 indicates a normal distribution, while positive and negative kurtosis values indicate a more peaked and flatter distribution, respectively. The kurtosis() function is not included in base R, but it can be found in various packages such as moments or psych. Values much different from 0 indicate non-normal distributions.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Basics Of Statistics Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html",
    "href": "data-manipulation.html",
    "title": "5¬† Data Manipulation Using R",
    "section": "",
    "text": "5.1 Overview\nData manipulation in data science refers to the process of transforming and modifying raw data to make it more suitable for analysis, modeling, and decision-making. It involves various operations such as filtering, sorting, aggregating, joining, reshaping, and creating derived variables. The goal of data manipulation is to organize, clean, and prepare data in a format that facilitates extracting meaningful insights and patterns.\nData manipulation is a critical step in the data science workflow for several reasons:\nAs a data scientist, you will find most of your time will be spent on exploring your data.\nFigure¬†5.1: The role of a data scientist and why we need them (www.raconteur.net/the-role-of-a-data-scientist-and-why-we-need-them)",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#overview",
    "href": "data-manipulation.html#overview",
    "title": "5¬† Data Manipulation Using R",
    "section": "",
    "text": "Data cleaning: Raw data often contains errors, missing values, outliers, or inconsistencies. Data manipulation allows for identifying and addressing these issues through techniques like data validation, imputation, removal of duplicates, and handling missing values. Clean data is essential for accurate analysis and modeling.\nFeature engineering: Data manipulation enables the creation of new variables (features) based on existing data, which can capture important patterns and relationships. Feature engineering involves operations such as transforming variables, generating interaction terms, extracting time-based features, and encoding categorical variables. Well-engineered features can significantly enhance the performance of machine learning models.\nData integration: Data manipulation facilitates combining data from multiple sources or tables through operations like merging, joining, or concatenating. This is particularly useful when working with relational databases or when integrating data from different systems. By combining disparate data sources, analysts can gain a comprehensive view and uncover insights that may not be apparent when examining individual datasets.\nData aggregation: Aggregating data involves summarizing or grouping data to derive higher-level insights. Data manipulation allows for aggregating data by factors such as time periods, geographic regions, or customer segments. Aggregating data can provide valuable summary statistics, identify trends, and support decision-making processes.\nData reshaping: Data manipulation allows for transforming data from one format to another, such as pivoting data from a wide format to a long format or vice versa. Reshaping data is useful for various analytical tasks, including data visualization, time series analysis, and data modeling. It enables data to be structured in a way that is most suitable for the analysis at hand.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#data-manipulation-with-dplyr",
    "href": "data-manipulation.html#data-manipulation-with-dplyr",
    "title": "5¬† Data Manipulation Using R",
    "section": "\n5.2 Data Manipulation with dplyr\n",
    "text": "5.2 Data Manipulation with dplyr\n\nThe R package dplyr is a powerful and popular package for data manipulation and transformation. It provides a set of functions that offer a consistent and intuitive syntax to perform common data manipulation tasks. dplyr focuses on data frames as the primary data structure and aims to make data manipulation more efficient and user-friendly.\nHere‚Äôs a brief overview of how to use dplyr:\n\nCreating a data frame: You can create a data frame or load one from external sources (e.g., CSV files) using the data.frame() function or read.csv() function, respectively.\nData manipulation: dplyr provides a set of core functions that enable efficient and readable data manipulation. Some commonly used functions include:\n\n\nSelecting columns: select(df, col1, col2)\n\nFiltering rows: filter(df, condition)\n\nAdding or modifying columns: mutate(df, new_col = expression)\n\nArranging rows: arrange(df, col)\n\nGrouping data: group_by(df, group_col)\n\nSummarizing data: summarize(df, new_col = function(col))\n\nJoining data: inner_join(df1, df2, by = \"key_col\")\n\n\n\nPiping %&gt;% operator: dplyr utilizes the %&gt;% operator from the magrittr package, allowing you to chain multiple operations together in a clear and readable manner. This ‚Äúpipe‚Äù operator enhances code readability and reduces the need for intermediate variables.\nEfficiency and performance: dplyr is designed to be efficient, leveraging optimized C++ code and lazy evaluation. It also integrates well with other packages like dbplyr for connecting to databases and working with large datasets.\n\nHere is an example:\n\n# Load dplyr package\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Create a data frame\ndf &lt;- data.frame(\n  CustomerID = c(1, 1, 2, 2, 3),\n  Date = as.Date(c(\"2023-01-01\", \"2023-01-05\", \"2023-01-02\", \"2023-01-06\", \"2023-01-03\")),\n  Product = c(\"A\", \"B\", \"A\", \"C\", \"B\"),\n  Quantity = c(10, 5, 7, 3, 8)\n)\n\n# Convert data frame to tibble (optional)\ndf &lt;- as_tibble(df)\n\n# Subset data and calculate total quantity by product\ndf_subset &lt;- df %&gt;%\n  filter(Date &gt;= as.Date(\"2023-01-02\"), Date &lt;= as.Date(\"2023-01-05\")) %&gt;%\n  select(-CustomerID)  # Exclude CustomerID column from output\ndf_summary &lt;- df_subset %&gt;%\n  group_by(Product) %&gt;%\n  summarize(TotalQuantity = sum(Quantity))\n\nhead(df_summary, 4)\n#&gt; # A tibble: 2 √ó 2\n#&gt;   Product TotalQuantity\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 A                   7\n#&gt; 2 B                  13",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#data-manipulation-with-data.table",
    "href": "data-manipulation.html#data-manipulation-with-data.table",
    "title": "5¬† Data Manipulation Using R",
    "section": "\n5.3 Data Manipulation with data.table\n",
    "text": "5.3 Data Manipulation with data.table\n\nThe R package data.table is an efficient and powerful extension of the base R data.frame. It provides a high-performance data manipulation tool with syntax and functionality optimized for large datasets. The primary goal of data.table is to offer fast and memory-efficient operations for handling substantial amounts of data.\nHere‚Äôs a brief overview of how to use data.table:\n\nCreating a data.table: You can create a data.table from a data.frame or by directly specifying the data.\nData manipulation: data.table provides concise and fast syntax for various data manipulation tasks, including filtering, sorting, aggregating, joining, and updating data. Some commonly used operations include:\n\n\nSubset rows using conditions: dt[condition]\n\nSelect columns: dt[, c(\"col1\", \"col2\")]\n\nModify or create columns: dt[, new_col := expression]\n\nSort data: dt[order(col)]\n\nAggregate data: dt[, .(mean_col = mean(col)), by = group_col]\n\nJoin data: dt1[dt2, on = \"key_col\"]\n\n\n\nEfficiency: data.table is designed to handle large datasets efficiently. It uses memory-mapped files and optimized algorithms to minimize memory usage and improve performance. Additionally, it provides parallel processing capabilities, allowing you to make use of multiple cores for faster computations.\n\ndata.table is especially beneficial in scenarios where you‚Äôre working with large datasets and need to perform complex data manipulations quickly. It shines in the following scenarios:\n\nBig data analysis: When dealing with datasets that are too large to fit in memory, data.table provides an efficient solution by minimizing memory usage and optimizing performance.\nSpeed optimization: data.table is specifically engineered for fast and scalable operations. It outperforms base R and other packages for tasks like subsetting, aggregating, and merging data.\nTime-series analysis: data.table offers powerful functionality for working with time-series data, such as rolling joins and efficient grouping and aggregation.\nData cleaning and preprocessing: data.table provides concise and efficient syntax for filtering, transforming, and reshaping data, making it ideal for data cleaning and preprocessing tasks.\n\nHere is an example\n\n# Load data.table package\nlibrary(data.table)\n#&gt; \n#&gt; Attaching package: 'data.table'\n#&gt; The following objects are masked from 'package:dplyr':\n#&gt; \n#&gt;     between, first, last\n\n# Create a data.table\ndt &lt;- data.table(\n  CustomerID = c(1, 1, 2, 2, 3),\n  Date = as.Date(c(\"2023-01-01\", \"2023-01-05\", \"2023-01-02\", \"2023-01-06\", \"2023-01-03\")),\n  Product = c(\"A\", \"B\", \"A\", \"C\", \"B\"),\n  Quantity = c(10, 5, 7, 3, 8)\n)\n\n# Subset data and calculate total quantity by product\ndt_subset &lt;- dt[Date &gt;= as.Date(\"2023-01-02\") & Date &lt;= as.Date(\"2023-01-05\")]\ndt_summary &lt;- dt_subset[, .(TotalQuantity = sum(Quantity)), by = Product]\n\nhead(dt_summary, 4)\n#&gt;    Product TotalQuantity\n#&gt;     &lt;char&gt;         &lt;num&gt;\n#&gt; 1:       B            13\n#&gt; 2:       A             7",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "data-manipulation.html#dplyr-vs.-data.table",
    "href": "data-manipulation.html#dplyr-vs.-data.table",
    "title": "5¬† Data Manipulation Using R",
    "section": "\n5.4 dplyr vs.¬†data.table\n",
    "text": "5.4 dplyr vs.¬†data.table\n\nWhen deciding between data.table and dplyr for data manipulation in R, several factors should be considered. Both packages offer powerful functionality, but they have different design philosophies and performance characteristics. Let‚Äôs compare them in terms of syntax, performance, memory usage, functionality, and use cases:\n\n\nSyntax:\n\n\ndplyr has a more intuitive and expressive syntax that closely resembles natural language. Its function names and the %&gt;% pipe operator contribute to readable and concise code.\n\ndata.table has a terser syntax designed for efficiency and speed. It uses square brackets [ ] for subsetting and assignment operations, which can take some time to get used to.\n\n\n\nPerformance:\n\n\ndata.table is specifically optimized for fast data manipulation and performs exceptionally well on large datasets. It uses memory-mapped files, efficient indexing, and parallel processing to achieve high performance.\n\ndplyr performs well for smaller datasets but may face performance limitations when dealing with very large datasets due to its use of in-memory operations.\n\n\n\nMemory usage:\n\n\ndata.table is memory efficient and optimized for handling large datasets by minimizing memory allocations. It uses a ‚Äúby-reference‚Äù approach, which reduces memory duplication and can be useful for memory-constrained environments.\n\ndplyr is more memory intensive, as it generally creates new copies of data frames during each operation. This can be a disadvantage when working with very large datasets that exceed available memory.\n\n\n\nFunctionality:\n\nBoth packages offer similar functionality for data manipulation tasks, including subsetting, filtering, aggregating, and joining data. However, data.table provides additional features like fast grouping, updating columns by reference, and rolling joins, which may not be available or as efficient in dplyr.\n\ndplyr has a broader ecosystem and integrates well with other tidyverse packages, such as tidyr andggplot2`, making it convenient for end-to-end data analysis workflows.\n\n\n\nUse cases:\n\nIf you primarily work with large datasets that require efficient and high-performance operations, data.table is a strong choice. It excels in scenarios involving big data, time-series analysis, and situations where speed is crucial.\nIf you prioritize code readability, prefer a more intuitive and user-friendly syntax, and work with smaller to medium-sized datasets, dplyr is a good fit. It is well-suited for interactive data exploration, data cleaning, and general data analysis tasks.\n\n\n\nIt‚Äôs worth noting that the choice between data.table and dplyr is not mutually exclusive. Both packages can coexist in the same R environment, allowing you to leverage the strengths of each when appropriate. You can even convert between data.table and dplyr objects using functions like as.data.table() and as_tibble().",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Manipulation Using R</span>"
    ]
  },
  {
    "objectID": "feature-eng.html",
    "href": "feature-eng.html",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "",
    "text": "6.1 Overview\nFeature engineering is the process of preparing the data for the learning algorithms.\nFeature engineering encompasses a variety of tasks, including:\nIt is essential to recognize that different learning algorithms have unique feature engineering requirements. A technique that is appropriate for a specific learning algorithm and context may be unnecessary or even detrimental for others. Consequently, there is no universal formula or a single correct approach to feature engineering. As such, it is recommended to experiment with various feature engineering techniques and allow the data to guide decision-making. This approach acknowledges that effective feature engineering requires a deep understanding of the data and the problem at hand. By leveraging data-driven insights, analysts can optimise feature engineering efforts and ultimately improve the performance of machine learning models.\nIn the following sections will describe several feature engineering techniques for tabular data. However, keep in mind that a key aspect of feature engineering is to come up with informative features. That depends on the context, rather than on specific techniques.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#overview",
    "href": "feature-eng.html#overview",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "",
    "text": "Extracting features from raw data;\nConstructing informative features based on domain knowledge;\nProcessing the data into the format required by different learning algorithms;\nProcessing the predictors in ways that help the learning algorithm to build better models;\nImputing missing values.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#feature-scaling-on-numerical-features",
    "href": "feature-eng.html#feature-scaling-on-numerical-features",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "\n6.2 Feature scaling on numerical features",
    "text": "6.2 Feature scaling on numerical features\nMost learning algorithms require the inputs to be on the same scale and sometimes be centred around zero. It is a critical preprocessing step in machine learning, especially for algorithms that are sensitive to the scale of input features. Feature scaling can help to improve model performance by ensuring that all features are on a similar scale and preventing some features from dominating others. Common methods for feature scaling include min-max scaling, z-score normalisation, and log scaling.\nHere‚Äôs an example in R using the iris dataset:\n\n# Load required package\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Load iris dataset\ndata(iris)\n\n# Select numerical features\nnumerical_features &lt;- iris %&gt;% select_if(is.numeric)\n\n# Perform feature scaling using the scale() function\nscaled_features &lt;- scale(numerical_features)\n\n# Convert scaled features back to a data frame\nscaled_df &lt;- as.data.frame(scaled_features)\n\n# Print the first few rows of the scaled dataset\nhead(scaled_df)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; 1   -0.8976739  1.01560199    -1.335752   -1.311052\n#&gt; 2   -1.1392005 -0.13153881    -1.335752   -1.311052\n#&gt; 3   -1.3807271  0.32731751    -1.392399   -1.311052\n#&gt; 4   -1.5014904  0.09788935    -1.279104   -1.311052\n#&gt; 5   -1.0184372  1.24503015    -1.335752   -1.311052\n#&gt; 6   -0.5353840  1.93331463    -1.165809   -1.048667\n\nIn this example, we use the iris dataset and select the numerical features using select_if(is.numeric). The scale() function is then applied to the numerical features to perform feature scaling. It standardizes each feature by subtracting the mean and dividing by the standard deviation. The resulting scaled features are stored in the scaled_features object.\nTo convert the scaled features back to a data frame, we use as.data.frame(scaled_features). Finally, we print the first few rows of the scaled dataset using head(scaled_df).\nAfter applying feature scaling, each numerical feature will have a mean of zero and a standard deviation of one, resulting in comparable scales across all features. This is particularly useful for algorithms that are sensitive to differences in feature scales, such as distance-based algorithms (e.g., k-means clustering) or regularization-based models (e.g., linear regression with L1 or L2 regularization).",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#categorical-features",
    "href": "feature-eng.html#categorical-features",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "\n6.3 Categorical features",
    "text": "6.3 Categorical features\nOne-hot encoding\nOne-hot encoding is a popular approach for encoding nominal variables is one-hot encoding. This technique involves constructing a binary indicator for each category of the nominal variable. For instance, if the nominal variable is colour ‚àà {blue, yellow, red}, we can represent it as a feature vector with binary indicators for each possible value: * x = [1,0,0] if blue, * x = [0,1,0] if yellow, * x = [0,0,1] if red.\nBy encoding nominal variables in this manner, analysts can use them as input features for machine learning algorithms that require numerical input.\nDummy encoding\nDummy encoding is a technique used to convert nominal variables into numerical features, similar to one-hot encoding. We start with one-hot encoding but delete one arbitrary feature to avoid perfect multicollinearity.If we have a predictor gender ‚àà {male, female}, we encode it as * x = 0 if male, * x = 1 if female.\nSparse category\nIn categorical analysis, some categories may have low counts, which can reduce the statistical power of the analysis or complicate interpretation. To address this issue, analysts may choose to merge all categories with a count below a certain minimum into a more general ‚Äúother‚Äù category. This approach is useful for simplifying the analysis and improving interpretability by reducing the number of categories. By aggregating low-count categories into a broader category, analysts can improve the accuracy and reliability of statistical models.\nHigh cardinality\nIn statistical analysis, the term ‚Äúcardinality‚Äù refers to the number of unique values in a variable. High cardinality indicates that there is a large number of categories, which can present challenges in data analysis. To address this issue, several techniques are commonly used: * Merging sparse categories: categories with low frequency can be merged into a more general category, improving the statistical power of the analysis. * Merging similar categories: categories that are similar in meaning can be combined to simplify the analysis and improve interpretability. * Hash encoding: this technique involves hashing the categorical values to a fixed number of bins, reducing the dimensionality of the data while retaining some of the information in the original categories. * Target encoding: this technique involves encoding categorical variables as the average value of the target variable within each category, which can improve the accuracy of predictive models. * Using specialised algorithms: some machine learning algorithms, such as CatBoost, are designed specifically to handle categorical features efficiently and accurately.\nBy employing these techniques, data scientists can effectively handle high-cardinality categorical variables and improve the accuracy and interpretability of statistical models.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(caret)\n#&gt; Loading required package: ggplot2\n#&gt; Loading required package: lattice\nlibrary(Matrix)\n\n# Create a sample data frame\ndataf &lt;- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"D\", \"C\"),\n  label = c(1, 0, 1, 0, 1, 0, 1)\n)\n\n# One-hot encoding using caret package\ndmy &lt;- caret::dummyVars(\" ~ .\", data = dataf)\none_hot_encoded &lt;- data.frame(predict(dmy, newdata = dataf))\nprint(one_hot_encoded)\n#&gt;   categoryA categoryB categoryC categoryD label\n#&gt; 1         1         0         0         0     1\n#&gt; 2         0         1         0         0     0\n#&gt; 3         0         0         1         0     1\n#&gt; 4         1         0         0         0     0\n#&gt; 5         0         1         0         0     1\n#&gt; 6         0         0         0         1     0\n#&gt; 7         0         0         1         0     1\n\n# Dummy encoding using dplyr package\ndummy_encoded &lt;- dataf %&gt;%\n  mutate(category = as.factor(category)) %&gt;%\n  bind_cols(model.matrix(~ category - 1, data = .))\n\n# Print the dummy encoded features\nprint(dummy_encoded)\n#&gt;   category label categoryA categoryB categoryC categoryD\n#&gt; 1        A     1         1         0         0         0\n#&gt; 2        B     0         0         1         0         0\n#&gt; 3        C     1         0         0         1         0\n#&gt; 4        A     0         1         0         0         0\n#&gt; 5        B     1         0         1         0         0\n#&gt; 6        D     0         0         0         0         1\n#&gt; 7        C     1         0         0         1         0\n\n# Handling sparse categories using Matrix package\nsparse_categories &lt;- dataf %&gt;%\n  mutate(category = as.factor(category)) %&gt;%\n  mutate(category = as.character(category))\n\n# Create a sparse matrix of the categories\nsparse_matrix &lt;- sparse.model.matrix(~ category - 1, data = sparse_categories)\n\n# Convert the sparse matrix to a regular matrix\nsparse_encoded &lt;- as.data.frame(as.matrix(sparse_matrix))\n\n# Print the sparse encoded features\nprint(sparse_encoded)\n#&gt;   categoryA categoryB categoryC categoryD\n#&gt; 1         1         0         0         0\n#&gt; 2         0         1         0         0\n#&gt; 3         0         0         1         0\n#&gt; 4         1         0         0         0\n#&gt; 5         0         1         0         0\n#&gt; 6         0         0         0         1\n#&gt; 7         0         0         1         0",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#mixed-features",
    "href": "feature-eng.html#mixed-features",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "\n6.4 Mixed features",
    "text": "6.4 Mixed features\nDifferent machine learning algorithms have varying abilities to handle mixed data types. Tree-based methods, for example, can naturally handle mixed data types, while methods like k-Nearest Neighbours (kNN) work best when all predictors are directly comparable. However, one challenge that arises when working with mixed data types is ensuring that all features are on the same scale. While there are natural ways to scale continuous variables, there is no straightforward way to scale categorical variables, which are often converted to numerical features using techniques such as one-hot encoding or dummy encoding. Despite this issue, in practice, analysts often treat numerical features constructed from categorical predictors in the same way as any other numerical predictor. While some methods may require all features to be on the same scale, this issue is frequently ignored when working with mixed data types.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#interaction-features",
    "href": "feature-eng.html#interaction-features",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "\n6.5 Interaction features",
    "text": "6.5 Interaction features\nAn interaction effect refers to a situation where the relationship between the response variable and a predictor variable is dependent on one or more other predictor variables. In other words, the effect of a predictor on the response varies based on the level or values of other predictors.\nThe model for the illustration is \\(f(x)\\) = \\(\\beta_0\\) + \\(\\beta_1\\) √ó income + \\(\\beta_2\\) √ó student + \\(\\beta_3\\) √ó income √ó student, and student = 1 if the observed individual is student and student = 0 otherwise. \\[\n\\begin{cases}\n    \\text{student} = 1 \\text{ if the observed individual is student}\\\\\n    \\text{student} = 0 \\text{ otherwise}\n\\end{cases}\n\\]\n\n# Create a sample data frame\ndata &lt;- data.frame(\n  feature1 = c(\"A\", \"B\", \"C\"),\n  feature2 = c(\"X\", \"Y\", \"Z\"),\n  label = c(1, 0, 1)\n)\n\n# Create interaction feature\ndata$interaction_feature &lt;- interaction(data$feature1, data$feature2, drop = TRUE)\n\n# Print the data frame with the interaction feature\nprint(data)\n#&gt;   feature1 feature2 label interaction_feature\n#&gt; 1        A        X     1                 A.X\n#&gt; 2        B        Y     0                 B.Y\n#&gt; 3        C        Z     1                 C.Z",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "feature-eng.html#curse-of-dimensionality",
    "href": "feature-eng.html#curse-of-dimensionality",
    "title": "6¬† Feature Engineering for Tabular Data",
    "section": "\n6.6 Curse of Dimensionality",
    "text": "6.6 Curse of Dimensionality\nThe curse of dimensionality refers to the difficulties that arise when working with high-dimensional data, especially in cases where the number of features (i.e., dimensions) greatly exceeds the number of observations. When dealing with interaction features, which are constructed by combining multiple predictors, the number of features can grow rapidly, exacerbating the problems associated with the curse of dimensionality. For instance:\n\nWhen \\(p=2\\) predictors, there is one possible interaction: \\(f_1(x_1, x_2)\\).\nWhen \\(p = 3\\), there are four: \\(f_1(x_1, x_2),\\text{ }f_2(x_1, x_3),\\text{ }f_3(x_2, x_3)\\), and \\(f_4(x_1, x_2, x_3)\\).\nWhen \\(p = 4\\), there are 11 combinations.\nWhen \\(p = 10\\), there are 1013 combinations.\nWhen \\(p = 50\\), there are there are 1,125,899,906,842,573 combinations.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Feature Engineering for Tabular Data</span>"
    ]
  },
  {
    "objectID": "handling-data-issues.html",
    "href": "handling-data-issues.html",
    "title": "7¬† Handling Data Issues",
    "section": "",
    "text": "7.1 Missing values\nIn dealing with missing values, several methods can be employed: * Removing rows that contain missing values should be avoided since it could lead to the loss of valuable information. * Similarly, removing columns that have too many missing values should also be avoided. * Simple imputation methods such as replacing missing values with the mean, median, or mode can be used, although this may result in biased estimates. * More sophisticated imputation methods such as building a model to predict missing values can be employed. * Another approach is to create a dummy variable that indicates the presence of missing values. In some cases, the fact that data is missing may carry valuable information. * Lastly, some learning algorithms can handle missing values directly, making it unnecessary to perform any imputation.\n# Load required package\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n# Create a sample data frame with missing values\ndata &lt;- data.frame(\n  feature1 = c(1, 2, NA, 4),\n  feature2 = c(\"A\", NA, \"C\", \"D\"),\n  feature3 = c(5, 6, 7, NA),\n  label = c(1, 0, 1, 0)\n)\n\n# Removing rows with missing values\ndata_without_missing_rows &lt;- na.omit(data)\n\n# Removing columns with too many missing values\ndata_without_missing_cols &lt;- data %&gt;% \n  select_if(function(x) sum(is.na(x)) &lt; 0.5 * nrow(data))\n\n# Imputation using mean, median, or mode\nimputed_data &lt;- data %&gt;%\n  mutate(\n    feature1_imputed = ifelse(is.na(feature1), mean(feature1, na.rm = TRUE), feature1),\n    feature2_imputed = ifelse(is.na(feature2), median(feature2, na.rm = TRUE), feature2),\n    feature3_imputed = ifelse(is.na(feature3), as.character(mode(feature3)), feature3)\n  )\n\n# Imputation using model-based approach\nlibrary(mice)\n#&gt; \n#&gt; Attaching package: 'mice'\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     cbind, rbind\nimputed_data_model &lt;- complete(mice(data))\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  feature1\n#&gt;   1   2  feature1\n#&gt;   1   3  feature1\n#&gt;   1   4  feature1\n#&gt;   1   5  feature1\n#&gt;   2   1  feature1\n#&gt;   2   2  feature1\n#&gt;   2   3  feature1\n#&gt;   2   4  feature1\n#&gt;   2   5  feature1\n#&gt;   3   1  feature1\n#&gt;   3   2  feature1\n#&gt;   3   3  feature1\n#&gt;   3   4  feature1\n#&gt;   3   5  feature1\n#&gt;   4   1  feature1\n#&gt;   4   2  feature1\n#&gt;   4   3  feature1\n#&gt;   4   4  feature1\n#&gt;   4   5  feature1\n#&gt;   5   1  feature1\n#&gt;   5   2  feature1\n#&gt;   5   3  feature1\n#&gt;   5   4  feature1\n#&gt;   5   5  feature1\n#&gt; Warning: Number of logged events: 2\n\n# Creating a dummy variable for missing values\ndata_with_dummy &lt;- data %&gt;%\n  mutate(\n    feature1_missing = ifelse(is.na(feature1), 1, 0),\n    feature2_missing = ifelse(is.na(feature2), 1, 0),\n    feature3_missing = ifelse(is.na(feature3), 1, 0)\n  )\n\n# Learning algorithms handling missing values\nlibrary(randomForest)\n#&gt; randomForest 4.7-1.1\n#&gt; Type rfNews() to see new features/changes/bug fixes.\n#&gt; \n#&gt; Attaching package: 'randomForest'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     combine\nrf_model &lt;- randomForest(label ~ ., data = data, na.action = na.exclude)\n#&gt; Warning in randomForest.default(m, y, ...): The response has five or fewer\n#&gt; unique values.  Are you sure you want to do regression?\n\n# Print the modified data frames\nprint(data_without_missing_rows)\n#&gt;   feature1 feature2 feature3 label\n#&gt; 1        1        A        5     1\nprint(data_without_missing_cols)\n#&gt;   feature1 feature2 feature3 label\n#&gt; 1        1        A        5     1\n#&gt; 2        2     &lt;NA&gt;        6     0\n#&gt; 3       NA        C        7     1\n#&gt; 4        4        D       NA     0\nprint(imputed_data)\n#&gt;   feature1 feature2 feature3 label feature1_imputed feature2_imputed\n#&gt; 1        1        A        5     1         1.000000                A\n#&gt; 2        2     &lt;NA&gt;        6     0         2.000000                C\n#&gt; 3       NA        C        7     1         2.333333                C\n#&gt; 4        4        D       NA     0         4.000000                D\n#&gt;   feature3_imputed\n#&gt; 1                5\n#&gt; 2                6\n#&gt; 3                7\n#&gt; 4          numeric\nprint(imputed_data_model)\n#&gt;   feature1 feature2 feature3 label\n#&gt; 1        1        A        5     1\n#&gt; 2        2     &lt;NA&gt;        6     0\n#&gt; 3        4        C        7     1\n#&gt; 4        4        D       NA     0\nprint(data_with_dummy)\n#&gt;   feature1 feature2 feature3 label feature1_missing feature2_missing\n#&gt; 1        1        A        5     1                0                0\n#&gt; 2        2     &lt;NA&gt;        6     0                0                1\n#&gt; 3       NA        C        7     1                1                0\n#&gt; 4        4        D       NA     0                0                0\n#&gt;   feature3_missing\n#&gt; 1                0\n#&gt; 2                0\n#&gt; 3                0\n#&gt; 4                1",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling Data Issues</span>"
    ]
  },
  {
    "objectID": "handling-data-issues.html#outliers",
    "href": "handling-data-issues.html#outliers",
    "title": "7¬† Handling Data Issues",
    "section": "\n7.2 Outliers",
    "text": "7.2 Outliers\nOutliers are values that are significantly different from the other values in a dataset. They can have a large effect on statistical estimates and machine learning models. Here are some common approaches for dealing with outliers: * Avoid deleting outliers unless they are clearly due to measurement or data entry errors. Outliers may contain valuable information about the data or the underlying process generating the data. * If an outlier is due to an error, try to fix the error if possible. If there is no way to correct the error, treat the outlier as a missing value. * Censoring involves setting a threshold value for a variable and any values beyond the threshold are set to the threshold value. This can be useful when extreme values are unlikely but still possible, and they can be safely considered as equivalent to the threshold. * Transforming the variable can sometimes make the data more amenable to analysis or modelling. For example, taking the logarithm of a skewed variable can help to make it more symmetric. * Creating a dummy variable to indicate outliers can be useful when outliers are expected to have a different effect on the response variable than other values. For example, a dummy variable could indicate whether a data point is an outlier or not, and this variable could be used as a predictor in a regression model. * Using a learning algorithm that is robust to outliers can also be effective. For example, robust regression methods can downright the influence of outliers on the model fit.\n\n# Load required packages\nlibrary(dplyr)\nlibrary(car)\n#&gt; Loading required package: carData\n#&gt; \n#&gt; Attaching package: 'car'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     recode\nlibrary(MASS)\n#&gt; \n#&gt; Attaching package: 'MASS'\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     select\n\n# Create a sample data frame with outliers\ndata &lt;- data.frame(\n  variable = c(1, 2, 3, 100, 5, 6, 200),\n  label = c(1, 0, 1, 0, 1, 0, 1)\n)\n\n# Avoid deleting outliers\n# Outliers may contain valuable information, so we don't remove them\n\n# Fix errors or treat outliers as missing values\ndata$variable_fixed &lt;- ifelse(data$variable &gt; 100, NA, data$variable)\n\n# Censoring outliers\nthreshold &lt;- 100\ndata$variable_censored &lt;- ifelse(data$variable &gt; threshold, threshold, data$variable)\n\n# Transforming the variable\ndata$variable_log &lt;- log(data$variable)\n\n# Creating a dummy variable to indicate outliers\ndata$outlier_dummy &lt;- ifelse(data$variable &gt; 100, 1, 0)\n\n# Using a robust learning algorithm\nmodel &lt;- rlm(formula = label ~ variable, data = data, method = \"MM\")\n\n# Print the modified data frame\nprint(data)\n#&gt;   variable label variable_fixed variable_censored variable_log outlier_dummy\n#&gt; 1        1     1              1                 1    0.0000000             0\n#&gt; 2        2     0              2                 2    0.6931472             0\n#&gt; 3        3     1              3                 3    1.0986123             0\n#&gt; 4      100     0            100               100    4.6051702             0\n#&gt; 5        5     1              5                 5    1.6094379             0\n#&gt; 6        6     0              6                 6    1.7917595             0\n#&gt; 7      200     1             NA               100    5.2983174             1",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling Data Issues</span>"
    ]
  },
  {
    "objectID": "handling-data-issues.html#data-leakage",
    "href": "handling-data-issues.html#data-leakage",
    "title": "7¬† Handling Data Issues",
    "section": "\n7.3 Data leakage",
    "text": "7.3 Data leakage\nLeakage in statistical learning refers to a situation where a model is trained using information that would not be available during prediction in real-world scenarios. Leakage can lead to over-optimistic estimates of the model‚Äôs performance, as the model may be relying on information that it would not have access to during deployment. To avoid leakage, it is essential to carefully inspect the variables used in the model and ensure that they are not revealing information that would not be available in a production environment. By avoiding leakage, we can create more accurate and reliable machine learning models.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Handling Data Issues</span>"
    ]
  },
  {
    "objectID": "data-visualisation.html",
    "href": "data-visualisation.html",
    "title": "8¬† Data Visualisation Using ggplot2",
    "section": "",
    "text": "8.1 Introduction and Basics\nThe utilization of ggplot2 for data visualization offers several advantages that make it a preferred choice among data analysts and data scientists.\nOne significant benefit is its adherence to the grammar of graphics, a conceptual framework that provides a consistent and structured approach to creating visualizations. The grammar of graphics breaks down a plot into its fundamental components: data, aesthetics, geometries, scales, and facets. This modular approach allows for greater flexibility and customization. For instance, with ggplot2, users can easily map variables to aesthetics, such as color or size, to reveal patterns and relationships in the data.\nAdditionally, ggplot2 simplifies the process of creating complex plots by providing a wide range of geometries, including points, lines, bars, and areas, which can be tailored to suit specific data types and analysis requirements. By understanding and leveraging the grammar of graphics, users can create visually compelling and informative plots that effectively communicate insights from their data.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation.html#customization-and-styling",
    "href": "data-visualisation.html#customization-and-styling",
    "title": "8¬† Data Visualisation Using ggplot2",
    "section": "\n8.2 Customization and Styling",
    "text": "8.2 Customization and Styling\nggplot2 is a powerful and versatile package for creating customizable and visually appealing plots. It provides a wide range of options to customize plots, allowing users to tailor their visualizations to meet specific requirements. With ggplot2, you can easily customize plot titles, axis labels, and legends to provide clear and descriptive information.\nFor example, to add a title to a plot, you can use the labs() function:\n\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n# Create a scatter plot\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Sepal Length vs. Sepal Width\",\n       x = \"Sepal Length\", y = \"Sepal Width\")\nplotly::ggplotly(p)\n\n\n\n\n\nMoreover, ggplot2 allows you to modify colors, shapes, and sizes of plot elements to enhance visual clarity and emphasize important aspects.\nFor instance, you can change the color of points in a scatter plot using the color argument:\n\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  labs(title = \"Scatter Plot with Colored Points\",\n       x = \"Sepal Length\", y = \"Sepal Width\")\nplotly::ggplotly(p)\n\n\n\n\n\nAdjusting axis scales and limits is another crucial aspect of plot customization. You can control the range of values displayed on the x-axis and y-axis using functions such as xlim() and ylim(). Here‚Äôs an example of adjusting the axis limits in a scatter plot:\n\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title = \"Scatter Plot with Adjusted Axis Limits\",\n       x = \"Sepal Length\", y = \"Sepal Width\") +\n  xlim(4, 8) +\n  ylim(2, 4.5)\nggplotly(p)\n\n\n\n\n\nGrouping and faceting in ggplot2 allow you to visualize subsets of data or create multiple plots based on categorical variables. By using the facet_wrap() or facet_grid() functions, you can split the data into panels based on specific grouping variables.\nHere‚Äôs an example that demonstrates faceting in a scatter plot by species:\n\np &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  labs(title = \"Scatter Plot with Grouping and Faceting\",\n       x = \"Sepal Length\", y = \"Sepal Width\") +\n  facet_wrap(~ Species)\nggplotly(p)\n\n\n\n\n\nWorking with multiple data sources is also facilitated by ggplot2. You can combine and overlay different datasets to create layered and composite visualizations.\nFor instance, you can combine a scatter plot and a line plot by using the + operator:\n\ndf1 &lt;- data.frame(x = 1:10, y = 1:10)\ndf2 &lt;- data.frame(x = 1:10, y = 10:1)\n\np &lt;- ggplot() +\n  geom_point(data = df1, aes(x, y)) +\n  geom_line(data = df2, aes(x, y)) +\n  labs(title = \"Overlaying Multiple Data Sources\")\nggplotly(p)",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation.html#creating-advanced-plot-types",
    "href": "data-visualisation.html#creating-advanced-plot-types",
    "title": "8¬† Data Visualisation Using ggplot2",
    "section": "\n8.3 Creating Advanced Plot Types",
    "text": "8.3 Creating Advanced Plot Types\nR ggplot2 is a versatile and powerful package for data visualization, allowing users to create advanced plot types and customize their visual appearance. One key aspect of ggplot2 is the ability to create advanced plot types, beyond basic scatter plots or bar charts, to effectively represent complex data patterns. This can be achieved by incorporating additional geometries, statistical transformations, and layering multiple visual elements. For example, users can create visually appealing heatmaps, boxplots, violin plots, or density plots, among others, using the extensive set of geometries and statistical functions provided by ggplot2.\nTo ensure consistent plot styling across different visualizations, ggplot2 offers themes and templates that allow users to define and apply predefined sets of formatting rules. By applying a theme, users can easily modify the appearance of various plot elements such as axes, titles, legends, background colors, and fonts. This helps to maintain a cohesive visual style throughout multiple plots in a project or presentation. Additionally, ggplot2 provides options to customize themes or create custom templates to suit specific design preferences or conform to brand guidelines.\nHere‚Äôs an example that demonstrates creating an advanced plot type (a violin plot) and applying a custom theme for consistent plot styling:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Create a dataset\ndata &lt;- data.frame(\n  Group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  Value = rnorm(300)\n)\n\n# Create a violin plot\np &lt;- ggplot(data, aes(x = Group, y = Value)) +\n  geom_violin(fill = \"#FF6666\", color = \"#990000\", alpha = 0.8) +\n  theme_minimal()  # Apply a minimal theme\n\n# Display the plot\nggplotly(p)\n\n\n\n\n\nIn the above example, we create a dataset with three groups (A, B, C) and corresponding values. We use the geom_violin() function to create a violin plot, specifying the fill color, border color, and transparency. Finally, we apply the theme_minimal() function to apply a minimal theme to the plot, which removes unnecessary background elements and provides a clean and focused visualization.\nBy exploring the various advanced plot types available in ggplot2 and leveraging themes and templates, users can create visually striking and consistent plots that effectively communicate complex data patterns.",
    "crumbs": [
      "Data Exploration And Visualisation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "supervised-learning.html",
    "href": "supervised-learning.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this third part of the book, our focus will be on the the following chapters:\n\nIn ?sec-basics-supervised-learning we will get to understand the basics of supervised learning from a theoretical and practical standpoint.\nIn 10¬† Linear Regression we will dive into linear regression from a theoretical and practical standpoint.\nIn 11¬† K-Nearest Neighbour Regression we will understand the k-Nearest Neighbours (kNN) regression algorithm as a classic method for nonlinear data.\nIn 12¬† Classification Using Logistic Model we will dive into classification models.",
    "crumbs": [
      "Supervised Learning"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html",
    "href": "basic-supervised-learning.html",
    "title": "9¬† Basics of Supervised Learning",
    "section": "",
    "text": "9.1 Overview\nIn this section, we will introduce the fundamentals of supervised learning algorithms. It is a widely used approach to machine learning, and it has numerous practical applications in various industries. Here are some real-life examples of supervised learning:\nThese are just a few examples of the many applications of supervised learning. With the increasing availability of labeled data and the development of advanced algorithms, the potential applications of supervised learning are constantly expanding.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#overview",
    "href": "basic-supervised-learning.html#overview",
    "title": "9¬† Basics of Supervised Learning",
    "section": "",
    "text": "Image Classification: Supervised learning algorithms can be trained to classify images into different categories, such as recognising faces, identifying objects in a scene, and detecting anomalies in medical images.\nFraud Detection: Supervised learning algorithms can be trained on labeled data to detect fraudulent transactions, identify unusual patterns in financial data, and prevent financial crimes.\nText and Sentiment Analysis: Supervised learning can be used for sentiment analysis, where algorithms are trained to classify text as positive or negative based on a labeled dataset. This can be used in customer feedback analysis, social media monitoring, and market research.\nMedical Diagnosis: Supervised learning algorithms can be used to diagnose diseases based on medical images, clinical data, and genetic data. Examples include the diagnosis of cancer, Alzheimer‚Äôs disease, and heart disease.\nSpeech Recognition: Supervised learning can be used to train speech recognition systems to transcribe spoken language into text, identify speakers based on their voice, and improve automatic translation systems.\nAutonomous Driving: Supervised learning algorithms can be used to train self-driving cars to recognise and respond to different traffic situations, road signs, and road conditions.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#fundamentals-of-supervised-learning-models",
    "href": "basic-supervised-learning.html#fundamentals-of-supervised-learning-models",
    "title": "9¬† Basics of Supervised Learning",
    "section": "\n9.2 Fundamentals of supervised learning models",
    "text": "9.2 Fundamentals of supervised learning models\nWe provide an overview of the key concepts and techniques required to build effective supervised learning models.\nWe‚Äôll use the notation \\(Y\\) for the output variable and \\(X_1,\\ldots, X_p\\) for the input variables.¬† For example, suppose we want to predict the sale price of residential properties. In this case, the output variable is the price. and the input variables are the characteristics of the house such as size, number of bedrooms, number of bathrooms, and location.\nTo build a statistical learning model, we need training data. We represent the output data as a vector: \\[\\begin{equation*}       \\boldsymbol y=\\begin{pmatrix}             y_{1} \\\\             y_{2}\\\\             \\vdots\\\\             y_{n} \\end{pmatrix},\\,\\,\\,\\,\\, \\end{equation*} \\]\nwhere \\(y_i\\) refers to the observed value of the output variable for observation i. We represent the input data as the matrix: \\[\\begin{equation*}   \\boldsymbol X=\\begin{bmatrix}               \\begin{array}{cccc}                 x_{11} & x_{12} &  \\ldots & x_{1p} \\\\                 x_{21} & x_{22} &  \\ldots & x_{2p} \\\\                 \\vdots & \\vdots  & \\ddots & \\vdots \\\\                 x_{n1} & x_{n2} &  \\ldots & x_{np} \\\\               \\end{array}    \\end{bmatrix},       \\end{equation*} \\]\nwhere \\(x_{ij}\\) denotes the value of input j for observation i. We refer to this matrix as the design matrix.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#generalisation",
    "href": "basic-supervised-learning.html#generalisation",
    "title": "9¬† Basics of Supervised Learning",
    "section": "\n9.3 Generalisation",
    "text": "9.3 Generalisation\nThe primary objective of supervised learning is to develop a model that can accurately predict new data. To evaluate the performance of a model, we need to test its ability to generalise to data it has not seen before. This ability to generalise is referred to as the model‚Äôs generalisation capability. Generalisation is a crucial concept in machine learning, particularly in the context of building models that perform well on live data in business production systems. While it is essential to train a model using existing data, the ultimate goal is to ensure that the model can generalise well to new data. To test the generalisation capability of a model, we use a separate dataset known as test data. This dataset consists of examples that are distinct from the training data and are used to evaluate the model‚Äôs performance. Test data may be pre-existing data that we set aside for evaluation purposes or future data that the model will encounter in the real world.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#training-and-validation-sets",
    "href": "basic-supervised-learning.html#training-and-validation-sets",
    "title": "9¬† Basics of Supervised Learning",
    "section": "\n9.4 Training and validation sets",
    "text": "9.4 Training and validation sets\nSupervised learning is a practical approach that involves using data to inform modelling decisions and iteratively improving the models. One common practice in supervised learning is to randomly split the training data into two subsets: a training set and a validation set. The training set is used to build machine learning models. In contrast, the validation set is used to measure the performance of different models and select the best one. This technique is known as cross-validation and is a critical step in the model building process. By splitting the data into training and validation sets, we can train the model on one subset and use the other subset to evaluate its performance. This approach allows us to estimate the generalisation error of the model and assess how well it will perform on new data.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "basic-supervised-learning.html#metrics",
    "href": "basic-supervised-learning.html#metrics",
    "title": "9¬† Basics of Supervised Learning",
    "section": "\n9.5 Metrics",
    "text": "9.5 Metrics\nAt the beginning of a supervised learning project, it is crucial to determine the appropriate method for evaluating the predictive performance of the machine learning models. One common approach is to use a metric, which is a function that assesses the quality of a model‚Äôs predictions on a given dataset, such as the validation set.\nThe most common metric for regression is the mean squared error (MSE), \\[\\textsf{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\widehat{y}_i)^2}, \\] where \\(y_i\\) and \\(\\widehat{y}\\) are the actual and predicted values respectively. We commonly report the root mean squared error (RMSE) \\[\\textsf{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\widehat{y}_i)^2}, \\] instead of the mean squared error as it‚Äôs on the same scale as the output variable. The prediction \\(\\text{R}^2\\) is \\[\\textsf{Prediction R}^2=1-\\frac{\\sum_{i=1}^{n}(y_i-\\widehat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\overline{y})^2}\\]\nis also derived from the MSE, but does not depend on the scale of the response variable. The mean absolute error (MAE) is \\[\\textsf{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}\\vert y_i-\\widehat{y}_i\\vert. \\]\nThe MAE is less sensitive to large prediction errors than the MSE.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Basics of Supervised Learning</span>"
    ]
  },
  {
    "objectID": "linear-reg.html",
    "href": "linear-reg.html",
    "title": "10¬† Linear Regression",
    "section": "",
    "text": "10.1 Overview\nIn the linear regression method, we use a linear function for prediction: \\[f(\\boldsymbol x; \\boldsymbol \\beta)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\ldots+\\beta_px_p, \\]\nwhere \\(\\boldsymbol x\\) is a vector of input values and \\(\\boldsymbol \\beta\\) is a vector of parameters.\nWe train the linear regression model by minimising the training mean squared error, \\[\\begin{align*} \\widehat{\\boldsymbol \\beta}=\\underset{\\boldsymbol \\beta}{\\operatorname{argmin}}\\,\\, \\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2\\right\\}, \\end{align*} \\]\nwhere \\(\\widehat{\\boldsymbol \\beta}\\) denotes the estimated parameters. This is known as the ordinary least squares (OLS) method for estimating a linear regression. To get the solution, let the design matrix be: \\[\\begin{equation*}    \\boldsymbol X=\\begin{bmatrix}               \\begin{array}{ccccc}                 1 & x_{11} & x_{12} &  \\ldots & x_{1p} \\\\                 1 & x_{21} & x_{22} &  \\ldots & x_{2p} \\\\                 \\vdots & \\vdots & \\vdots  & \\ddots & \\vdots \\\\                 1 & x_{n1} & x_{n2} &  \\ldots & x_{np} \\\\               \\end{array}    \\end{bmatrix},   \\end{equation*} \\] noting that we added a column of ones at the beginning of the design matrix.\nAs before, the output vector is:\n\\[\\begin{equation*}       \\boldsymbol y=\\begin{pmatrix}             y_{1} \\\\             y_{2}\\\\             \\vdots\\\\             y_{n} \\end{pmatrix}.  \\end{equation*} \\]\nWe can show that the OLS problem \\[\\begin{align*} \\widehat{\\boldsymbol \\beta}=\\underset{\\boldsymbol \\beta}{\\operatorname{argmin}}\\,\\, \\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_j x_{ij}\\right)^2\\right\\}, \\end{align*} \\]\nhas solution \\[\\widehat{\\boldsymbol \\beta} = (\\boldsymbol X^\\top\\boldsymbol X)^{-1} \\boldsymbol X^\\top\\boldsymbol y, \\] under the assumption that the columns of \\(\\boldsymbol X\\) are linearly independent.\nUpon observing a new input \\(\\boldsymbol x_0\\), we make the prediction: \\[\\widehat{f}(\\boldsymbol x_0)=\\widehat{\\beta}_0+\\sum_{j=1}^p\\widehat{\\beta}_j x_{0j}. \\]",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-reg.html#statistical-modelling-for-linear-regression",
    "href": "linear-reg.html#statistical-modelling-for-linear-regression",
    "title": "10¬† Linear Regression",
    "section": "\n10.2 Statistical modelling for linear regression",
    "text": "10.2 Statistical modelling for linear regression\nIt‚Äôs useful to contrast supervised learning with the traditional statistical modelling approach that may be familiar to you from other units.\nThe goal of supervised learning is to accurately predict new data.\nIn traditional statistical modelling, the goal is to make inferences about the data generating process (DGP). Prediction is just one type of statistical inference. Do you remember procedures such as confidence intervals and hypothesis testing? This type of modelling requires several assumptions regarding the DGP. In particular, we assume that the model is a good description of the data generating process.\nIn machine learning, we can regard the data generating process as a black box. The model doesn‚Äôt need to be a realistic description of the data generating process, it only needs to be useful for prediction. Nevertheless, statistical modelling is still useful for machine learning.\n\n10.2.1 Classical assumptions of the linear regression model\nIn the statistical approach to linear regression, the classical linear regression model is based on the following assumptions: * Linearity: if \\(X=\\boldsymbol x\\), then \\(Y=\\beta_0+\\beta_1x_1+\\ldots+\\beta_px_p+\\varepsilon\\) for some population parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) and a random error . * The errors have zero conditional mean: \\(\\mathbb E(\\varepsilon |X=\\boldsymbol x)=0\\) for all \\(\\boldsymbol x\\). * Constant error variance: \\(\\mathbb V(\\varepsilon|X=\\boldsymbol x)=\\sigma^2\\) for all \\(\\boldsymbol x\\). * Independent errors: all the errors are independent of each other. * No perfect multicollinearity. * Gaussian errors (optional): that \\(\\varepsilon \\sim N(0,\\sigma^2)\\). The linear regression algorithm for supervised learning only requires the assumption of no perfect multicollinearity, which is necessary for the least squares solution to be defined. The twist is that the classical assumptions are still desirable because they represent the ideal situation for using a linear regression model trained by OLS.\n\n10.2.2 Potential problems\nIn regression analysis, there are several potential issues that can arise and affect the accuracy and validity of the results. These issues include nonlinearity, non-constant error variance, dependent errors, perfect and imperfect multicollinearity, non-Gaussian errors, and outliers and leverage points.\n\nNonlinearity: One potential issue in regression analysis is nonlinearity. This occurs when the relationship between the predictor variables and the response variable is not linear. In other words, the relationship between the variables is not a straight line, but instead is curved or has other shapes. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nNon-constant Error Variance: Another potential issue is non-constant error variance, also known as heteroscedasticity. This occurs when the variability of the errors is not constant across the range of the predictor variables. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nDependent Errors: also known as autocorrelation, occur when the errors in the regression model are correlated with each other. This violates the assumption of independence of the errors, which can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nPerfect Multicollinearity: when one or more of the predictor variables are perfectly correlated with each other. This can lead to an inability to estimate the regression coefficients and affect the accuracy of the predictions.\nImperfect Multicollinearity: when there is high correlation between predictor variables, but not perfect correlation. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nNon-Gaussian Errors: the errors in the regression model do not follow a normal distribution. This can lead to biased estimates of the regression coefficients and affect the accuracy of the predictions.\nOutliers and Leverage Points: data points that are significantly different from the other data points in the dataset. Outliers can affect the estimates of the regression coefficients, while leverage points can have a large influence on the regression line. Both can affect the accuracy of the predictions.\n\nResponse transformation\nThe response variable is often highly skewed in business data. In this case, a log transformation of the output variable can:\n\nAccount for some forms of nonlinearity.\nStabilise the variance of the errors.\nReduce skewness in the errors.\n\nIn the example below, we load the mtcars dataset, which contains information about various car models. We then perform a linear regression analysis to predict the miles per gallon (mpg) based on the weight (wt) of the cars. The lm() function is used to fit the linear regression model, with mpg ~ wt specifying the dependent and independent variables.\nNext, we print the summary of the regression model using summary(model). This provides important information such as the coefficients, standard errors, p-values, and goodness-of-fit measures.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-reg.html#an-r-example",
    "href": "linear-reg.html#an-r-example",
    "title": "10¬† Linear Regression",
    "section": "\n10.3 An R Example",
    "text": "10.3 An R Example\nHere is an R example to illustrate the concept:\n\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\n# read dataset\ndf = mtcars\n\n# create multiple linear model\nlm_fit &lt;- lm(mpg ~ wt, data=df)\nsummary(lm_fit)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = mpg ~ wt, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.5432 -2.3647 -0.1252  1.4096  6.8727 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\n#&gt; wt           -5.3445     0.5591  -9.559 1.29e-10 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.046 on 30 degrees of freedom\n#&gt; Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 \n#&gt; F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n# this is predicted line comparing only chosen variables\np &lt;- ggplot(data = df, aes(x = wt, y = mpg)) + \n  geom_point(color='blue') +\n  geom_abline(slope = coef(lm_fit)[[\"wt\"]], \n              intercept = coef(lm_fit)[[\"(Intercept)\"]], \n              color = 'red') + \n  labs(title=\"Miles per gallon\",\n       x=\"Weight (lb/1000)\", y = \"Miles/(US) gallon\") + theme_linedraw()\nggplotly(p)",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "k-nearest-neighbour-reg.html",
    "href": "k-nearest-neighbour-reg.html",
    "title": "11¬† K-Nearest Neighbour Regression",
    "section": "",
    "text": "11.1 Overview\nThe kNN method keeps all training examples in memory to make predictions. When a new input comes in, kNN finds the training examples that are more similar to that new input. The prediction is the average value of the output variable for these nearest neighbours.\nIn mathematical notation, the kNN prediction is \\[\\begin{align*} \\widehat{f}(\\boldsymbol x)&=\\text{Average}\\Big[\\,y_i \\,\\Big|\\, i \\;\\text{is in}\\; \\mathcal{N}_k(\\boldsymbol x,\\mathcal{D}) \\, \\Big]\\\\ &=\\frac{1}{k}\\sum_{i \\in \\mathcal{N}_k(\\boldsymbol x,\\mathcal{D})}y_i \\end{align*}\\]\nwhere \\(\\mathcal{N}_k(\\boldsymbol x,\\mathcal{D})\\) is the set of indexes for the closest \\(k\\) data points to \\(x\\) in \\(\\mathcal{D}\\) according to some distance function \\(\\text{dist}(\\boldsymbol x,\\boldsymbol x_i)\\).\nThe kNN method is highly flexible: in principle, it can approximate any ‚Äúreasonable‚Äù regression function assuming what it may look like.\nUnfortunately, the price to pay for this flexibility is that kNN performs poorly when the number of inputs increases. This problem is known as the curse of dimensionality. We‚Äôll explain it in more detail later in the unit.\nHyperparameters A hyperparameter is a parameter of a learning algorithm that is set before the training process begins and is not directly learnable from the data. The choice of hyperparameters is crucial for many learning algorithms, as they determine their behaviour.\nThe KNN method has two hyperparameters: the number of neighbours (K) and the distance function.\nAbove, you saw how the choice of the number of neighbours has an important effect on the learned predictive function. For now, try to reach your own intuitive conclusions regarding how the number of neighbours affects the KNN method.\nA common choice of distance function is the Euclidean distance: \\[\\begin{align*} \\textsf{dist}(\\boldsymbol x_i,\\boldsymbol x_l)&=\\Vert \\boldsymbol x_i-\\boldsymbol x_l\\Vert_2\\\\[3pt] &=\\sqrt{\\sum_{j=1}^{p}(x_{ij}-x_{lj})^2}, \\end{align*}\\]\nThere are many other distance functions available. The Mahalonobis distance is \\[\\textsf{dist}(\\boldsymbol x_i,\\boldsymbol x_l)=\\sqrt{(\\boldsymbol x_i-\\boldsymbol x_l)^\\top\\boldsymbol S^{-1}(\\boldsymbol x_i-\\boldsymbol x_l)},\\]\nwhere \\(\\boldsymbol S\\) is the sample covariance matrix of the predictors. In addition to automatic scaling, this approach accounts for the correlations between the predictors.\nAn advanced approach is metric learning, which aims to learn a task-specific distance metric from data. For example, we can use metric learning metrics to choose the \\(\\boldsymbol S\\) matrix in the Mahalanobis distance. Metric learning can significantly improve KNN, but is computationally expensive.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>K-Nearest Neighbour Regression</span>"
    ]
  },
  {
    "objectID": "k-nearest-neighbour-reg.html#an-r-example",
    "href": "k-nearest-neighbour-reg.html#an-r-example",
    "title": "11¬† K-Nearest Neighbour Regression",
    "section": "\n11.2 An R Example",
    "text": "11.2 An R Example\nHere‚Äôs an example in base R to illustrate the concept of k-nearest neighbor (KNN):\n\n# Create a sample dataset\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- sin(x) + rnorm(length(x), mean = 0, sd = 0.3)\ndata &lt;- data.frame(x, y)\n\n# Define a function for KNN regression\nknn_regression &lt;- function(train_data, test_point, k) {\n  distances &lt;- sqrt((train_data$x - test_point$x)^2)\n  sorted_indices &lt;- order(distances)\n  neighbors &lt;- train_data$y[sorted_indices[1:k]]\n  predicted_value &lt;- mean(neighbors)\n  return(predicted_value)\n}\n\n# Predict the value using KNN regression\ntest_point &lt;- data.frame(x = 8.5)\nk &lt;- 3\npredicted_value &lt;- knn_regression(data, test_point, k)\n\n# Print the predicted value\nprint(predicted_value)\n#&gt; [1] 0.5460616\n\nIn this example, we create a sample dataset with two variables x and y. The x variable represents the input feature, and the y variable represents the target variable we want to predict.\nWe define a function knn_regression that takes three arguments: train_data (the training dataset), test_point (the test point for which we want to make a prediction), and k (the number of nearest neighbors to consider).\nInside the knn_regression function, we calculate the Euclidean distance between the x values of the training data and the x value of the test point. Then, we sort the distances and select the k nearest neighbors based on the sorted indices. We extract the corresponding y values of the neighbors and calculate the mean as the predicted value.\nWe predict the value for a test point with x = 8.5 and k = 3 by calling the knn_regression function with the sample dataset, test point, and k value. Finally, we print the predicted value.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>K-Nearest Neighbour Regression</span>"
    ]
  },
  {
    "objectID": "class-log-reg.html",
    "href": "class-log-reg.html",
    "title": "12¬† Classification Using Logistic Model",
    "section": "",
    "text": "12.1 Overview\nThe linear models that we have constructed so far have proven to be effective in forecasting numerical outcomes such as the housing price or the number of customers who may click on a hyperlink. These models, known as regression models, are suited for such applications. Nonetheless, there are situations where we require the ability to predict events or categories, rather than just a numerical quantity. For instance, we may want to ascertain the species of a flower, whether a person is likely to click on an emailed link, or the likelihood of a tree dying in a given year. For these tasks, we need to use a classification model.\nFor logistic regression, we use a ‚Äúsoft threshold‚Äù, by choosing a logistic function, \\(\\theta\\), that has a sigmoidal shape. The sigmoidal function can take on various forms, such as the following:\n\\[\\theta\\left(s\\right) = \\frac{e^s}{1+e^s}\\] This model implements a probability that has a genuine probability interpretation. The likelihood of any dataset, \\(\\mathcal{D} = \\left(\\mathbf{x_1},y_1\\right), \\dots, \\left(\\mathbf{x_N},y_N\\right)\\), that we wish to maximize is given by:\n\\[\\prod\\limits_{i=1}^N P\\left(y_i | \\mathbf{x_i}\\right) = \\prod\\limits_{i=1}^N \\theta\\left(y_i \\mathbf{w^T x_i}\\right)\\]\nIt is possible to derive an error measure (that would maximise the above likelihood measure), which has a probabilistic connotation, and is called the in-sample ‚Äúcross-entropy‚Äù error. It is based on assuming the hypothesis (of the logistic regression function) as the target function:\n\\[E_{in}\\left(\\mathbf{w}\\right) = \\frac{1}{N}\\sum\\limits_{n=1}^N \\ln\\left[1 + \\exp\\left(-y_n \\mathbf{w^T x_n}\\right)\\right]\\]\\[E_{in}\\left(\\mathbf{w}\\right) = \\frac{1}{N}\\sum\\limits_{n=1}^N e\\left[ h\\left(\\mathbf{x_n}\\right), y_n \\right]\\]\nWhile the above does not have a closed form solution, it is a convex function and therefore we can find the weights corresponding to the minimum of the above error measure using various techniques. Such techniques include gradient descent (and its variations, such as stochastic gradient descent and batch gradient descent) and there are others which make use of second order derivatives (such as the conjugate gradient method) or Hessians.\nTo walk through how these work, let‚Äôs start using a new dataset included in base R - mtcars which contains information on different car models, including their fuel efficiency (mpg), number of cylinders (cyl), engine displacement (disp), horsepower (hp), weight (wt), and other features. To obtain more information on the dataset, you can type ?mtcars in the console.\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n# Load mtcars dataset\ndata(mtcars)\n\np &lt;- ggplot(mtcars, aes(mpg, am)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) +\n  theme_minimal()\nggplotly(p)\n#&gt; `geom_smooth()` using formula = 'y ~ x'\nThere are a few reasons based on statistics why using a simple straight line is not appropriate for classification problems. When the data has heavy tails, meaning that there are data points that have a high probability (such as 95%) of belonging to one group, the linear formula becomes less effective. Similarly, when predictors are non-linear, where high and low values of a predictor tend to make belonging to group 0 more likely and middle values tend to be 1s, the linear model performs poorly. Instead, a logistic model is preferred, which produces a line that looks more like this:\np &lt;- ggplot(mtcars, aes(mpg, am)) + \n  geom_point() + \n  geom_smooth(method = \"loess\", se = F)+\n  theme_minimal()\n\nggplotly(p)\n#&gt; `geom_smooth()` using formula = 'y ~ x'\nLogistic Regression vs.¬†Linear Regression Logistic regression works similarly to linear regression, where data points with probabilities higher than 50% are assigned to the class ‚Äú1‚Äù and so on. Comparing the two, the logistic model demonstrates slightly better classification performance. For example, using the same model formula, logistic regression achieved 81% accuracy, outperforming the linear model.\nWhile some experts, such as certain professors, argue that the simplicity of linear models makes them the best choice for the majority of datasets, logistic models are generally as easy to implement and understand. Moreover, logistic regression usually provides better classification results. As such, logistic models are commonly relied upon for classifying datasets.\nCreating Logistic Regression Models The process of creating logistic regression models is quite similar to that of linear regression models. Instead of using the lm() function, the glm() function is employed, as logistic regression belongs to the family of algorithms known as generalized linear models. The formula and data arguments still need to be supplied, just as with lm(). The main difference when using glm() is the additional requirement to specify the argument family. In this case, setting family = binomial will calculate the logistic model.\nWe can build a logistic regression model using the following line of code:\nglm(am ~ mpg, data = mtcars, family = binomial)\n#&gt; \n#&gt; Call:  glm(formula = am ~ mpg, family = binomial, data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          mpg  \n#&gt;      -6.604        0.307  \n#&gt; \n#&gt; Degrees of Freedom: 31 Total (i.e. Null);  30 Residual\n#&gt; Null Deviance:       43.23 \n#&gt; Residual Deviance: 29.68     AIC: 33.68\nThe model summary can be viewed just like we did with lm():\nlogmod &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nsummary(logmod)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = am ~ mpg, family = binomial, data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept)  -6.6035     2.3514  -2.808  0.00498 **\n#&gt; mpg           0.3070     0.1148   2.673  0.00751 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 43.230  on 31  degrees of freedom\n#&gt; Residual deviance: 29.675  on 30  degrees of freedom\n#&gt; AIC: 33.675\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\nThe output presents various metrics that help to understand and interpret the logistic regression model:\nWe can further refine our model by adding more variables and changing the formula, just as we did with linear models:\nsummary(glm(am ~ hp + wt, data = mtcars, family=\"binomial\"))\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = am ~ hp + wt, family = \"binomial\", data = mtcars)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept) 18.86630    7.44356   2.535  0.01126 * \n#&gt; hp           0.03626    0.01773   2.044  0.04091 * \n#&gt; wt          -8.08348    3.06868  -2.634  0.00843 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 43.230  on 31  degrees of freedom\n#&gt; Residual deviance: 10.059  on 29  degrees of freedom\n#&gt; AIC: 16.059\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 8\nWe can see the AIC decreased significantly after adding an extra feature.\nPredicting with Logistic Regression Models In fact, logistic models created with the glm() function work similarly to linear models made with lm(). However, there are some key differences to consider. For instance, when using the model to make predictions, let‚Äôs assign the model to mtmod:\nmtmod &lt;- glm(am ~ hp + wt, data = mtcars, family=\"binomial\")\nRecall how we generated predictions from our models using predict(model, data). Here is what happens if we do something similar with logistic regression:\nhead(predict(mtmod, mtcars))\n#&gt;         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive \n#&gt;         1.6757093        -0.3855769         3.4844067        -3.1339584 \n#&gt; Hornet Sportabout           Valiant \n#&gt;        -2.5961266        -5.2956878\nThis does not produce the expected probabilities. The reason is that R is attempting to use the logistic model as if it were a linear model. To fix this, we must specify type = \"response\" in our predict() call:\nhead(predict(mtmod, mtcars, type = \"response\"))\n#&gt;         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive \n#&gt;       0.842335537       0.404782533       0.970240822       0.041728035 \n#&gt; Hornet Sportabout           Valiant \n#&gt;       0.069388122       0.004988159\nWith this adjustment, we can generate the correct probabilities.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Classification Using Logistic Model</span>"
    ]
  },
  {
    "objectID": "class-log-reg.html#overview",
    "href": "class-log-reg.html#overview",
    "title": "12¬† Classification Using Logistic Model",
    "section": "",
    "text": "Deviance Residuals: These are a measure of the difference between the observed values and the values predicted by the model. The minimum, 1st quartile, median, 3rd quartile, and maximum deviance residuals are shown. Smaller values indicate better model fit.\nCoefficients: These are the estimated coefficients for the model predictors (intercept and mpg in this case). The Estimate column shows the estimated value of each coefficient, while the Std. Error column presents the standard error for the estimate. The z value is the test statistic (estimate divided by standard error), and Pr(&gt;|z|) is the p-value, which indicates the significance of each predictor in the model. Significance codes are provided to help interpret the p-value (e.g., *** for p &lt; 0.001, ** for p &lt; 0.01, and * for p &lt; 0.05).\nDispersion parameter: For the binomial family, the dispersion parameter is taken to be 1. This indicates that the model assumes the variance of the response variable is equal to its mean.\nNull deviance: This is the deviance of the null model (a model with no predictors). In this case, the null deviance is 43.230, calculated based on 31 degrees of freedom.\nResidual deviance: This is the deviance of the fitted model, which measures the goodness-of-fit. The residual deviance is 29.675, calculated based on 30 degrees of freedom. A lower residual deviance compared to the null deviance indicates a better model fit.\nAIC (Akaike Information Criterion): This is a measure of the model‚Äôs quality in terms of the trade-off between goodness-of-fit and model complexity. A lower AIC value suggests a better model.\nNumber of Fisher Scoring iterations: This is the number of iterations taken by the Fisher Scoring algorithm to converge. In this case, the algorithm converged in 5 iterations.",
    "crumbs": [
      "Supervised Learning",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Classification Using Logistic Model</span>"
    ]
  },
  {
    "objectID": "basic-unsupervised-learning.html",
    "href": "basic-unsupervised-learning.html",
    "title": "13¬† Basics Of Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning is a type of machine learning where algorithms learn patterns and relationships in data without any labeled information. The primary goal is to identify underlying structures and extract valuable insights from the data. Common unsupervised learning techniques include clustering, dimensionality reduction, and anomaly detection. A few examples:\n\n\n\n\n\n\n\n\n\nAlgorithm\nType\nR Package\nPros\nCons\n\n\n\nK-means\nClustering\ncluster\nFast, scalable, easy to implement\nRequires predefined K, sensitive to initial conditions\n\n\nHierarchical Clustering\nClustering\ncluster\nNo predefined K, dendrogram representation\nSlower, not ideal for large datasets\n\n\nPrincipal Component Analysis\nDimensionality Reduction (Linear)\nbase R\nFast, interpretable results, noise reduction\nAssumes linear relationships, information loss\n\n\nt-SNE\nDimensionality Reduction (Non-linear)\nRtsne\nPreserves local structure, visual clustering\nSlower, non-deterministic, hard to interpret in high-dimensions\n\n\nIsolation Forest\nAnomaly Detection\nrandomForest\nEfficient for high-dimensional datasets\nMay struggle with low-density clusters\n\n\nLocal Outlier Factor\nAnomaly Detection\nRlof\nConsiders local density, handles varying densities\nSlower, sensitive to parameter choices\n\n\n\nUnsupervised learning has a wide range of applications across various industries. Here are some real-life examples of unsupervised learning:\n\nClustering Customer Segmentation: Clustering techniques can be used to group customers based on their purchasing patterns, demographics, and preferences. This information can then be used to create targeted marketing campaigns, improve customer retention, and optimise product offerings.\nAnomaly Detection in Cybersecurity: Unsupervised learning can be used to detect anomalies in network traffic, identify malicious activity, and prevent cybersecurity breaches. Techniques such as Isolation Forest and Local Outlier Factor are commonly used for this purpose.\nTopic Modelling in Natural Language Processing: Topic modelling is a technique used in natural language processing to identify common themes and topics within large text datasets. This can be used to analyse customer feedback, social media posts, and news articles.\nImage and Video Recognition: Unsupervised learning techniques such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbour Embedding (t-SNE) can be used for image and video recognition tasks. These techniques can help identify patterns and similarities in visual data without requiring labeled examples.\nDrug Discovery: Unsupervised learning can be used in drug discovery to identify patterns and relationships in large datasets of chemical compounds. This can help researchers develop new drugs and improve existing treatments.\nRecommendation Systems: Clustering and association rule mining techniques can be used to develop personalised recommendation systems in e-commerce and entertainment industries. These systems analyse user behaviour and preferences to suggest products or content that are relevant to individual users.\n\nThese are just a few examples of the many applications of unsupervised learning. With the increasing availability of large datasets and the development of advanced algorithms, the potential applications of unsupervised learning are constantly expanding.\nIn this section of the course, we provide an overview of popular clustering, dimensionality reduction, and anomaly detection algorithms in R. By understanding these techniques and their respective R packages, you will be well-equipped to handle various unsupervised learning tasks.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Basics Of Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "k-means-clust.html",
    "href": "k-means-clust.html",
    "title": "14¬† K-Mean Clustering",
    "section": "",
    "text": "14.1 Principles of K-Mean Clustering\nK-mean clustering is a popular unsupervised learning algorithm used to group similar data points together into a fixed number of clusters. It is a centroid-based algorithm, which means that it assigns data points to clusters based on their proximity to a central point called the centroid. In R, the K-Means algorithm can be implemented using the following code:\n# Load iris dataset\ndata(iris)\n\n# Extract relevant columns for clustering\niris_data &lt;- iris[, 1:4]\n\nstats::kmeans(iris_data, centers = 3, nstart = 10)\n#&gt; K-means clustering with 3 clusters of sizes 50, 62, 38\n#&gt; \n#&gt; Cluster means:\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; 1     5.006000    3.428000     1.462000    0.246000\n#&gt; 2     5.901613    2.748387     4.393548    1.433871\n#&gt; 3     6.850000    3.073684     5.742105    2.071053\n#&gt; \n#&gt; Clustering vector:\n#&gt;   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#&gt;  [37] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt;  [73] 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3\n#&gt; [109] 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3\n#&gt; [145] 3 3 2 3 3 2\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 15.15100 39.82097 23.87947\n#&gt;  (between_SS / total_SS =  88.4 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"    \n#&gt; [5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"        \n#&gt; [9] \"ifault\"\nHere, the parameters are as follows: - x: A numeric data matrix containing the observations - centers: The predefined number of clusters, k - nstart: The number of times the K-Means algorithm is repeated to enhance the resulting model, as it has a random component\nTo learn about k-means, let‚Äôs use the iris dataset with the sepal and petal length variables only (to facilitate visualisation).\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\n# Perform K-Means clustering with 3 clusters\nset.seed(123)\nkmeans_result &lt;- kmeans(iris_data, centers = 3)\n\n# Visualize the clusters\nlibrary(ggplot2)\np &lt;- ggplot(iris, aes(Petal.Length, Petal.Width, color = factor(kmeans_result$cluster))) + \n  geom_point() + \n  labs(title = \"K-Means Clustering of Iris Dataset\",\n       x = \"Petal Length\",\n       y = \"Petal Width\",\n       color = \"Cluster\") + \n  theme_minimal()\nggplotly(p)",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>K-Mean Clustering</span>"
    ]
  },
  {
    "objectID": "hier-clust.html",
    "href": "hier-clust.html",
    "title": "15¬† Hierarchical Clustering",
    "section": "",
    "text": "15.1 Overview\nHierarchical clustering is a type of clustering algorithm used to group similar data points together in a hierarchy. It is an unsupervised learning method, which means that it does not require labeled data. The algorithm creates a hierarchical structure of clusters, with data points that are similar to each other being grouped together.\nThe two main types of hierarchical clustering are agglomerative and divisive. Agglomerative clustering is more commonly used and works by starting with each data point as a separate cluster and gradually merging them into larger clusters. Divisive clustering, on the other hand, starts with all data points in one cluster and then recursively splits them into smaller clusters.\nIn agglomerative clustering, the algorithm starts by calculating the distance between each pair of data points and then merges the two closest points into a single cluster. The algorithm then repeats this process, calculating the distance between the new cluster and each of the remaining data points, and merging the closest point or cluster until all data points are part of a single cluster. The resulting hierarchical structure can be represented as a dendrogram, which shows the hierarchical relationships between clusters.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Hierarchical Clustering</span>"
    ]
  },
  {
    "objectID": "hier-clust.html#implementation-of-hierarchical-clustering",
    "href": "hier-clust.html#implementation-of-hierarchical-clustering",
    "title": "15¬† Hierarchical Clustering",
    "section": "\n15.2 Implementation of hierarchical clustering",
    "text": "15.2 Implementation of hierarchical clustering\nThe distance between clusters is typically calculated using a linkage method, which determines how the distance between clusters is measured. The most common linkage methods are: - Single linkage: the distance between the closest pair of data points in the two clusters. - Complete linkage: the distance between the farthest pair of data points in the two clusters. - Average linkage: the average distance between all pairs of data points in the two clusters.\nThe choice of linkage method can affect the resulting clusters, so it is important to choose an appropriate method based on the characteristics of the data set.\nLet‚Äôs use the iris dataset to apply the hierarchical clustering:\n\nlibrary(plotly)\n#&gt; Loading required package: ggplot2\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\nlibrary(dendextend)\n#&gt; \n#&gt; ---------------------\n#&gt; Welcome to dendextend version 1.17.1\n#&gt; Type citation('dendextend') for how to cite the package.\n#&gt; \n#&gt; Type browseVignettes(package = 'dendextend') for the package vignette.\n#&gt; The github page is: https://github.com/talgalili/dendextend/\n#&gt; \n#&gt; Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues\n#&gt; You may ask questions at stackoverflow, use the r and dendextend tags: \n#&gt;   https://stackoverflow.com/questions/tagged/dendextend\n#&gt; \n#&gt;  To suppress this message use:  suppressPackageStartupMessages(library(dendextend))\n#&gt; ---------------------\n#&gt; \n#&gt; Attaching package: 'dendextend'\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     cutree\nlibrary(ggplot2)\n\n# Load iris dataset\ndata(iris)\n\n# Extract relevant columns for clustering\niris_data &lt;- iris[, 1:4]\n\n# perform hierarchical clustering using complete linkage\ndend &lt;- dist(iris_data) %&gt;%\n  hclust(method = \"complete\") %&gt;%\n  as.dendrogram()\ndend2 &lt;- color_branches(dend, 5)\n\n# plot the dendrogram\np &lt;- ggplot(dend2, horiz = F, offset_labels = -3) + labs(title = \"Dendogram\")\nggplotly(p)\n\n\n\n\n\nAfter performing hierarchical clustering on the data, we can use the resulting dendrogram to determine the appropriate number of clusters. This can be done by specifying a cutoff height at which the dendrogram should be cut to produce the desired number of clusters.\nFor example, let‚Äôs say we have performed hierarchical clustering on a test dataset and obtained a dendrogram. We can visually inspect the dendrogram and determine a cutoff height that results in the desired number of clusters. In this case, let‚Äôs say we decide to cut the dendrogram at a distance around 3.4, which would produce 3 clusters.\nTo implement this in R, we can use the cutree() function to cut the dendrogram at the desired height and obtain the cluster assignments. Here‚Äôs an example code:\n\np &lt;- ggplot(dend2, horiz = F, offset_labels = -3) + geom_hline(aes(yintercept = 3.4, colour = \"red\")) + labs(title = \"Dendogram\")\nggplotly(p)\n\n\n\n\n\nNote that the appropriate cutoff height for the dendrogram may depend on the specific data set and clustering objective, and may require some trial and error to determine.\nLet‚Äôs also compare it with the K-means algorithm:\n\nlibrary(plotly)\nlibrary(cowplot)\nlibrary(ggplot2)\n\nkm &lt;- kmeans(iris[, 1:4], centers = 3, nstart = 10)\nhcl &lt;- hclust(dist(iris[, 1:4]))\n\ndf1 = data.frame(PetalLength = iris$Petal.Length, SetalLength=iris$Sepal.Length, cluster = as.factor(km$cluster))\n\ndf2 = data.frame(PetalLength = iris$Petal.Length, SetalLength=iris$Sepal.Length, cluster = as.factor(cutree(hcl, k = 3)))\n\n# plots\np1 &lt;- ggplot(df1) +\n  aes(x = PetalLength, y = SetalLength, colour = cluster) +\n  geom_point(shape = \"circle\", size = 2) +\n  scale_color_hue(direction = 1) +\n  labs(title = \"K-Means\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10L, face = \"bold.italic\"))\n  \np2 &lt;- ggplot(df2) +\n  aes(x = PetalLength, y = SetalLength, colour = cluster) +\n  geom_point(shape = \"circle\", size = 2) +\n  scale_color_hue(direction = 1) +\n  labs(title = \"Hierarchical Clustering\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10L, face = \"bold.italic\"))\n\nplot_grid(p1, p2, ncol = 2)\n\n\n\n\n\n\n\nHierarchical clustering has a number of advantages, such as the ability to create a detailed hierarchical structure of clusters and the absence of the need to specify the number of clusters in advance. However, it can be computationally expensive for large data sets, and the resulting hierarchy can be difficult to interpret for complex data sets.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Hierarchical Clustering</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "16¬† Principal Component Analysis",
    "section": "",
    "text": "16.1 Overview\nDimensionality reduction techniques are a set of widely employed and versatile methods utilised for the following purposes:\nThe fundamental concept behind these techniques is to transform the original data into a new space with a reduced number of dimensions while preserving the salient properties of the dataset. This new space, containing fewer informative dimensions, is ideal for data visualisation.\nPrincipal Component Analysis (PCA) is a popular dimensionality reduction method that transforms the original n-dimensional dataset into a new n-dimensional space. The new dimensions in this transformed space, known as principal components, are linear combinations of the original variables, meaning they comprise varying proportions of the original features. The dataset exhibits the maximum variability along the first principal component, followed by the second, and so on. Additionally, principal components are orthogonal, or uncorrelated, with one another.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#overview",
    "href": "pca.html#overview",
    "title": "16¬† Principal Component Analysis",
    "section": "",
    "text": "Uncovering underlying structure in the feature space,\nPre-processing data for other machine learning algorithms, and\nFacilitating visualisation of high-dimensional data.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#the-mathematics-of-pca",
    "href": "pca.html#the-mathematics-of-pca",
    "title": "16¬† Principal Component Analysis",
    "section": "\n16.2 The Mathematics of PCA",
    "text": "16.2 The Mathematics of PCA\nMathematically, PCA is implemented as follows:\n\nGiven \\(m\\) data points, \\(\\{x_1,x_2,...,x_m\\} \\in \\mathbb{R}^n\\), with their mean \\(\\mu = \\frac{1}{m}\\sum_{i=1}^{m}x_i\\)\n\nFind a direction \\(w \\in \\mathbb{R}^n\\) where \\(\\|w\\| \\leq 1\\)\n\nSuch that the variance of the data along the direction \\(w\\) is maximized \\[\\max_{w:\\|w\\|\\leq 1}\\underbrace{\\frac{1}{m}\\sum_{i=1}^{m}\\left(w^Tx_i-w^T\\mu\\right)^2}_{\\text{variance}}\\]\n\nIt can be easily shown that this equals \\[w^T\\underbrace{\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu \\right)\\left(x_i-\\mu \\right)^T\\right)}_{\\text{covariance matrix C}}w = w^TCw\\] So, the optimization problem becomes \\[\\max_{w:\\|w\\|\\leq 1}w^TCw\\]\n\nThis can be formulated as an eigenvalue problem\n\nGiven a symmetric matrix \\(C \\in \\mathbb{R}^{n\\times n}\\)\n\nFind a row vector \\(w \\in R^n\\) and \\(\\|w\\| = 1\\)\n\nSuch that \\(Cw = \\lambda w\\)\n\n\n\nThere will be multiple solutions of \\(w_1, w_2, ...\\) (eigenvectors) with different \\(\\lambda_1,\\lambda_2,...\\) (eigenvalues)\n\nThey are ortho-normal: \\(w_i^Tw_i = 1, w_i^Tw_j=0\\)\n\n\n\n\nTo find the top \\(k\\) principal components, first find the mean and covariance matrix from the data \\[ \\mu = \\frac{1}{m}\\sum_{i=1}^{m}x_i \\ and \\ C = \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu\\right)\\left(x_i-\\mu\\right)^T\\] calculate the first \\(k\\) eigenvectors \\(w_1,w_2,...,w_k\\) of \\(C\\) corresponding to the largest eigenvalues \\(\\lambda_1,\\lambda_2,...,\\lambda_k\\).\nThen compute the reduced representation \\[z_i = \\left(\\begin{split}w_1^T\\left(x_i-\\mu\\right)/\\sqrt{\\lambda_1}\\cr w_2^T\\left(x_i-\\mu\\right)/\\sqrt{\\lambda_2}\\end{split}\\right)\\]",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#an-r-example",
    "href": "pca.html#an-r-example",
    "title": "16¬† Principal Component Analysis",
    "section": "\n16.3 An R Example",
    "text": "16.3 An R Example\nIn R, the prcomp function is commonly used for performing Principal Component Analysis. Let‚Äôs apply PCA to the Iris dataset as an example. The dataset contains four variables, making it challenging to visualise the three distinct groups across all dimensions.\n\n# Load the required library and data\ndata(iris)\n\n# Perform PCA on the iris data (excluding the Species column)\npca_result &lt;- prcomp(iris[, -5], center = TRUE, scale. = TRUE)\n\n# Print the PCA results\nsummary(pca_result)\n#&gt; Importance of components:\n#&gt;                           PC1    PC2     PC3     PC4\n#&gt; Standard deviation     1.7084 0.9560 0.38309 0.14393\n#&gt; Proportion of Variance 0.7296 0.2285 0.03669 0.00518\n#&gt; Cumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nThe provided results from the PCA can be interpreted as follows:\n\nStandard deviation: This row represents the standard deviation of each principal component. A higher standard deviation signifies a larger spread in the data along that component. In this case, PC1 has the highest standard deviation (1.7084), followed by PC2 (0.9560), PC3 (0.38309), and PC4 (0.14393).\nProportion of Variance: This row displays the proportion of the total variance in the data explained by each principal component. PC1 accounts for 72.96% (0.7296) of the total variance, while PC2 explains 22.85% (0.2285). The remaining components, PC3 and PC4, explain much smaller proportions of the variance, at 3.669% (0.03669) and 0.518% (0.00518), respectively.\nCumulative Proportion: This row shows the cumulative proportion of variance explained by each component and all the preceding components combined. PC1 alone accounts for 72.96% (0.7296) of the total variance. When considering both PC1 and PC2, they together explain 95.81% (0.9581) of the total variance. Adding PC3 increases the cumulative proportion to 99.482% (0.99482), and finally, including PC4, 100% (1.00000) of the total variance is accounted for.\n\nBased on this interpretation, we can conclude that the first two principal components (PC1 and PC2) retain the majority of the information in the data, accounting for 95.81% of the total variance. This simplifies visualisation and further analysis, as most of the variability can be captured using just these two components.\nTo visualise the components, we can use biplot function:\n\nlibrary(ggplot2)\nlibrary(AMR)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\np &lt;- ggplot_pca(pca_result) + \n      theme_minimal() +\n      labs(title = \"PCA Biplot\")\nggplotly(p)\n\n\n\n\n\nA biplot is a valuable visualisation tool in PCA that combines two aspects: - The projection of the original data points onto the first two principal components (PC1 and PC2), and - The projection of the original features as vectors onto the same PC space. The figure above shows the length and width are two orthogonal dimensions of the iris data.\nBy reducing the dimensionality of the data and retaining the most significant information, PCA facilitates easier visualisation, interpretation, and further analysis. However, it is essential to remember that PCA is a linear technique and may not perform well on data with non-linear relationships.\nAdditionally, real-world datasets frequently contain missing values, which should be encoded as NA in R. Unfortunately, PCA is unable to handle missing values, and observations containing NA values will be automatically excluded. This approach is only suitable when the proportion of missing values is relatively low. An alternative solution is to impute missing values, a process discussed in more detail in the handling data problems.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "t-sne.html",
    "href": "t-sne.html",
    "title": "17¬† t-Distributed Stochastic Neighbor Embedding",
    "section": "",
    "text": "17.1 Overview\nt-Distributed Stochastic Neighbour Embedding (t-SNE) is a non-linear dimensionality reduction technique particularly effective for visualising high-dimensional data in a lower-dimensional space. t-SNE is especially useful for data that does not adhere to linear assumptions, where linear techniques like PCA may not provide satisfactory results.\nt-SNE operates by attempting to maintain the relative pairwise distances between data points in the high-dimensional space when they are mapped onto a lower-dimensional space. It does so by minimising the divergence between two probability distributions: one representing pairwise similarities in the high-dimensional space, and the other representing pairwise similarities in the lower-dimensional space.\nThe algorithm involves a stochastic optimisation process, which means that the final results may vary between different runs. Despite this stochastic nature, t-SNE produces visualisations that often reveal the intrinsic structure of the data, such as clusters or groupings.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>t-Distributed Stochastic Neighbor Embedding</span>"
    ]
  },
  {
    "objectID": "t-sne.html#the-mathematics-of-t-sne",
    "href": "t-sne.html#the-mathematics-of-t-sne",
    "title": "17¬† t-Distributed Stochastic Neighbor Embedding",
    "section": "\n17.2 The Mathematics of t-SNE",
    "text": "17.2 The Mathematics of t-SNE\nData in real-life applications can have thousands of dimensions. However, very often, the points lie around a lower-dimensional manifold. In practice, this means that not all features are necessary to represent the data faithfully: by cleverly combining some features, others can be closely approximated.\n\n\n\n\n\n\n\n\nThe problem is, finding this is extremely difficult. There are two main issues. One issue is nonlinearity; the other is the inexact nature of this problem. Putting this into mathematical form, suppose that we have a dataset \\[\n\\mathbf{X} = x_{i=1}^{n}, x_i \\in \\mathbb{R}^n\n\\] and our goal is to find a lower-dimensional representation \\[\n\\mathbf{Y} = y_{i=1}^{n}, y_i \\in \\mathbb{R}^m\n\\] where \\(n\\) is the number of raw features and \\(m\\) is the number of features after dimensionality reduction (\\(m &lt;&lt;&lt; n\\)). Popular methods like PCA only work if new features are linear combinations of the old ones. How can this be done for more complex problems? To provide a faithful lower-dimensional representation, we have one main goal in mind: close points should remain tight, distant points shall stay far.\nt-SNE achieves this by modeling the dataset with a dimension-agnostic probability distribution, finding a lower-dimensional approximation with a closely matching distribution. It was introduced by Laurens van der Maaten and Geoffrey Hinton in their paper Visualizing High-Dimensional Data Using t-SNE.\nSince we also want to capture a possible underlying cluster structure, we define a probability distribution on the \\(\\mathbf{x}_i\\)-s that reflect this. For each data point \\(\\mathbf{x}_j\\), we model the probability of \\(\\mathbf{x}_i\\) belonging to the same class (‚Äúbeing neighbors‚Äù) with a Gaussian distribution:\n\n\n\n\n\n\n\n\nThe variance \\(\\sigma_j\\) is a parameter that is essentially given as an input. We don‚Äôt set this directly. Instead, we specify the expected number of neighbors, called perplexity.\nTo make the optimization easier, these probabilities are symmetrized. With these symmetric probabilities, we form the distribution \\(\\mathcal{P}\\) that represents our high-dimensional data:\n\n\n\n\n\n\n\n\nSimilarly, we define the distribution \\(\\mathcal{Q}\\) for the \\(\\mathbf{y}_i\\)-s, our (soon to be identified) lower-dimensional representation by\n\n\n\n\n\n\n\n\nHere, we model the ‚Äúneighborhood-relation‚Äù with the Student t-distribution. This is where the t in t-SNE comes from.\nOur goal is to find the \\(\\mathbf{y}_i\\)-s through optimization such that \\(\\mathcal{P}\\) and \\(\\mathcal{Q}\\) are as close together as possible. (In a distributional sense.) This closeness is expressed with the Kullback-Leibler divergence, defined by:\n\n\n\n\n\n\n\n\nWe have successfully formulated the dimensionality reduction problem as optimization!\nFrom here, we calculate the gradient of KL divergence with respect to the \\(\\mathbf{y}_i\\)-s and find an optimum with gradient descent. Fortunately, we can calculate the gradient simply:",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>t-Distributed Stochastic Neighbor Embedding</span>"
    ]
  },
  {
    "objectID": "t-sne.html#an-r-example",
    "href": "t-sne.html#an-r-example",
    "title": "17¬† t-Distributed Stochastic Neighbor Embedding",
    "section": "\n17.3 An R Example",
    "text": "17.3 An R Example\nIn R, the Rtsne function from the Rtsne package can be used to perform t-SNE. Before applying t-SNE to the Iris dataset, it‚Äôs necessary to remove any duplicated entries. Here‚Äôs an example of how to do this:\n\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\n# Load the iris dataset\ndata(iris)\n\n# Remove duplicated entries in the iris dataset\niris_unique &lt;- unique(iris)\n\n# Perform t-SNE on the iris data (excluding the Species column)\ntsne_result &lt;- Rtsne(iris_unique[, -5], check_duplicates = FALSE)\ndf_tsne &lt;- data.frame(tsne_result$Y, Species = as.factor(iris_unique$Species))\n\np &lt;- ggplot(df_tsne) +\n  aes(x = X1, y = X2, colour = Species) +\n  geom_point(shape = \"circle\", size = 2) +\n  scale_color_hue(direction = 1) +\n  labs(title = \"t-SNE Clustering\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10L, face = \"bold.italic\"))\n\nggplotly(p)\n\n\n\n\n\nAs with PCA, t-SNE visualisations can help reveal the underlying structure of high-dimensional data, such as clusters, groupings, or other patterns. However, it is essential to consider that the distances between clusters in the lower-dimensional space do not always accurately represent their distances in the high-dimensional space.\nAdditionally, careful selection of hyper parameters can further enhance the quality of the clustering. t-SNE, along with numerous other methods, particularly classification algorithms, possesses two critical parameters that can greatly impact the clustering of the data:\n\nPerplexity: This parameter balances the global and local aspects of the data. A higher perplexity value emphasises global relationships, while a lower value highlights local relationships. Choosing an appropriate perplexity value is essential for obtaining meaningful visualisations.\nIterations: This parameter specifies the number of iterations before the clustering process is halted. Increasing the number of iterations may lead to a more stable and accurate representation of the data. However, it may also increase the computational time required to perform the analysis.\n\nDespite these limitations, t-SNE provides a powerful tool for visualising high-dimensional data, especially when linear techniques like PCA are not suitable.",
    "crumbs": [
      "unsupervised-learn.qmd",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>t-Distributed Stochastic Neighbor Embedding</span>"
    ]
  },
  {
    "objectID": "other-topic-ds.html",
    "href": "other-topic-ds.html",
    "title": "Other Topics in Data Science",
    "section": "",
    "text": "In this fifth part of the book, our focus will be on the the following chapters:\n\nIn 18¬† Basics of time series we will get to understand the basics of the time series.\nIn 19¬† Support Vector Machine we will dive into support vector machine.\nIn 20¬† Tree-based Methods we will understand semi-supervised learning.\nIn 21¬† Basics of Neural Network we will dive into neural networks.",
    "crumbs": [
      "Other Topics in Data Science"
    ]
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "18¬† Basics of time series",
    "section": "",
    "text": "18.1 Overview\nIn today‚Äôs data-driven world, decision-making relies heavily on data analysis. Often, we come across data that is recorded sequentially in time, called time-series data, that drives our decisions. For instance, as an investor, stock prices become a crucial factor influencing our buy or sell decisions. In this section, we will explore how to analyse time-series data using R, with stock prices as a case study.\nTime-series data is collected sequentially over a specific time interval, such as daily, monthly, or yearly. When plotted, it shows patterns like trend, seasonality, or a combination of both. Identifying these patterns can help in making decisions. For instance, recognising a seasonal pattern in stock prices can aid in determining the optimal time to buy or sell. By decomposing the data in R, we can explore these patterns in detail.\nObtaining stock prices in R is easy using the quantmod package, which assists quantitative traders in exploring and building trading models. The package uses Yahoo Finance as its source of stock prices, so it‚Äôs essential to check the ticker of your stock before using it. In this article, we will analyze the stock prices of ‚ÄúApple‚Äù (AAPL) from 2020 to 2022. However, note that stock price analysis is just one of several factors that influence investment decisions, and it‚Äôs crucial to consider other factors as well.\nlibrary(quantmod)\n#&gt; Loading required package: xts\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n#&gt; Loading required package: TTR\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\nprice_aapl &lt;- getSymbols(\"AAPL\", auto.assign=FALSE, from=\"2020-01-01\", to=\"2022-12-31\")\nhead(price_aapl)\n#&gt;            AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted\n#&gt; 2020-01-02   74.0600   75.1500  73.7975    75.0875   135480400      72.96046\n#&gt; 2020-01-03   74.2875   75.1450  74.1250    74.3575   146322800      72.25114\n#&gt; 2020-01-06   73.4475   74.9900  73.1875    74.9500   118387200      72.82685\n#&gt; 2020-01-07   74.9600   75.2250  74.3700    74.5975   108872000      72.48434\n#&gt; 2020-01-08   74.2900   76.1100  74.2900    75.7975   132079200      73.65034\n#&gt; 2020-01-09   76.8100   77.6075  76.5500    77.4075   170108400      75.21474\nThe ts class\nThe ts class is a built-in R class designed specifically for time-series data, and provides a convenient way to store and work with time-series objects. It allows users to perform various time-series operations, such as subsetting, aggregating, and plotting the data, as well as decomposing and forecasting the time-series. The ts class is widely used in the R community, and is an essential tool for any data analyst or researcher working with time-series data.\nBasic time series plots using ts class\nlibrary(ggplot2)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\np &lt;- autoplot(price_aapl$AAPL.Close, color = I(\"blue\"))+\nggtitle(\"Apple, 2020-2022\")+\nxlab(\"Days\")+\nylab(\"Stock Price\")+\n  theme_linedraw()\n\nggplotly(p)\nCreating a time series object with ts()\nlibrary(ggfortify)\n# Convert data_vector to a ts object with start = 2020 and frequency = 4\ntime_series &lt;- ts(price_aapl$AAPL.Close, start = 2020, frequency = 252)\n\n# View time_series\np &lt;- autoplot(time_series, color = I(\"red\"))+\nggtitle(\"Apple, 2020-2022\")+\nxlab(\"Days\")+\nylab(\"Stock Price\")+\n  theme_linedraw()\n\nggplotly(p)\nWhen working with time series data in R, it is important to define the frequency of the data. The frequency refers to the number of observations before the seasonal pattern repeats. Here the 252 days refers to the number of trading days in a year. If you want to use a ts object, then you need to decide which of these is the most important.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Basics of time series</span>"
    ]
  },
  {
    "objectID": "time-series.html#basic-time-series-models",
    "href": "time-series.html#basic-time-series-models",
    "title": "18¬† Basics of time series",
    "section": "\n18.2 Basic time series models",
    "text": "18.2 Basic time series models\n\n18.2.1 White noise and random walk\nQuite often, we may find stock prices are randomly fluctuating without any underlying trend or pattern. We call this phenomenon as white noise, it is a random process where the time series is uncorrelated and the variance is constant over time. In other words, the next value in the series is completely unpredictable and unrelated to the previous value. White noise is often used as a baseline for comparison when evaluating other time series models.\n\nlibrary(ggfortify)\n\n# Simulate a WN model with list(order = c(0, 0, 0))\nwhite_noise&lt;- arima.sim(model = list(order = c(0, 0, 0)), n = 100)\n\n# Plot the WN observations\np &lt;- autoplot(white_noise, color = I(\"red\"))+\nggtitle(\"White Noise\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nOn the other hand, a random walk is a stochastic process where the next value in the series is a random step from the previous value. In this model, the time series has no determinate trend or pattern, but there is a tendency for the price to drift up or down over time.\n\nlibrary(ggfortify)\n\n# Generate a random walk model using arima.sim\nrandom_walk &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 100)\n\n# Plot random_walk\np &lt;- autoplot(random_walk, color = I(\"red\"))+\nggtitle(\"Random Walk\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nIn the context of stock prices, a random walk would suggest that prices are unpredictable and that there is no reliable way to forecast future movements. This is because the future price depends only on the current price and a random step, and not on any other information or factors.\nThe White noise model is a stationary process model, which means its statistical properties such as mean, variance, and covariance remain constant over time. Such models are parsimonious and have distributional stability over time. Weak stationarity is a type of stationarity in which the mean, variance, and covariance remain constant over time. This property can be used to estimate the mean accurately using the sample mean. While many financial time series do not exhibit strict stationarity, they often show changes that are approximately stationary, with random oscillations around some fixed level known as mean-reversion.\nThe white noise and random walk models are related, with the latter being non-stationary. A mean-zero white noise process when cumulatively summed results in a random walk process, while a non-zero mean white noise process when cumulatively summed leads to a random walk process. Let‚Äôs see them in a plot.\n\nlibrary(cowplot)\nlibrary(ggfortify)\nlibrary(plotly)\n\n# Use arima.sim() to generate white noise data\nwhite_noise &lt;- arima.sim(model = list(order = c(0, 0, 0)), n = 100)\n\n# Use cumsum() to convert your white noise data to a random walk\nrandom_walk &lt;- ts(cumsum(white_noise))\n  \n# Use arima.sim() to generate white noise drift data\nwn_drift &lt;- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 0.4)\n  \n# Use cumsum() to convert your white noise drift data to random walk\nrw_drift &lt;- ts(cumsum(wn_drift))\n\n# Plot all four data objects\np1 &lt;- autoplot(white_noise, color = I(\"red\"))+\nggtitle(\"White Noise\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\np2 &lt;- autoplot(random_walk, color = I(\"red\"))+\nggtitle(\"White Noise Data to Random Walk\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\np3 &lt;- autoplot(wn_drift, color = I(\"red\"))+\nggtitle(\"White Noise Drift Data\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\np4 &lt;- autoplot(rw_drift, color = I(\"red\"))+\nggtitle(\"White Noise Drift Data to Random Walk\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nplot_grid(p1, p2, p3, p4, cols = 1)\n#&gt; Warning in plot_grid(p1, p2, p3, p4, cols = 1): Argument 'cols' is\n#&gt; deprecated. Use 'ncol' instead.\n\n\n\n\n\n\n\nAs you can see, it is easy to reverse-engineer the random walk data by simply generating a cumulative sum of white noise data.\nExample: characteristics of financial time series\nScatterplots are used in financial time series analysis to understand the relationship between asset prices and returns. The primary goal of investing is to make a profit, and returns measure the changes in price relative to the initial price over a given period.\n\nlibrary(ggfortify)\n\n# Use this code to convert prices to daily returns, the sequence length is 756 trading days \nreturns &lt;- time_series[-1,] / time_series[-756,] - 1\n\n# Convert returns to ts\nreturns &lt;- ts(returns, start = c(2020, 0), frequency = 252)\n\n# Plot returns\np &lt;- autoplot(returns, color = I(\"red\"))+\nggtitle(\"Returns\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nLog returns, also known as continuously compounded returns, are commonly used in financial time series analysis. They are the log of gross returns, which are the changes or first differences in the logarithm of prices. The difference between daily prices and returns is substantial, while the difference between daily returns and log returns is usually small. One advantage of using log returns is that calculating multi-period returns from individual periods is simplified as they can be added together.\n\nlibrary(ggfortify)\n\n# Use this code to convert prices to log returns\nlogreturns &lt;- diff(log(time_series))\n\n# Plot logreturns\np &lt;- autoplot(logreturns, color = I(\"red\"))+\nggtitle(\"Log Returns\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nDaily net returns and daily log returns are two valuable metrics for financial data.\n\nlibrary(ggplot2)\nlibrary(plotly)\n\napple_percentreturns &lt;- returns * 100\n\napple_df &lt;- data.frame(Y=as.matrix(apple_percentreturns), date=time(apple_percentreturns))\n\n# Display a histogram of percent returns\np &lt;- ggplot(apple_df) +\n  aes(x = Y) +\n  geom_histogram(aes(y = ..density..), bins = 50L, \n                     fill = \"white\", colour = 1) +\n      theme_linedraw() + \n  geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25) +\n  labs(\n    x = \"Percentage Returns\",\n    y = \"Frequency\",\n    title = \"Histogram of Apple Percentage Returns\"\n  )\n\nggplotly(p)\n#&gt; Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n#&gt; ‚Ñπ Please use `after_stat(density)` instead.\n#&gt; ‚Ñπ The deprecated feature was likely used in the ggplot2 package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/ggplot2/issues&gt;.\n\n\n\n\n\nThe average daily return for AAPL is about 0.09% over the last 3 years with a standard deviation as high as 2.32%. We are able to check the normality of the distribution using the normal quantile plots:\n\nlibrary(ggplot2)\nlibrary(plotly)\n\np &lt;- ggplot(apple_df, aes(sample = Y)) + \n      stat_qq() + \n      stat_qq_line() + \n      theme_minimal()\n\nggplotly(p)\n\n\n\n\n\n#qqnorm(apple_percentreturns)\n#qqline(apple_percentreturns)\n\nNote that the vast majority of returns are near zero, but some daily returns are greater than 5 percentage points in magnitude. Similarly, note the clear departure from normality, especially in the tails of the distributions, as evident in the normal quantile plots.\nThere are different types of patterns that may appear in time series data. Some data may not have a clear trend, while others may exhibit linear, positive or negative autocorrelation, rapid growth or decay, periodicity, or changes in variance. To analyse and model time series data, it is often necessary to apply transformations to remove or adjust for these patterns.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Basics of time series</span>"
    ]
  },
  {
    "objectID": "time-series.html#models-with-seasonal-trend",
    "href": "time-series.html#models-with-seasonal-trend",
    "title": "18¬† Basics of time series",
    "section": "\n18.3 Models with seasonal trend",
    "text": "18.3 Models with seasonal trend\nDifferencing is a useful technique for removing trends in a time series. It involves calculating the changes between successive observations over time. The first difference, or change series, can be obtained in R using the diff() function. This series allows for the examination of the increments or changes in a given time series and always has one fewer observations than the original series. By removing the linear trend, differencing can help to stabilise the level of a time series.\n\n# Generate the first difference of apple time_series\ndz &lt;- diff(time_series)\n  \n# Plot dz\np &lt;- autoplot(dz, color = I(\"red\"))+\nggtitle(\"First Difference\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nBy removing the long-term time trend, you can view the amount of change from one observation to the next.\nSeasonal trends can be removed by applying seasonal differencing to time series. Seasonal patterns can be seen in time series data that exhibit a strong pattern that repeats every fixed interval of time, such as a yearly or quarterly cycle. Seasonal differencing is applied to remove these periodic patterns. This method is useful when changes in behaviour from year to year may be more important than changes from month to month, which may largely follow the overall seasonal pattern. The function diff(..., lag = s) can be used to calculate the lag s difference or length s seasonal change series. An appropriate value of s for monthly or quarterly data would be 12 or 4, respectively. The diff() function defaults to a lag of 1 for first differencing. The seasonally differenced series will have s fewer observations than the original series.\n\n# Generate a diff of time_series with n = 30. Save this to dx\ndx &lt;- diff(time_series, lag = 30)\n  \n# Plot dx\np &lt;- autoplot(dx, color = I(\"red\"))+\nggtitle(\"Difference in Time Series with n = 30\")+\nxlab(\"Time\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nOnce again differencing allows you to remove the longer-term time trend - in this case, seasonal volatility - and focus on the change from one month to another.\nAutocorrelation is a statistical method used in time series analysis to identify patterns of dependence between observations separated by a given lag. In other words, autocorrelation measures the degree to which a time series is correlated with itself at different time lags. It can be used to identify the presence of trends, seasonality, and other patterns in time series data. It is an important tool in understanding the behaviour of time series data and can be used to develop models for forecasting future values of the series.\nTo assess the relationship between a time series and its past, autocorrelations can be estimated at different lags. The acf() function can estimate autocorrelations from lag 0 up to a specified value using the lag.max argument. The acf() function can also be used to explore other applications. Let‚Äôs see in action:\n\n#Assesing the autocorrelation upto 100 lags.\naapl_acf &lt;- acf(returns, lag.max = 100, plot = FALSE)\n\np &lt;- autoplot(aapl_acf, color = I(\"red\"))+\nggtitle(\"ACF\")+\nxlab(\"Lag\")+\nylab(\"Values\")+\n  theme_linedraw()\n\nggplotly(p)\n\n\n\n\n\nIn this example, we can see the value of return is most correlated with the return one day before, and drops quick to approximately zero. A zero autocorrelation means that the series is random and that there is no pattern or trend in its evolution. On the other hand, if we observe a positive autocorrelation, it will indicate that the series is positively correlated with its lagged values, while a negative autocorrelation indicates that the series is negatively correlated with its lagged values.\nTo apply autocorrelation and differencing, there are several types of models that can be used . Three popular types of time series models are the Autoregressive (AR)¬†model, the Moving Average (MA)¬†model, and the Autoregressive Integrated Moving Average (ARIMA) model.\nThe Autoregressive (AR(p)) model is defined as:\n\\[ x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + \\cdots + \\phi_p x_{t-p} + w_t, \\]\nwhere \\(x_t\\) is the value of the time series at time \\(t\\), \\(w_t\\) is the error term at time \\(t\\), and \\(\\phi_1, \\phi_2, \\cdots, \\phi_p\\) are the autoregressive coefficients. The parameter \\(p\\) is the order of the AR model. This equation represents the linear relationship between the current value of the time series and its past values, where the value at each lag is weighted by its corresponding coefficient. The error term accounts for any unexplained variation in the time series that is not accounted for by the AR terms. The AR model is widely used in time series analysis, and it is a special case of the more general ARIMA models.\nIn R, the arima() function can also be used to estimate an AR model. To estimate an AR(p) model, you can set the order parameter to c(p,0,0)\n\n# Estimate an AR(1) model\narima(returns, order = c(1, 0, 0))\n#&gt; \n#&gt; Call:\n#&gt; arima(x = returns, order = c(1, 0, 0))\n#&gt; \n#&gt; Coefficients:\n#&gt;           ar1  intercept\n#&gt;       -0.1507      1e-03\n#&gt; s.e.   0.0360      7e-04\n#&gt; \n#&gt; sigma^2 estimated as 0.0005284:  log likelihood = 1777.18,  aic = -3548.37\n\nHere‚Äôs a breakdown of the output:\n\nCoefficients: the estimated coefficients for the AR model. In this case, the model has one AR term, represented as ar1, and an intercept term. The value of ar1 is -0.1507, indicating a negative correlation between the current value of the time series and its past value. The intercept term is 1e-03, which represents the estimated mean of the returns.\ns.e.: the standard errors of the estimated coefficients.\nsigma^2 estimated as: the estimated variance of the error term in the AR model. In this case, the estimated value is 0.0005284.\nlog likelihood: the log-likelihood of the estimated model, which is a measure of how well the model fits the data. In this case, the log-likelihood is 1777.18.\naic: the Akaike Information Criterion (AIC) of the estimated model, which is a measure of the model‚Äôs quality that balances the goodness of fit and the model complexity. The lower the AIC, the better the model. In this case, the AIC is -3548.37.\n\nThe second model is the moving average (MA(q)) model, which is defined as:\n\\[ x_t = w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2} + \\cdots + \\theta_q w_{t-q}, \\]\nwhere \\(x_t\\) is the value of the time series at time \\(t\\), \\(w_t\\) is the error term at time \\(t\\), and \\(\\theta_1, \\theta_2, \\cdots, \\theta_q\\) are the moving average coefficients. The parameter \\(q\\) is the order of the MA model. This equation represents the linear relationship between the current value of the time series and its past error terms, where the value at each lag is weighted by its corresponding coefficient. The error term accounts for any unexplained variation in the time series that is not accounted for by the MA terms.\n\n# Estimate an MA(2) model\narima(returns, order = c(0, 0, 2))\n#&gt; \n#&gt; Call:\n#&gt; arima(x = returns, order = c(0, 0, 2))\n#&gt; \n#&gt; Coefficients:\n#&gt;           ma1     ma2  intercept\n#&gt;       -0.1495  0.0189      1e-03\n#&gt; s.e.   0.0363  0.0362      7e-04\n#&gt; \n#&gt; sigma^2 estimated as 0.0005285:  log likelihood = 1777.12,  aic = -3546.23\n\nIn this case, the model has two MA terms, represented as ma1 and ma2, and an intercept term. The value of ma1 is -0.1495 and ma2 is 0.0189, indicating a negative correlation between the current value of the time series and its past two error terms.\nFinally, the Autoregressive Integrated Moving Average (ARIMA) is a model that combines the autoregressive (AR) model, the moving average (MA) model, and differencing to handle non-stationary data. It can be used for time series analysis, forecasting, and modelling various economic, financial, and scientific phenomena. The ARIMA(p,d,q) model is defined as:\n\\[\\left(1 - \\sum_{i=1}^{p} \\phi_i L^i\\right)(1 - L)^d x_t = \\left(1 + \\sum_{i=1}^{q} \\theta_i L^i\\right) w_t,\\]\nwhere \\(x_t\\) is the value of the time series at time \\(t\\), \\(w_t\\) is the error term at time \\(t\\), \\(L\\) is the lag operator such that \\(L^i x_t = x_{t-i}\\), and \\(\\phi_i\\), \\(\\theta_i\\) are the AR and MA coefficients, respectively. The parameter \\(p\\) is the order of the AR component, \\(d\\) is the degree of differencing, and \\(q\\) is the order of the MA component.\n\n# Estimate an ARIMA(1,1,1) model\narima(returns, order = c(1, 1, 1))\n#&gt; \n#&gt; Call:\n#&gt; arima(x = returns, order = c(1, 1, 1))\n#&gt; \n#&gt; Coefficients:\n#&gt;           ar1      ma1\n#&gt;       -0.1514  -0.9961\n#&gt; s.e.   0.0361   0.0044\n#&gt; \n#&gt; sigma^2 estimated as 0.00053:  log likelihood = 1771.1,  aic = -3536.19\n\nWe can see the model has one AR term, represented as ar1, one MA term, represented as ma1, and no intercept term. The value of ar1 is -0.1514, indicating a negative correlation between the current value of the time series and its past value up to 1 lag. The value of ma1 is -0.9961, indicating a strong negative correlation between the current value of the time series and its past error term.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Basics of time series</span>"
    ]
  },
  {
    "objectID": "time-series.html#ar-model-estimation-and-forecasting",
    "href": "time-series.html#ar-model-estimation-and-forecasting",
    "title": "18¬† Basics of time series",
    "section": "\n18.4 AR model estimation and forecasting¬†**",
    "text": "18.4 AR model estimation and forecasting¬†**\nIn this section, we will explore a new example from the R base library - the AirPassengers dataset. AirPassengers is a pre-existing time series dataset in R, which includes the monthly total number of airline passengers from January 1949 to December 1960. The dataset is organised as a time series object in R, where the passenger counts represent the data and the time index represents the time dimension.\n\np &lt;- autoplot(AirPassengers, geom = \"line\") + \n      labs(x=\"Time\", y =\"Passenger numbers ('000)\", title=\"Air Passengers from 1949 to 1961\") +\n      theme_minimal()\nggplotly(p)\n\n\n\n\n\nCompare to the AAPL stock price series, the AirPassengers is a stationary time series with predictable pattern. To test whether a time series is a random walk, you can perform an augmented Dickey-Fuller (ADF) test. The ADF test is a statistical hypothesis test that tests whether a unit root is present in a time series, which is a key characteristic of a random walk process. For example, for the AAPL series, we have:\n\nlibrary(tseries)\nadf.test(time_series) #AAPL time series\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  time_series\n#&gt; Dickey-Fuller = -1.2623, Lag order = 9, p-value = 0.8906\n#&gt; alternative hypothesis: stationary\n\nThe p-value is greater than the significance level of 0.05, which suggests that there is not enough evidence to reject the null hypothesis that the data has a unit root and is non-stationary (i.e., is a random walk process). On the other hand:\n\nadf.test(AirPassengers)\n#&gt; Warning in adf.test(AirPassengers): p-value smaller than printed p-value\n#&gt; \n#&gt;  Augmented Dickey-Fuller Test\n#&gt; \n#&gt; data:  AirPassengers\n#&gt; Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01\n#&gt; alternative hypothesis: stationary\n\nWe can reject the null hypothesis as p-value &lt; 0.05 and confirm the AirPassengers series is not a random walk.\nNow, let‚Äôs fit a simple AR model:\n\n# Fit the AR model to AirPassengers\nAR = arima(AirPassengers, order = c(1, 0, 0))\nAR\n#&gt; \n#&gt; Call:\n#&gt; arima(x = AirPassengers, order = c(1, 0, 0))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1  intercept\n#&gt;       0.9646   278.4649\n#&gt; s.e.  0.0214    67.1141\n#&gt; \n#&gt; sigma^2 estimated as 1119:  log likelihood = -711.09,  aic = 1428.18\n\nWe can plot the fitted value with the true value of the time series as using the following codes, where the red dots are the fitted values and black lines are the observed values.\n\nts.plot(AirPassengers)\nAR_fitted &lt;- AirPassengers - residuals(AR)\npoints(AR_fitted, type = \"l\", col = 2, lty = 2)\n\n\n\n\n\n\n\nFitting an AR model to the AirPassengers data in a reproducible fashion has allowed us to successfully model the data. This model can be used to predict future observations based on the fitted AR data.\n\n# Use predict() to make forecast for next 10 steps\npredict(AR, n.ahead = 10)\n#&gt; $pred\n#&gt;           Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n#&gt; 1961 426.5698 421.3316 416.2787 411.4045 406.7027 402.1672 397.7921 393.5717\n#&gt;           Sep      Oct\n#&gt; 1961 389.5006 385.5735\n#&gt; \n#&gt; $se\n#&gt;           Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n#&gt; 1961 33.44577 46.47055 55.92922 63.47710 69.77093 75.15550 79.84042 83.96535\n#&gt;           Sep      Oct\n#&gt; 1961 87.62943 90.90636\n\nWe can plot the forecasted value with the standard error as:\n\n# Run to plot the Nile series plus the forecast and 95% prediction intervals\nts.plot(AirPassengers, xlim = c(1955, 1962))\nAR_forecast &lt;- predict(AR, n.ahead = 10)$pred\nAR_forecast_se &lt;- predict(AR, n.ahead = 10)$se\npoints(AR_forecast, type = \"l\", col = 2)\npoints(AR_forecast - 2*AR_forecast_se, type = \"l\", col = 2, lty = 2)\npoints(AR_forecast + 2*AR_forecast_se, type = \"l\", col = 2, lty = 2)\n\n\n\n\n\n\n\nBased on the data available, the predictions for the period from Jan to Oct 1961 looks only captures the decline in the first month and assuming the decline will continue. The wider confidence band (represented by dotted lines) is a reflection of the low persistence in the AirPassengers data. We are able to improve the model by increasing the number of AR terms:\n\nAR = arima(AirPassengers, order = c(5, 0, 0))\nprint(AR)\n#&gt; \n#&gt; Call:\n#&gt; arima(x = AirPassengers, order = c(5, 0, 0))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ar1      ar2     ar3      ar4     ar5  intercept\n#&gt;       1.3062  -0.5299  0.1417  -0.1779  0.2407   289.3494\n#&gt; s.e.  0.0814   0.1385  0.1465   0.1405  0.0862    95.8814\n#&gt; \n#&gt; sigma^2 estimated as 889.9:  log likelihood = -695.12,  aic = 1404.24\nts.plot(AirPassengers, xlim = c(1955, 1962))\nAR_forecast &lt;- predict(AR, n.ahead = 10)$pred\nAR_forecast_se &lt;- predict(AR, n.ahead = 10)$se\npoints(AR_forecast, type = \"l\", col = 2)\npoints(AR_forecast - 2*AR_forecast_se, type = \"l\", col = 2, lty = 2)\npoints(AR_forecast + 2*AR_forecast_se, type = \"l\", col = 2, lty = 2)\n\n\n\n\n\n\n\nWe can see an improvement of log likelihood by increasing the number of AR terms from 1 to 5.\nThe forecasted trend is also more reasonable, which resembles the pattern of the previous observed trends.\nNow lets compare the AR model with a simple MA model:\n\nMA = arima(AirPassengers, order = c(0, 0, 5))\nprint(MA)\n#&gt; \n#&gt; Call:\n#&gt; arima(x = AirPassengers, order = c(0, 0, 5))\n#&gt; \n#&gt; Coefficients:\n#&gt;          ma1     ma2     ma3     ma4     ma5  intercept\n#&gt;       1.7132  1.9434  1.9189  1.7217  0.9097   283.1498\n#&gt; s.e.  0.0715  0.1081  0.1076  0.1193  0.0815    22.0011\n#&gt; \n#&gt; sigma^2 estimated as 850.1:  log likelihood = -697.37,  aic = 1408.75\nts.plot(AirPassengers, xlim = c(1955, 1962))\nMA_forecast &lt;- predict(MA, n.ahead = 10)$pred\nMA_forecast_se &lt;- predict(MA, n.ahead = 10)$se\npoints(MA_forecast, type = \"l\", col = 2)\npoints(MA_forecast - 2*MA_forecast_se, type = \"l\", col = 2, lty = 2)\npoints(MA_forecast + 2*MA_forecast_se, type = \"l\", col = 2, lty = 2)\n\n\n\n\n\n\n\nNote that the MA model can only produce a q-step forecast (where q is the MA component of the MA model). For additional forecasting periods, the predict() command simply extends the original q-step forecast. This explains the unexpected horizontal lines after Jun 1961.\nCompare AR and MA models Comparing AR and MA models involves assessing their goodness of fit using information criterion measures in model fitting outputs. The lower the AIC values, the better the fit of the model.\nAutoregressive (AR) and Moving Average (MA) are two popular methods for modelling time series. To determine which model is more appropriate in practice, you can calculate the AIC values for each model. These measures penalise models with more estimated parameters to avoid overfitting, and smaller values are preferred. If all other factors are equal, a model with a lower AIC value than another model is considered to be a better fit.\nAlthough the predictions from both models are very similar (indeed, they have a correlation coefficient of 0.92), the AIC indicate that the AR model is a slightly better fit for the AirPassengers data.\n\ncor(AR_forecast, MA_forecast)\n#&gt; [1] 0.9233236\n\n\n18.4.1 Advanced time series analytics in R\nIf you‚Äôre interested in further exploring time series analysis in R, here are some recommendations for next steps:\n\nExplore more advanced time series models: In addition to the basic models covered in this course, there are many more advanced time series models available in R, such as Seasonal ARIMA, Vector Autoregression (VAR), and Dynamic Factor Models.\nIncorporate external variables: Time series data can often be affected by external variables, such as holidays or economic events. Consider learning how to incorporate external variables into your time series models using techniques like regression with ARIMA errors or vector autoregression (VAR).\nConsider machine learning approaches: In addition to traditional time series models, machine learning techniques such as neural networks, random forests, and support vector machines can also be used for time series forecasting.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Basics of time series</span>"
    ]
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "19¬† Support Vector Machine",
    "section": "",
    "text": "19.1 Overview\nA widely used nonparametric supervised learning algorithm for classification and regression tasks is the Support Vector Machine (SVM). Introduced in the 1990s by Vapnik and colleagues, SVM aims to find a hyperplane that best separates the input data into different classes.\nSVM has several advantages over other classification algorithms. It can handle high-dimensional data well, is less prone to overfitting, and can handle nonlinear data using the kernel trick. Overall, SVM is a powerful and flexible machine learning algorithm applicable in various fields, such as text classification, image classification, and bioinformatics.\nTo illustrate the fundamentals of Support Vector Machines (SVM), we can use a straightforward example in R (‚ÄúESL.mixture.rda‚Äù from the textbook Elements of Statistical Learning). Suppose we have two categories, ‚Äúred‚Äù and ‚Äúblack,‚Äù and our dataset has two attributes, ‚Äúx‚Äù and ‚Äúy.‚Äù We aim to construct a classifier that, given a set of (x,y) coordinates, can predict if it belongs to the ‚Äúred‚Äù or ‚Äúblack‚Äù category. We can plot our labeled training data on a plane using the following R code:\nload(file = \"data/ESL.mixture.rda\")\nnames(ESL.mixture)\n#&gt; [1] \"x\"        \"y\"        \"xnew\"     \"prob\"     \"marginal\" \"px1\"     \n#&gt; [7] \"px2\"      \"means\"\nrm(x, y)\n#&gt; Warning in rm(x, y): object 'x' not found\n#&gt; Warning in rm(x, y): object 'y' not found\nattach(ESL.mixture)\nplot(x, col = y + 1)\nWe aim to find a decision boundary that separates these two categories as accurately as possible. This boundary is called the hyperplane in SVM and can be a linear or nonlinear function of the attributes ‚Äúx‚Äù and ‚Äúy.‚Äù\nTo draw a sample decision boundary in the above code, we can use the svm function from the e1071 package in R. Here is an example code that fits an SVM model with a linear boundary:\nlibrary(e1071)\n\n# Fit an SVM model with a linear kernel\ndat = data.frame(y = factor(y), x)\nfit = svm(factor(y) ~ ., data = dat, scale = FALSE, kernel = \"radial\", cost = 5)\n\n# Make the plot\nxgrid = expand.grid(X1 = px1, X2 = px2)\nygrid = predict(fit, xgrid)\nplot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)\npoints(x, col = y + 1, pch = 19)\nThe SVM model aims to maximise the margin between the decision boundary and the closest training points from each category (i.e., support vectors). The kernel used in this example allows for a linear decision boundary.\nNow let‚Äôs move on to the non-linear version of SVM. You‚Äôre going to use the kernel support vector machine to try and learn that boundary.\ndat = data.frame(y = factor(y), x)\nfit = svm(factor(y) ~ ., data = dat, scale = FALSE, kernel = \"linear\", cost = 5)\nfit\n#&gt; \n#&gt; Call:\n#&gt; svm(formula = factor(y) ~ ., data = dat, kernel = \"linear\", cost = 5, \n#&gt;     scale = FALSE)\n#&gt; \n#&gt; \n#&gt; Parameters:\n#&gt;    SVM-Type:  C-classification \n#&gt;  SVM-Kernel:  linear \n#&gt;        cost:  5 \n#&gt; \n#&gt; Number of Support Vectors:  124\nThe decision boundary, to a large extent, follows where the data is, but in a very non-linear way.\nSupport Vector Machines (SVMs) are a type of supervised classifier that can partition a feature space into multiple groups based on known class labels. In simple cases, the separation boundary is linear and leads to groups that are split up by lines or planes in high-dimensional spaces. However, in more complex cases where groups are not nicely separated, SVMs can use a kernel function to carry out non-linear partitioning. This makes them sophisticated and capable classifiers but also prone to overfitting. Despite the black box nature of SVMs, they are useful in situations where the data is non-linearly separated, and their ability to automatically transform data for linear separation makes them a powerful tool. While they may be difficult to interpret, understanding SVMs provides an alternative to GLMs and decision trees for classification.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "semi-sup.html",
    "href": "semi-sup.html",
    "title": "20¬† Tree-based Methods",
    "section": "",
    "text": "20.1 Decision Tree\nA decision tree is a type of supervised learning algorithm used in machine learning and data mining. It is a tree-like structure where each internal node represents a decision or test on an attribute or feature of the data, and each branch represents an outcome or decision based on the result of the test. The leaves of the tree represent the final outcomes or classifications. Decision trees are used for both classification and regression tasks. To illustrate, we use the iris dataset and the rpart.plot library in R:\nlibrary(rpart)\ndata(iris)\n\n# Create a decision tree model using the rpart package\nmodel &lt;- rpart(Species ~ ., data = iris)\n\n# Plot the decision tree using the rpart.plot package\nlibrary(rpart.plot)\nrpart.plot(model, main=\"Decision Tree Example\", box.palette = \"RdYlBu\", nn = TRUE,\n           branch.lty = 3, under = TRUE, cex=0.8, tweak=1.5, fallen.leaves = TRUE,\n           compress=TRUE, box.col=c(\"white\", \"white\", \"white\"), split.cex=0.8)\n#&gt; Warning: cex and tweak both specified, applying both\nThis will generate a plot of the decision tree model. Here‚Äôs a description of what each label represents:\nThe construction of a decision tree involves recursively splitting the data into subsets based on the values of one of the features or attributes, in a way that maximises the information gain at each split. The information gain is a measure of how much the uncertainty or entropy of the data is reduced by the split. The process continues until the data in each leaf node is homogeneous or has the same class, or until some stopping criterion is met.\nOne of the advantages of decision trees is that they are easy to interpret and visualise, making them a popular choice for exploratory data analysis. They can also handle both categorical and numerical data, and can handle missing values by assigning probabilities to each possible outcome. However, decision trees can be prone to overfitting, especially when the tree is too complex or the data is noisy. Pruning techniques and other methods can be used to address this issue. Overfitting and Bias-variance tradeoff Overfitting and the bias-variance tradeoff are important concepts in machine learning that relate to the ability of a model to generalise to new data.\nOverfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new data. This can happen when a model is too flexible and captures noise in the data, rather than the underlying patterns. Underfitting, on the other hand, occurs when a model is too simple and fails to capture the underlying patterns in the data. This results in poor performance on both the training data and new data.\nThe bias-variance tradeoff describes the relationship between model complexity and performance. Bias refers to the error introduced by approximating a real-world problem with a simpler model, while variance refers to the amount by which the model would change if it was trained on a different set of data. High bias models are typically too simple and under-fit the data, while high variance models are too complex and overfit the data.\nIn order to find the optimal balance between bias and variance, it is important to consider the complexity of the model and the size and quality of the dataset. This can be achieved through techniques such as cross-validation and regularisation.\nHere is an example in R:\n# Generate a noisy sine wave\nx &lt;- seq(0, 2*pi, length.out = 50)\ny &lt;- sin(x) + rnorm(50, sd = 0.1)\n\n# Fit polynomial models of varying degrees\nfit1 &lt;- lm(y ~ poly(x, 1))\nfit2 &lt;- lm(y ~ poly(x, 2))\nfit3 &lt;- lm(y ~ poly(x, 5))\nfit4 &lt;- lm(y ~ poly(x, 10))\n\n# Plot the data and models\nplot(x, y, main = \"Fitting polynomial models of varying degrees\")\nlines(x, predict(fit1), col = \"red\", lwd = 2)\nlines(x, predict(fit2), col = \"blue\", lwd = 2)\nlines(x, predict(fit3), col = \"green\", lwd = 2)\nlines(x, predict(fit4), col = \"purple\", lwd = 2)\n\n# Calculate the training error for each model\nmse1 &lt;- mean((y - predict(fit1))^2)\nmse2 &lt;- mean((y - predict(fit2))^2)\nmse3 &lt;- mean((y - predict(fit3))^2)\nmse4 &lt;- mean((y - predict(fit4))^2)\n\n# Print the training errors\ncat(\"MSE for linear model: \", mse1, \"\\n\")\n#&gt; MSE for linear model:  0.2374216\ncat(\"MSE for quadratic model: \", mse2, \"\\n\")\n#&gt; MSE for quadratic model:  0.2370386\ncat(\"MSE for fifth-degree polynomial: \", mse3, \"\\n\")\n#&gt; MSE for fifth-degree polynomial:  0.007500802\ncat(\"MSE for tenth-degree polynomial: \", mse4, \"\\n\")\n#&gt; MSE for tenth-degree polynomial:  0.006737454\nIn this example, we generate a noisy sine wave and fit polynomial models of varying degrees to the data. As the degree of the polynomial increases, the models become more complex and better fit the training data. However, as the complexity increases, the models become more prone to overfitting and perform poorly on new data. The mean squared error (MSE) is used as a measure of the training error for each model, and we can see that the MSE decreases as the complexity increases. However, the optimal degree of the polynomial is likely to be somewhere between the linear and tenth-degree models, depending on the size and quality of the dataset.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "semi-sup.html#decision-tree",
    "href": "semi-sup.html#decision-tree",
    "title": "20¬† Tree-based Methods",
    "section": "",
    "text": "Node: A point in the tree where a decision is made based on a feature of the data.\n\nBranch: A path from one node to another that represents the possible outcomes based on the decision made at the previous node.\n\nSplit criterion: The metric used to determine the best feature to split on at each node. This is often based on measures of impurity or information gain.\n\nLeaf node: The end point of a branch that represents the final outcome or decision based on the previous splits.\n\nPrediction: The predicted class label for the given input, based on the decision tree model.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "semi-sup.html#random-forest-and-bagging",
    "href": "semi-sup.html#random-forest-and-bagging",
    "title": "20¬† Tree-based Methods",
    "section": "\n20.2 Random forest and bagging",
    "text": "20.2 Random forest and bagging\nRandom Forest is an extension of the decision tree algorithm that uses an ensemble of decision trees to improve the accuracy and robustness of the model. The main difference between Random Forest and a single decision tree is that Random Forest creates multiple trees instead of a single tree, and each tree is trained on a random subset of the data. This results in a more diverse set of trees that are less likely to overfit to the training data. The final prediction of the Random Forest model is based on the combined predictions of all the trees in the forest. Compared to a single decision tree, Random Forest generally has better accuracy and is less prone to overfitting. We will illustrate the construction of a random forest in the exercise material.\nRandom Forest is trained on different subsets of the training data and bagging comes into play when selecting these subsets. Bagging, short for Bootstrap Aggregating, is an ensemble method in machine learning used to improve the performance and stability of a model. In the context of Random Forest, bagging is one of the key components that contribute to the robustness of the algorithm.\nBagging helps to reduce overfitting by introducing diversity among the individual trees in the ensemble. Since each tree is trained on a different subset of the data, they will capture different aspects of the data and make different errors. When the results from all trees are combined, usually by majority vote for classification or averaging for regression, the ensemble prediction tends to be more accurate and stable than that of any single tree.\nHere is an R example:\n\nlibrary(randomForest)\n#&gt; randomForest 4.7-1.1\n#&gt; Type rfNews() to see new features/changes/bug fixes.\nlibrary(ggplot2)\n#&gt; \n#&gt; Attaching package: 'ggplot2'\n#&gt; The following object is masked from 'package:randomForest':\n#&gt; \n#&gt;     margin\n\nrf &lt;- randomForest(Species ~ Petal.Length + Petal.Width, data = iris, \n                   proximity = TRUE)\n\nrf_df &lt;- expand.grid(Petal.Width = seq(0, 3, length.out = 100),\n                  Petal.Length = seq(0, 7, length.out = 100))\n\nrf_df$Species &lt;- predict(rf, rf_df)\n\np &lt;- ggplot(iris, aes(Petal.Width, Petal.Length, fill = Species)) +\n  geom_raster(data = rf_df, alpha = 0.5) +\n  geom_point(shape = 21, size = 3) +\n  theme_minimal()\n\np",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "semi-sup.html#boosted-trees-and-gradient-boosting",
    "href": "semi-sup.html#boosted-trees-and-gradient-boosting",
    "title": "20¬† Tree-based Methods",
    "section": "\n20.3 Boosted trees and gradient boosting",
    "text": "20.3 Boosted trees and gradient boosting\nBoosting is an ensemble method in machine learning that combines weak classifiers into a single strong classifier. The main idea behind boosting is to iteratively train weak learners, usually decision trees, by assigning weights to the training data. At each iteration, the algorithm focuses on the samples that were misclassified in the previous round by assigning them higher weights. Then, a new weak learner is trained on this re-weighted dataset. The final strong classifier is a weighted combination of the individual weak classifiers.\nHere‚Äôs an example using the R and the AdaBoost algorithm, a popular boosting method. We will use the ‚Äúadabag‚Äù library and the famous ‚Äúiris‚Äù dataset to illustrate the concept:\n\nlibrary(adabag)\n#&gt; Loading required package: caret\n#&gt; Loading required package: lattice\n#&gt; Loading required package: foreach\n#&gt; Loading required package: doParallel\n#&gt; Loading required package: iterators\n#&gt; Loading required package: parallel\n\n#Load the iris dataset and split it into training and testing sets:\ndata(iris)\nset.seed(42)\nindices &lt;- sample(1:nrow(iris), size = 0.7 * nrow(iris))\ntrain_set &lt;- iris[indices,]\ntest_set &lt;- iris[-indices,]\n\n#Train the AdaBoost model:\niris_boost &lt;- boosting(Species ~ ., data = train_set, boos = TRUE, mfinal = 100)\n\n#Make predictions using the trained model:\npredictions &lt;- predict(iris_boost, newdata = test_set)\n\n#Evaluate the model's performance:\nconfusion_matrix &lt;- table(predictions$class, test_set$Species)\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(accuracy)\n#&gt; [1] 0.9555556\n\nBoth Boosted Trees and Random Forests are powerful ensemble methods that use decision trees as base learners. They have different training strategies and strengths, which makes them suitable for different situations. Boosted Trees tend to perform well when the weak learners can be combined effectively to create a strong classifier, while Random Forests excel when many independent trees can be averaged to reduce variance and improve generalisation. It is often beneficial to try both methods on a given dataset and choose the one that provides the best performance based on the specific problem and evaluation metrics.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Tree-based Methods</span>"
    ]
  },
  {
    "objectID": "basics-nn.html",
    "href": "basics-nn.html",
    "title": "21¬† Basics of Neural Network",
    "section": "",
    "text": "21.1 Overview\nNeural networks are a class of machine learning algorithms that are inspired by the structure and function of the human brain. Like random forests, they are used for both classification and regression tasks. However, instead of using a collection of decision trees, neural networks consist of layers of interconnected nodes (or neurons) that can process and learn from complex data.\nIt‚Äôs possible to use gradient boosting with other types of models, including neural networks. In this case, instead of combining decision trees, the algorithm combines multiple neural networks to create a powerful ensemble model. The main idea remains the same: iteratively train models on the residual errors of the previous models to minimise a loss function.\nThe basic building block of a neural network is the perceptron, which takes in a set of inputs, applies weights to them, and produces an output. These outputs are then passed through activation functions that determine whether the neuron will fire or not. By stacking layers of perceptrons and activation functions, a neural network can learn to model highly non-linear relationships in the data.\nThe following R code demonstrates creating and visualising a feedforward neural network using the nnet and deepnet packages. It uses the iris dataset to predict species based on sepal and petal dimensions. The numeric features are normalised, and the target variable is converted to a factor. A neural network with one hidden layer containing 5 neurons is defined and trained using the logistic activation function and a maximum of 100,000 iterations. Finally, the network architecture is visualised using the plotnet function from the deepnet package.\nlibrary(nnet)\nlibrary(deepnet)\nlibrary(NeuralNetTools)\n\n# Load data\ndata(iris)\n\n# Normalize the numeric features\nnormalize &lt;- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\niris[,1:4] &lt;- lapply(iris[,1:4], normalize)\n\n# Convert the target variable into a factor\niris$Species &lt;- as.factor(iris$Species)\n\nset.seed(42)\n\n# Define the neural network architecture\nnn &lt;- nnet(\n  Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n  data = iris,\n  size = 5,\n  act.fct = \"logistic\",\n  linout = FALSE,\n  maxit = 1e+05\n)\n#&gt; # weights:  43\n#&gt; initial  value 180.827907 \n#&gt; iter  10 value 9.913449\n#&gt; iter  20 value 6.015153\n#&gt; iter  30 value 5.921213\n#&gt; iter  40 value 5.553101\n#&gt; iter  50 value 4.239151\n#&gt; iter  60 value 1.156178\n#&gt; iter  70 value 0.296792\n#&gt; iter  80 value 0.012652\n#&gt; iter  90 value 0.006984\n#&gt; iter 100 value 0.000851\n#&gt; iter 110 value 0.000175\n#&gt; final  value 0.000060 \n#&gt; converged\n\n# Visualise\nplotnet(nn)\nThe example neural network only has 1 hidden layer, however, it can have many hidden layers, which allows them to capture highly complex patterns and relationships in the data. However, this complexity can also lead to overfitting, where the model fits the training data too closely and fails to generalise to new data. To avoid overfitting, techniques like regularisation and early stopping can be used.\nIn the next example, we‚Äôll demonstrate L2 regularisation in a neural network using the keras package in R. We will continue using the iris dataset for this illustration. Our goal is to predict the species of iris flowers based on their sepal and petal dimensions.\n#Install and load the required libraries:\nlibrary(keras)\n#&gt; \n#&gt; Attaching package: 'keras'\n#&gt; The following object is masked _by_ '.GlobalEnv':\n#&gt; \n#&gt;     normalize\n\n#Load the iris dataset and pre-process it:\ndata(iris)\n\n# Normalize the numeric features\nnormalize &lt;- function(x) {\n  return((x - min(x)) / (max(x) - min(x)))\n}\n\niris[,1:4] &lt;- lapply(iris[,1:4], normalize)\n\n# Convert the target variable into a one-hot encoded matrix\niris$Species &lt;- as.factor(iris$Species)\ny &lt;- to_categorical(as.numeric(iris$Species) - 1)\nX &lt;- as.matrix(iris[, 1:4])\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Define the L2 regularization term\nl2_regularizer &lt;- regularizer_l2(l = 0.01)\n\n# Define the neural network architecture\n# \nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 5, activation = \"relu\", input_shape = ncol(X),\n              kernel_regularizer = l2_regularizer) %&gt;%\n  layer_dense(units = 3, activation = \"softmax\", kernel_regularizer = l2_regularizer)\n\n\n\n# Compile the model\nmodel %&gt;% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\n# Train the model\nhistory &lt;- model %&gt;% fit(\n  X, y,\n  epochs = 100,\n  batch_size = 16,\n  validation_split = 0.2\n)\n#&gt; Epoch 1/100\n#&gt; 8/8 - 1s - loss: 1.2971 - accuracy: 0.1667 - val_loss: 0.5406 - val_accuracy: 1.0000 - 903ms/epoch - 113ms/step\n#&gt; Epoch 2/100\n#&gt; 8/8 - 0s - loss: 1.2692 - accuracy: 0.1667 - val_loss: 0.5738 - val_accuracy: 1.0000 - 76ms/epoch - 10ms/step\n#&gt; Epoch 3/100\n#&gt; 8/8 - 0s - loss: 1.2405 - accuracy: 0.1667 - val_loss: 0.6075 - val_accuracy: 1.0000 - 49ms/epoch - 6ms/step\n#&gt; Epoch 4/100\n#&gt; 8/8 - 0s - loss: 1.2158 - accuracy: 0.1667 - val_loss: 0.6421 - val_accuracy: 1.0000 - 49ms/epoch - 6ms/step\n#&gt; Epoch 5/100\n#&gt; 8/8 - 0s - loss: 1.1907 - accuracy: 0.1667 - val_loss: 0.6759 - val_accuracy: 1.0000 - 70ms/epoch - 9ms/step\n#&gt; Epoch 6/100\n#&gt; 8/8 - 0s - loss: 1.1682 - accuracy: 0.1667 - val_loss: 0.7108 - val_accuracy: 1.0000 - 62ms/epoch - 8ms/step\n#&gt; Epoch 7/100\n#&gt; 8/8 - 0s - loss: 1.1476 - accuracy: 0.1750 - val_loss: 0.7468 - val_accuracy: 1.0000 - 64ms/epoch - 8ms/step\n#&gt; Epoch 8/100\n#&gt; 8/8 - 0s - loss: 1.1260 - accuracy: 0.2000 - val_loss: 0.7790 - val_accuracy: 1.0000 - 51ms/epoch - 6ms/step\n#&gt; Epoch 9/100\n#&gt; 8/8 - 0s - loss: 1.1073 - accuracy: 0.2333 - val_loss: 0.8129 - val_accuracy: 1.0000 - 41ms/epoch - 5ms/step\n#&gt; Epoch 10/100\n#&gt; 8/8 - 0s - loss: 1.0889 - accuracy: 0.3167 - val_loss: 0.8452 - val_accuracy: 1.0000 - 39ms/epoch - 5ms/step\n#&gt; Epoch 11/100\n#&gt; 8/8 - 0s - loss: 1.0734 - accuracy: 0.4417 - val_loss: 0.8795 - val_accuracy: 1.0000 - 40ms/epoch - 5ms/step\n#&gt; Epoch 12/100\n#&gt; 8/8 - 0s - loss: 1.0560 - accuracy: 0.5417 - val_loss: 0.9047 - val_accuracy: 1.0000 - 36ms/epoch - 4ms/step\n#&gt; Epoch 13/100\n#&gt; 8/8 - 0s - loss: 1.0418 - accuracy: 0.6167 - val_loss: 0.9318 - val_accuracy: 0.9333 - 46ms/epoch - 6ms/step\n#&gt; Epoch 14/100\n#&gt; 8/8 - 0s - loss: 1.0286 - accuracy: 0.6667 - val_loss: 0.9616 - val_accuracy: 0.8667 - 46ms/epoch - 6ms/step\n#&gt; Epoch 15/100\n#&gt; 8/8 - 0s - loss: 1.0150 - accuracy: 0.7583 - val_loss: 0.9884 - val_accuracy: 0.7667 - 43ms/epoch - 5ms/step\n#&gt; Epoch 16/100\n#&gt; 8/8 - 0s - loss: 1.0017 - accuracy: 0.8083 - val_loss: 1.0147 - val_accuracy: 0.7000 - 51ms/epoch - 6ms/step\n#&gt; Epoch 17/100\n#&gt; 8/8 - 0s - loss: 0.9893 - accuracy: 0.8417 - val_loss: 1.0418 - val_accuracy: 0.6667 - 47ms/epoch - 6ms/step\n#&gt; Epoch 18/100\n#&gt; 8/8 - 0s - loss: 0.9785 - accuracy: 0.8583 - val_loss: 1.0720 - val_accuracy: 0.5000 - 49ms/epoch - 6ms/step\n#&gt; Epoch 19/100\n#&gt; 8/8 - 0s - loss: 0.9656 - accuracy: 0.8333 - val_loss: 1.0908 - val_accuracy: 0.3333 - 91ms/epoch - 11ms/step\n#&gt; Epoch 20/100\n#&gt; 8/8 - 0s - loss: 0.9553 - accuracy: 0.8500 - val_loss: 1.1139 - val_accuracy: 0.2333 - 42ms/epoch - 5ms/step\n#&gt; Epoch 21/100\n#&gt; 8/8 - 0s - loss: 0.9448 - accuracy: 0.8417 - val_loss: 1.1361 - val_accuracy: 0.1333 - 43ms/epoch - 5ms/step\n#&gt; Epoch 22/100\n#&gt; 8/8 - 0s - loss: 0.9345 - accuracy: 0.8417 - val_loss: 1.1550 - val_accuracy: 0.1000 - 44ms/epoch - 5ms/step\n#&gt; Epoch 23/100\n#&gt; 8/8 - 0s - loss: 0.9251 - accuracy: 0.8250 - val_loss: 1.1751 - val_accuracy: 0.0667 - 38ms/epoch - 5ms/step\n#&gt; Epoch 24/100\n#&gt; 8/8 - 0s - loss: 0.9154 - accuracy: 0.8333 - val_loss: 1.1901 - val_accuracy: 0.0667 - 37ms/epoch - 5ms/step\n#&gt; Epoch 25/100\n#&gt; 8/8 - 0s - loss: 0.9063 - accuracy: 0.8333 - val_loss: 1.2023 - val_accuracy: 0.0667 - 39ms/epoch - 5ms/step\n#&gt; Epoch 26/100\n#&gt; 8/8 - 0s - loss: 0.8982 - accuracy: 0.8250 - val_loss: 1.2200 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 27/100\n#&gt; 8/8 - 0s - loss: 0.8892 - accuracy: 0.8250 - val_loss: 1.2326 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 28/100\n#&gt; 8/8 - 0s - loss: 0.8808 - accuracy: 0.8333 - val_loss: 1.2419 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 29/100\n#&gt; 8/8 - 0s - loss: 0.8729 - accuracy: 0.8333 - val_loss: 1.2523 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 30/100\n#&gt; 8/8 - 0s - loss: 0.8654 - accuracy: 0.8333 - val_loss: 1.2609 - val_accuracy: 0.0000e+00 - 41ms/epoch - 5ms/step\n#&gt; Epoch 31/100\n#&gt; 8/8 - 0s - loss: 0.8581 - accuracy: 0.8333 - val_loss: 1.2715 - val_accuracy: 0.0000e+00 - 40ms/epoch - 5ms/step\n#&gt; Epoch 32/100\n#&gt; 8/8 - 0s - loss: 0.8506 - accuracy: 0.8333 - val_loss: 1.2738 - val_accuracy: 0.0000e+00 - 48ms/epoch - 6ms/step\n#&gt; Epoch 33/100\n#&gt; 8/8 - 0s - loss: 0.8434 - accuracy: 0.8333 - val_loss: 1.2829 - val_accuracy: 0.0000e+00 - 41ms/epoch - 5ms/step\n#&gt; Epoch 34/100\n#&gt; 8/8 - 0s - loss: 0.8365 - accuracy: 0.8333 - val_loss: 1.2911 - val_accuracy: 0.0000e+00 - 39ms/epoch - 5ms/step\n#&gt; Epoch 35/100\n#&gt; 8/8 - 0s - loss: 0.8297 - accuracy: 0.8333 - val_loss: 1.2948 - val_accuracy: 0.0000e+00 - 87ms/epoch - 11ms/step\n#&gt; Epoch 36/100\n#&gt; 8/8 - 0s - loss: 0.8232 - accuracy: 0.8333 - val_loss: 1.3012 - val_accuracy: 0.0000e+00 - 50ms/epoch - 6ms/step\n#&gt; Epoch 37/100\n#&gt; 8/8 - 0s - loss: 0.8165 - accuracy: 0.8333 - val_loss: 1.3010 - val_accuracy: 0.0000e+00 - 53ms/epoch - 7ms/step\n#&gt; Epoch 38/100\n#&gt; 8/8 - 0s - loss: 0.8102 - accuracy: 0.8333 - val_loss: 1.3019 - val_accuracy: 0.0000e+00 - 44ms/epoch - 5ms/step\n#&gt; Epoch 39/100\n#&gt; 8/8 - 0s - loss: 0.8039 - accuracy: 0.8333 - val_loss: 1.3054 - val_accuracy: 0.0000e+00 - 42ms/epoch - 5ms/step\n#&gt; Epoch 40/100\n#&gt; 8/8 - 0s - loss: 0.7979 - accuracy: 0.8333 - val_loss: 1.3026 - val_accuracy: 0.0000e+00 - 47ms/epoch - 6ms/step\n#&gt; Epoch 41/100\n#&gt; 8/8 - 0s - loss: 0.7918 - accuracy: 0.8333 - val_loss: 1.3017 - val_accuracy: 0.0000e+00 - 52ms/epoch - 6ms/step\n#&gt; Epoch 42/100\n#&gt; 8/8 - 0s - loss: 0.7859 - accuracy: 0.8333 - val_loss: 1.3025 - val_accuracy: 0.0000e+00 - 36ms/epoch - 4ms/step\n#&gt; Epoch 43/100\n#&gt; 8/8 - 0s - loss: 0.7801 - accuracy: 0.8333 - val_loss: 1.2989 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 44/100\n#&gt; 8/8 - 0s - loss: 0.7746 - accuracy: 0.8333 - val_loss: 1.3047 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 45/100\n#&gt; 8/8 - 0s - loss: 0.7688 - accuracy: 0.8333 - val_loss: 1.3036 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 46/100\n#&gt; 8/8 - 0s - loss: 0.7632 - accuracy: 0.8333 - val_loss: 1.3005 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 47/100\n#&gt; 8/8 - 0s - loss: 0.7579 - accuracy: 0.8333 - val_loss: 1.3013 - val_accuracy: 0.0000e+00 - 33ms/epoch - 4ms/step\n#&gt; Epoch 48/100\n#&gt; 8/8 - 0s - loss: 0.7528 - accuracy: 0.8333 - val_loss: 1.2915 - val_accuracy: 0.0000e+00 - 32ms/epoch - 4ms/step\n#&gt; Epoch 49/100\n#&gt; 8/8 - 0s - loss: 0.7472 - accuracy: 0.8333 - val_loss: 1.2899 - val_accuracy: 0.0000e+00 - 32ms/epoch - 4ms/step\n#&gt; Epoch 50/100\n#&gt; 8/8 - 0s - loss: 0.7421 - accuracy: 0.8333 - val_loss: 1.2866 - val_accuracy: 0.0000e+00 - 30ms/epoch - 4ms/step\n#&gt; Epoch 51/100\n#&gt; 8/8 - 0s - loss: 0.7371 - accuracy: 0.8333 - val_loss: 1.2873 - val_accuracy: 0.0000e+00 - 32ms/epoch - 4ms/step\n#&gt; Epoch 52/100\n#&gt; 8/8 - 0s - loss: 0.7322 - accuracy: 0.8333 - val_loss: 1.2802 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 53/100\n#&gt; 8/8 - 0s - loss: 0.7273 - accuracy: 0.8333 - val_loss: 1.2806 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 54/100\n#&gt; 8/8 - 0s - loss: 0.7225 - accuracy: 0.8333 - val_loss: 1.2814 - val_accuracy: 0.0000e+00 - 38ms/epoch - 5ms/step\n#&gt; Epoch 55/100\n#&gt; 8/8 - 0s - loss: 0.7179 - accuracy: 0.8333 - val_loss: 1.2858 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 56/100\n#&gt; 8/8 - 0s - loss: 0.7134 - accuracy: 0.8333 - val_loss: 1.2793 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 57/100\n#&gt; 8/8 - 0s - loss: 0.7090 - accuracy: 0.8333 - val_loss: 1.2748 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 58/100\n#&gt; 8/8 - 0s - loss: 0.7045 - accuracy: 0.8333 - val_loss: 1.2692 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 59/100\n#&gt; 8/8 - 0s - loss: 0.7003 - accuracy: 0.8333 - val_loss: 1.2657 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 60/100\n#&gt; 8/8 - 0s - loss: 0.6962 - accuracy: 0.8333 - val_loss: 1.2570 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 61/100\n#&gt; 8/8 - 0s - loss: 0.6920 - accuracy: 0.8333 - val_loss: 1.2458 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 62/100\n#&gt; 8/8 - 0s - loss: 0.6877 - accuracy: 0.8333 - val_loss: 1.2423 - val_accuracy: 0.0000e+00 - 32ms/epoch - 4ms/step\n#&gt; Epoch 63/100\n#&gt; 8/8 - 0s - loss: 0.6840 - accuracy: 0.8333 - val_loss: 1.2358 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 64/100\n#&gt; 8/8 - 0s - loss: 0.6798 - accuracy: 0.8333 - val_loss: 1.2373 - val_accuracy: 0.0000e+00 - 33ms/epoch - 4ms/step\n#&gt; Epoch 65/100\n#&gt; 8/8 - 0s - loss: 0.6761 - accuracy: 0.8333 - val_loss: 1.2359 - val_accuracy: 0.0000e+00 - 32ms/epoch - 4ms/step\n#&gt; Epoch 66/100\n#&gt; 8/8 - 0s - loss: 0.6725 - accuracy: 0.8333 - val_loss: 1.2430 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 67/100\n#&gt; 8/8 - 0s - loss: 0.6690 - accuracy: 0.8333 - val_loss: 1.2358 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 68/100\n#&gt; 8/8 - 0s - loss: 0.6652 - accuracy: 0.8333 - val_loss: 1.2337 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 69/100\n#&gt; 8/8 - 0s - loss: 0.6618 - accuracy: 0.8333 - val_loss: 1.2264 - val_accuracy: 0.0000e+00 - 36ms/epoch - 5ms/step\n#&gt; Epoch 70/100\n#&gt; 8/8 - 0s - loss: 0.6584 - accuracy: 0.8333 - val_loss: 1.2254 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 71/100\n#&gt; 8/8 - 0s - loss: 0.6552 - accuracy: 0.8333 - val_loss: 1.2223 - val_accuracy: 0.0000e+00 - 38ms/epoch - 5ms/step\n#&gt; Epoch 72/100\n#&gt; 8/8 - 0s - loss: 0.6519 - accuracy: 0.8333 - val_loss: 1.2094 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 73/100\n#&gt; 8/8 - 0s - loss: 0.6487 - accuracy: 0.8333 - val_loss: 1.2020 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 74/100\n#&gt; 8/8 - 0s - loss: 0.6460 - accuracy: 0.8333 - val_loss: 1.1919 - val_accuracy: 0.0000e+00 - 36ms/epoch - 5ms/step\n#&gt; Epoch 75/100\n#&gt; 8/8 - 0s - loss: 0.6428 - accuracy: 0.8333 - val_loss: 1.1898 - val_accuracy: 0.0000e+00 - 50ms/epoch - 6ms/step\n#&gt; Epoch 76/100\n#&gt; 8/8 - 0s - loss: 0.6398 - accuracy: 0.8333 - val_loss: 1.1825 - val_accuracy: 0.0000e+00 - 39ms/epoch - 5ms/step\n#&gt; Epoch 77/100\n#&gt; 8/8 - 0s - loss: 0.6368 - accuracy: 0.8333 - val_loss: 1.1822 - val_accuracy: 0.0000e+00 - 38ms/epoch - 5ms/step\n#&gt; Epoch 78/100\n#&gt; 8/8 - 0s - loss: 0.6341 - accuracy: 0.8333 - val_loss: 1.1809 - val_accuracy: 0.0000e+00 - 36ms/epoch - 5ms/step\n#&gt; Epoch 79/100\n#&gt; 8/8 - 0s - loss: 0.6314 - accuracy: 0.8333 - val_loss: 1.1760 - val_accuracy: 0.0000e+00 - 37ms/epoch - 5ms/step\n#&gt; Epoch 80/100\n#&gt; 8/8 - 0s - loss: 0.6288 - accuracy: 0.8333 - val_loss: 1.1785 - val_accuracy: 0.0000e+00 - 36ms/epoch - 5ms/step\n#&gt; Epoch 81/100\n#&gt; 8/8 - 0s - loss: 0.6262 - accuracy: 0.8333 - val_loss: 1.1850 - val_accuracy: 0.0000e+00 - 43ms/epoch - 5ms/step\n#&gt; Epoch 82/100\n#&gt; 8/8 - 0s - loss: 0.6237 - accuracy: 0.8333 - val_loss: 1.1831 - val_accuracy: 0.0000e+00 - 39ms/epoch - 5ms/step\n#&gt; Epoch 83/100\n#&gt; 8/8 - 0s - loss: 0.6213 - accuracy: 0.8333 - val_loss: 1.1809 - val_accuracy: 0.0000e+00 - 40ms/epoch - 5ms/step\n#&gt; Epoch 84/100\n#&gt; 8/8 - 0s - loss: 0.6190 - accuracy: 0.8333 - val_loss: 1.1886 - val_accuracy: 0.0000e+00 - 39ms/epoch - 5ms/step\n#&gt; Epoch 85/100\n#&gt; 8/8 - 0s - loss: 0.6166 - accuracy: 0.8333 - val_loss: 1.1798 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 86/100\n#&gt; 8/8 - 0s - loss: 0.6141 - accuracy: 0.8333 - val_loss: 1.1826 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 87/100\n#&gt; 8/8 - 0s - loss: 0.6120 - accuracy: 0.8333 - val_loss: 1.1784 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 88/100\n#&gt; 8/8 - 0s - loss: 0.6099 - accuracy: 0.8333 - val_loss: 1.1697 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 89/100\n#&gt; 8/8 - 0s - loss: 0.6076 - accuracy: 0.8333 - val_loss: 1.1688 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n#&gt; Epoch 90/100\n#&gt; 8/8 - 0s - loss: 0.6056 - accuracy: 0.8333 - val_loss: 1.1706 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 91/100\n#&gt; 8/8 - 0s - loss: 0.6036 - accuracy: 0.8333 - val_loss: 1.1741 - val_accuracy: 0.0000e+00 - 36ms/epoch - 4ms/step\n#&gt; Epoch 92/100\n#&gt; 8/8 - 0s - loss: 0.6015 - accuracy: 0.8333 - val_loss: 1.1681 - val_accuracy: 0.0000e+00 - 35ms/epoch - 4ms/step\n#&gt; Epoch 93/100\n#&gt; 8/8 - 0s - loss: 0.5997 - accuracy: 0.8333 - val_loss: 1.1632 - val_accuracy: 0.0000e+00 - 36ms/epoch - 4ms/step\n#&gt; Epoch 94/100\n#&gt; 8/8 - 0s - loss: 0.5977 - accuracy: 0.8333 - val_loss: 1.1643 - val_accuracy: 0.0000e+00 - 36ms/epoch - 5ms/step\n#&gt; Epoch 95/100\n#&gt; 8/8 - 0s - loss: 0.5960 - accuracy: 0.8333 - val_loss: 1.1651 - val_accuracy: 0.0000e+00 - 44ms/epoch - 6ms/step\n#&gt; Epoch 96/100\n#&gt; 8/8 - 0s - loss: 0.5942 - accuracy: 0.8333 - val_loss: 1.1544 - val_accuracy: 0.0000e+00 - 50ms/epoch - 6ms/step\n#&gt; Epoch 97/100\n#&gt; 8/8 - 0s - loss: 0.5923 - accuracy: 0.8333 - val_loss: 1.1473 - val_accuracy: 0.0000e+00 - 45ms/epoch - 6ms/step\n#&gt; Epoch 98/100\n#&gt; 8/8 - 0s - loss: 0.5909 - accuracy: 0.8333 - val_loss: 1.1571 - val_accuracy: 0.0000e+00 - 45ms/epoch - 6ms/step\n#&gt; Epoch 99/100\n#&gt; 8/8 - 0s - loss: 0.5889 - accuracy: 0.8333 - val_loss: 1.1479 - val_accuracy: 0.0000e+00 - 38ms/epoch - 5ms/step\n#&gt; Epoch 100/100\n#&gt; 8/8 - 0s - loss: 0.5873 - accuracy: 0.8333 - val_loss: 1.1397 - val_accuracy: 0.0000e+00 - 34ms/epoch - 4ms/step\n\n\n#Plot the training and validation accuracy:\nplot(history)\nNote the l2_regularizer applied in the code, which is a popular technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function. In a advanced course of neural networks, you may also encounter advanced regulisation techniques including dropout, batch normalisation, data augmentation, early stopping, and adversarial training to prevent overfitting and improve generalisation performance.\nHyperparameters\nThe terms such as epochs, batch_size and validation_split as in the above code are called hyperparameters in machine learning. One of the key challenges in neural networks is the tuning of hyperparameters, such as the number of layers and nodes in each layer, the learning rate, and the activation function. Finding the optimal values for these hyperparameters can be time-consuming and computationally expensive.\nDespite these challenges, neural networks have shown to be highly effective in a wide range of applications, such as image and speech recognition, natural language processing, and predictive modelling. In fact, deep learning, which refers to the use of neural networks with many layers, has revolutionized many fields, including computer vision and natural language processing.\nDeep learning\nDeep learning is a subfield of machine learning that uses artificial neural networks with multiple layers to learn hierarchical representations of data. Neural networks, on the other hand, are a type of algorithm that are inspired by the structure and function of biological neurons, and can be used for a variety of tasks, including classification, regression, and clustering.\nThe main difference between deep learning and neural networks is that deep learning refers specifically to neural networks with many layers, typically more than three or four, while neural networks can have any number of layers. Deep learning is often used for tasks such as image recognition, natural language processing, and speech recognition, where the data is complex and high-dimensional, and hierarchical features are important for accurate modelling.\nAnother difference between deep learning and neural networks is that deep learning often requires a lot of computational resources, especially for training large models with millions of parameters, while neural networks can be trained on more modest hardware. Additionally, deep learning often involves more complex optimisation techniques, such as stochastic gradient descent with momentum and adaptive learning rate methods, to train the deep neural network effectively.\nIn summary, while neural networks and deep learning are related concepts, deep learning specifically refers to the use of neural networks with many layers to learn hierarchical representations of data, often requiring more computational resources and complex optimisation techniques.\nChallenge\nThere are several popular packages in R used for neural network and deep learning, each with its own strengths and weaknesses. Here are some of the most commonly used packages:\nChoose one package and try to use it to predict furniture price as detailed here.",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Basics of Neural Network</span>"
    ]
  },
  {
    "objectID": "basics-nn.html#overview",
    "href": "basics-nn.html#overview",
    "title": "21¬† Basics of Neural Network",
    "section": "",
    "text": "Package\nDescription\nAdvantages\nLimitations\n\n\n\nneuralnet\nA basic package for training feedforward neural networks with one hidden layer\nEasy to use, supports various activation functions and training algorithms\nLimited to a single hidden layer\n\n\nnnet\nA package for training feedforward neural networks with a single hidden layer\nEasy to use, supports various activation functions and training algorithms\nLimited to a single hidden layer\n\n\ncaret\nA package that provides a unified interface to multiple machine learning algorithms, including neural networks\nSupports many different types of neural networks and training algorithms\nCan be slow and memory-intensive for large datasets\n\n\nkeras\nA popular package for building and training deep learning models using the Keras API\nSupports a wide range of neural network architectures and layers, with GPU acceleration for faster training\nRequires installation of TensorFlow or other backends\n\n\ntensorflow\nA package that provides an R interface to the TensorFlow library for building and training deep learning models\nProvides a wide range of tools and features for working with neural networks and deep learning\nCan be more difficult to use than some other packages\n\n\nmxnet\nA package for building and training deep learning models using the MXNet library\nSupports a wide range of neural network architectures and layers, with GPU acceleration for faster training\nCan be more difficult to use than some other packages",
    "crumbs": [
      "Other Topics in Data Science",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Basics of Neural Network</span>"
    ]
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Natural Language Processing (NLP) has become an essential tool in the era of data science, enabling computers to understand, interpret, and generate human languages. It has paved the way for many exciting applications, such as machine translation, sentiment analysis, and chatbots, among others.\nR is a popular programming language and environment for statistical computing and data analysis, making it an ideal choice for exploring NLP. Its extensive ecosystem of packages and libraries, combined with its intuitive syntax, empower users to extract insights from text data effortlessly. This chapter provides an introduction to NLP with R, focusing on essential concepts and techniques for beginners.\nEmbarking on this journey will equip you with the tools and knowledge necessary to unlock the hidden value in text data. Get ready to harness the power of R and NLP to solve real-world problems and enhance your data-driven decision-making capabilities.\nIn this sixth part of the book, our focus will be on the the following chapters:\n\nIn 22¬† Basics of NLP we will get to understand the basics of natural language processing.\nIn 23¬† Understand a simple NLP model we will dive into simple NLP models.\nIn 24¬† Apply NLP Techniques to Real World Data we will understand how to apply NLP in real-world.",
    "crumbs": [
      "Natural Language Processing"
    ]
  },
  {
    "objectID": "basics-nlp.html",
    "href": "basics-nlp.html",
    "title": "22¬† Basics of NLP",
    "section": "",
    "text": "22.1 Overview\nText data can be understood as sequences of characters or sequences of words, which form the foundation of human language communication. These sequences can be analysed and processed to extract meaningful insights and patterns for various applications. Natural Language Processing (NLP) focuses on understanding and interpreting this text data to enable computers to interact with human language effectively.\nNLP is inherently challenging due to several factors:\nThese factors, among others, contribute to the challenges faced by NLP practitioners and researchers in developing systems capable of understanding, interpreting, and generating human languages effectively. Despite these challenges, significant progress has been made in recent years, with NLP systems becoming increasingly sophisticated and accurate, thanks to advancements in machine learning, deep learning, and the availability of vast amounts of textual data.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Basics of NLP</span>"
    ]
  },
  {
    "objectID": "basics-nlp.html#overview",
    "href": "basics-nlp.html#overview",
    "title": "22¬† Basics of NLP",
    "section": "",
    "text": "Ambiguity: Human languages are often ambiguous, making it difficult for computers to understand the intended meaning. Ambiguity can occur at various levels, such as lexical (e.g., homonyms and polysemy), syntactic (e.g., ambiguous sentence structures), and semantic (e.g., metaphor and sarcasm). Disambiguating the intended meaning requires context, world knowledge, and reasoning abilities, which can be challenging for NLP systems to acquire.\nVariability: Human languages exhibit a high degree of variability, including dialects, accents, slang, and domain-specific terminology. People use different words, phrases, and sentence structures to express the same meaning, making it difficult for NLP systems to recognise and interpret such variations consistently.\nComplexity: Human languages are complex, with intricate grammatical rules, extensive vocabularies, and numerous linguistic phenomena like anaphora, ellipsis, and co-reference. NLP systems need to account for these complexities and possess a deep understanding of language structure and semantics to process and analyse text effectively.\nIdiomatic Expressions: Languages contain idiomatic expressions, phrases, and proverbs that have meanings not easily deducible from their individual words. Understanding and interpreting these expressions can be challenging for NLP systems, as it requires not only linguistic knowledge but also cultural context and awareness.\nEvolution: Human languages are constantly evolving, with new words, phrases, and usages emerging regularly. NLP systems need to adapt and stay up-to-date with these changes to remain effective in understanding and interpreting the ever-evolving landscape of human language.\nContextual Dependency: The meaning of words and sentences often depends on the context in which they are used. NLP systems need to be able to consider the broader context and incorporate external knowledge to accurately interpret and disambiguate the meaning of the text.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Basics of NLP</span>"
    ]
  },
  {
    "objectID": "basics-nlp.html#large-language-models-llms",
    "href": "basics-nlp.html#large-language-models-llms",
    "title": "22¬† Basics of NLP",
    "section": "\n22.2 Large Language Models (LLMs)",
    "text": "22.2 Large Language Models (LLMs)\nSimple Natural Language Processing (NLP) models usually refer to traditional, rule-based, or statistical models that can tackle specific tasks. In contrast, Large Language Models (LLMs) like GPT-4 from OpenAI are based on deep learning techniques and can perform a wide range of tasks with much greater proficiency. Here are some key differences between the two types of models:\n\nArchitecture: Simple NLP models typically employ traditional techniques like bag-of-words, term frequency-inverse document frequency (TF-IDF), or statistical methods like Na√Øve Bayes and logistic regression. These models often require manual feature engineering and may involve rule-based approaches. LLMs like ChatGPT, on the other hand, are based on advanced deep learning architectures, such as the Transformer, which allows them to capture complex patterns and relationships in the text data without manual feature engineering.\nScope: Simple NLP models are generally designed for specific tasks and may require task-specific adjustments or tuning to work effectively. LLMs like ChatGPT are pre-trained on massive amounts of data and can be fine-tuned for a variety of tasks, including text classification, sentiment analysis, question-answering, and more, with minimal task-specific adjustments.\nPerformance: LLMs like ChatGPT often outperform simple NLP models on a wide range of NLP tasks due to their ability to capture complex patterns in the data, learn from vast amounts of training data, and generalise well to new, unseen data. Simple NLP models may struggle with certain linguistic phenomena, ambiguity, or variability, while LLMs can better handle these challenges.\nTraining Data and Computational Requirements: LLMs like ChatGPT require massive amounts of training data and significant computational resources for training, which can be prohibitive for many users or organisations. Simple NLP models, on the other hand, can often be trained on smaller datasets and require fewer computational resources, making them more accessible and easier to deploy.\nInterpretability: Simple NLP models tend to be more interpretable and easier to understand, as their inner workings are based on well-understood techniques or rules. LLMs like ChatGPT, with their deep learning architectures, are often considered ‚Äúblack boxes,‚Äù making it difficult to explain or understand their predictions and decision-making processes.\n\nIn summary, simple NLP models covered in this chapter are generally more accessible and interpretable, suitable for specific tasks, and require less computational resources. In contrast, LLMs like ChatGPT are more versatile, better at handling complex linguistic phenomena, and generally outperform simple models but require vast amounts of data and computational resources to train and maintain.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Basics of NLP</span>"
    ]
  },
  {
    "objectID": "basics-nlp.html#basic-nlp-techniques",
    "href": "basics-nlp.html#basic-nlp-techniques",
    "title": "22¬† Basics of NLP",
    "section": "\n22.3 Basic NLP Techniques",
    "text": "22.3 Basic NLP Techniques\nTo process and analyse text data effectively, NLP employs various techniques, such as tokenization, which consists of defining the unit of analysis, like words, sequences of words, or entire sentences.\n\n22.3.1 Tokenization\nTokenization is a crucial step in the text analysis process, as it helps break down unstructured text data into structured, manageable units that can be further processed and analysed. The need for tokenization arises from several factors:\n\nSimplifying Text Data: Unstructured text data can be challenging to analyse due to its complexity and variability. Tokenization divides the text into smaller, more manageable units (tokens), such as words or sentences. This simplification makes it easier to apply subsequent analysis techniques and extract meaningful insights.\nEnabling Text Preprocessing: Tokenization is a prerequisite for various text preprocessing tasks, such as stop word removal, stemming, and lemmatization. These preprocessing tasks are essential for cleaning and normalising the text data, which in turn improves the accuracy and effectiveness of text analysis and machine learning algorithms.\nFacilitating Feature Extraction: In many text analysis and natural language processing applications, such as text classification and sentiment analysis, features need to be extracted from the text data to be used as input for machine learning models. Tokenization allows for the extraction of features like word frequency, term frequency-inverse document frequency (TF-IDF), and n-grams, which can significantly impact the performance of these models.\nEnhancing Computational Efficiency: By breaking down text data into tokens, you can reduce the computational complexity of text analysis tasks. Working with smaller units of text data allows for more efficient searching, sorting, and indexing operations, which can be particularly beneficial when dealing with large text corpora.\n\nWithout tokenization, it would be challenging to perform accurate and effective text analysis and natural language processing tasks.\nIn R, we will use the tokenizers package, which provides a variety of tokenisation functions for text data.\n\nlibrary(tokenizers)\n#let's create a simple text data example to tokenize:\ntext &lt;- \"Natural Language Processing with R is an exciting journey!\"\n\n#Now, we will tokenize the text at the word level using the tokenize_words() function:\nword_tokens &lt;- tokenize_words(text)\nprint(word_tokens)\n#&gt; [[1]]\n#&gt; [1] \"natural\"    \"language\"   \"processing\" \"with\"       \"r\"         \n#&gt; [6] \"is\"         \"an\"         \"exciting\"   \"journey\"\n\nNotice that the function has tokenized the text into a list of words, including the exclamation mark as a separate token.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Basics of NLP</span>"
    ]
  },
  {
    "objectID": "basics-nlp.html#stop-words-handling",
    "href": "basics-nlp.html#stop-words-handling",
    "title": "22¬† Basics of NLP",
    "section": "\n22.4 Stop words handling",
    "text": "22.4 Stop words handling\nIn the context of text analysis, it is often necessary to remove stop words. Stop words are words that do not contribute significantly to the meaning of a text and are therefore not useful for analysis. These words are typically extremely common in a given language, including words such as ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúto‚Äù, and so forth in English. By eliminating stop words from a text, the remaining words will provide more meaningful insights, allowing for more effective analysis and interpretation. To demonstrate how to remove common stop words from a text using R, we will use the tidytext and dplyr packages.\n\nlibrary(tidytext)\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n#let's create a simple text data example with some common stop words:\ntext &lt;- \"The quick brown fox jumps over the lazy dog.\"\n\n#Now, we will tokenize the text into words using the unnest_tokens() function from the tidytext package:\ntext_df &lt;- tibble(line = 1, text = text) %&gt;%\n  unnest_tokens(word, text)\nprint(text_df)\n#&gt; # A tibble: 9 √ó 2\n#&gt;    line word \n#&gt;   &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     1 the  \n#&gt; 2     1 quick\n#&gt; 3     1 brown\n#&gt; 4     1 fox  \n#&gt; 5     1 jumps\n#&gt; 6     1 over \n#&gt; # ‚Ñπ 3 more rows\n\nNow, let‚Äôs remove the common stop words using the anti_join() function from the dplyr package with the stop_words dataset from the tidytext package:\n\ntext_no_stop_words &lt;- text_df %&gt;%\n  anti_join(stop_words)\n#&gt; Joining with `by = join_by(word)`\nprint(text_no_stop_words)\n#&gt; # A tibble: 6 √ó 2\n#&gt;    line word \n#&gt;   &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     1 quick\n#&gt; 2     1 brown\n#&gt; 3     1 fox  \n#&gt; 4     1 jumps\n#&gt; 5     1 lazy \n#&gt; 6     1 dog\n\nAs you can see, the stop words ‚Äúthe‚Äù and ‚Äúover‚Äù have been removed from the text. The remaining words are more meaningful for further text analysis.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Basics of NLP</span>"
    ]
  },
  {
    "objectID": "basics-nlp.html#words-frequencies",
    "href": "basics-nlp.html#words-frequencies",
    "title": "22¬† Basics of NLP",
    "section": "\n22.5 Words frequencies",
    "text": "22.5 Words frequencies\nA fundamental step in text analysis is determining the frequency of words within a given text. Analysing word frequencies can reveal patterns and trends that provide valuable insights into the content and its underlying themes. In this section, we will demonstrate how to calculate word frequencies using R.\nContinue with the same example sentence, we first tokenize the text into words using the unnest_tokens() function from the tidytext package:\n\ntext_df &lt;- tibble(line = 1, text = text) %&gt;%\n  unnest_tokens(word, text)\n\nThen, we calculate the word frequencies using the count() function from the dplyr package:\n\nword_frequencies &lt;- text_df %&gt;%\n  count(word, sort = TRUE)\nprint(word_frequencies)\n#&gt; # A tibble: 8 √ó 2\n#&gt;   word      n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 the       2\n#&gt; 2 brown     1\n#&gt; 3 dog       1\n#&gt; 4 fox       1\n#&gt; 5 jumps     1\n#&gt; 6 lazy      1\n#&gt; # ‚Ñπ 2 more rows\n\nThe output shows the frequency of each word in the text, sorted in descending order. You can observe that the word ‚Äúthe‚Äù appears three times, while ‚Äúdog‚Äù and ‚Äúlazy‚Äù appear twice, and the other words appear only once. This analysis provides a simple overview of the most common words within the text.\nYou can further refine the analysis by removing stop words or applying other preprocessing techniques to focus on the most relevant words for your specific task.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Basics of NLP</span>"
    ]
  },
  {
    "objectID": "understanding-nlp.html",
    "href": "understanding-nlp.html",
    "title": "23¬† Understand a simple NLP model",
    "section": "",
    "text": "23.1 NLP with R\nR offers a rich ecosystem of packages and tools for Natural Language Processing (NLP) tasks, making it an excellent choice for text analysis and processing. In this section, we will describe and explain the basic building blocks of a simple NLP model for text classification. The example provided will help you understand the fundamental steps involved in building an NLP model for various tasks.\nThese building blocks provide a foundation for developing a simple NLP model for text classification tasks. By understanding these fundamental steps, you can adapt and customise the process for various NLP tasks and problems.\nIt is the process of transforming text into numeric tensors. It consists of applying some tokenisation scheme and then associating numeric vectors with the generated tokens. The generated vector are packed into sequence tensors and fed into deep neural network. There are different ways to associate a vector within a token such as one-hot encoding and token embedding (typically used for words and called word embedding).",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Understand a simple NLP model</span>"
    ]
  },
  {
    "objectID": "understanding-nlp.html#nlp-with-r",
    "href": "understanding-nlp.html#nlp-with-r",
    "title": "23¬† Understand a simple NLP model",
    "section": "",
    "text": "Data Collection: The first step is collecting a dataset relevant to the problem you are trying to solve. For text classification tasks, the dataset should consist of text documents along with their corresponding labels or categories. This labeled dataset will be used for training and testing the model.\n\nText Preprocessing: Text data is often unstructured, noisy, and contains irrelevant information. Text preprocessing is the process of cleaning and preparing the text data for analysis. Common preprocessing techniques include:\n\nLowercasing: Converting all text to lowercase to maintain consistency.\nRemoving punctuation: Eliminating punctuation marks to reduce noise.\nTokenization: Breaking the text into individual words or tokens.\nStop word removal: Removing common words that do not provide meaningful information (e.g., ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúand‚Äù).\nStemming or Lemmatization: Reducing words to their root forms to normalise the text and group similar words together.\n\n\n\nFeature Extraction: After preprocessing, the text data needs to be converted into a structured, numerical format that can be used as input for the classification model. Feature extraction is the process of transforming the text data into a numerical representation. Common techniques include:\n\nBag-of-words: A representation where text documents are described by the frequency of words, disregarding the order in which they appear.\nTerm frequency-inverse document frequency (TF-IDF): A representation that weighs the importance of words based on their frequency within a document and across the entire dataset.\nWord embeddings: Dense vector representations of words that capture their semantic meaning and relationships with other words (e.g., Word2Vec or GloVe).\n\n\nModel Selection: Choose a suitable classification model for the task at hand. For simple NLP models, common choices include logistic regression, Na√Øve Bayes, decision trees, or support vector machines. These models are relatively easy to interpret and require less computational power compared to deep learning models.\nModel Training: Train the selected model using the preprocessed text data and extracted features. This involves adjusting the model‚Äôs parameters to minimise the classification error on the training dataset. The model learns to recognise patterns and relationships in the text data that can be used to predict the labels or categories of the documents.\nModel Evaluation: Evaluate the performance of the trained model on a separate test dataset. Common evaluation metrics for text classification tasks include accuracy, precision, recall, and F1 score. The goal is to assess how well the model generalises to new, unseen data and determine whether it is suitable for deployment or requires further refinement.\nModel Deployment: Once the model has been trained and evaluated, deploy it to a production environment to process and classify new text data.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Understand a simple NLP model</span>"
    ]
  },
  {
    "objectID": "understanding-nlp.html#word-embeddings",
    "href": "understanding-nlp.html#word-embeddings",
    "title": "23¬† Understand a simple NLP model",
    "section": "\n23.2 Word embeddings",
    "text": "23.2 Word embeddings\nWord embeddings are dense vector representations of words that capture their semantic meaning and relationships with other words. These representations are learned by training algorithms on large text corpora, enabling more effective and accurate text analysis. In this introduction, we will explore two popular word embedding techniques - Word2Vec, using the R programming language with the Keras deep learning library.\nFor this example, we‚Äôll use the IMDB movie review dataset, which is available in Keras. Let‚Äôs download and preprocess the data:\n\nlibrary(keras)\nimdb &lt;- dataset_imdb(num_words = 10000) #will take around 30 seconds to download \nx_train &lt;- imdb$train$x\ny_train &lt;- imdb$train$y\nx_test &lt;- imdb$test$x\ny_test &lt;- imdb$test$y\n\n# Pad sequences to the same length\nmaxlen &lt;- 500\nx_train &lt;- pad_sequences(x_train, maxlen = maxlen)\nx_test &lt;- pad_sequences(x_test, maxlen = maxlen)\n\nIn this step, we‚Äôll create an embedding layer using Keras, which will learn the word embeddings as part of the neural network training process:\n\nvocab_size &lt;- 10000\nembedding_dim &lt;- 50\n\nembedding_layer &lt;- layer_embedding(input_dim = vocab_size,\n                                   output_dim = embedding_dim,\n                                   input_length = maxlen)\n\nNow, we‚Äôll build a neural network with the embedding layer for a sentiment analysis task. We‚Äôll use a simple architecture with an embedding layer, a global average pooling layer, and a dense output layer with a sigmoid activation function:\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  embedding_layer %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %&gt;% compile(optimizer = \"adam\",\n                  loss = \"binary_crossentropy\",\n                  metrics = c(\"accuracy\"))\n\nhistory &lt;- model %&gt;% fit(x_train, y_train,\n                         epochs = 10,\n                         batch_size = 32,\n                         validation_split = 0.2)\n#&gt; Epoch 1/10\n#&gt; 625/625 - 4s - loss: 0.6502 - accuracy: 0.6848 - val_loss: 0.5743 - val_accuracy: 0.7962 - 4s/epoch - 7ms/step\n#&gt; Epoch 2/10\n#&gt; 625/625 - 4s - loss: 0.4931 - accuracy: 0.8340 - val_loss: 0.4314 - val_accuracy: 0.8578 - 4s/epoch - 6ms/step\n#&gt; Epoch 3/10\n#&gt; 625/625 - 4s - loss: 0.3763 - accuracy: 0.8719 - val_loss: 0.3624 - val_accuracy: 0.8712 - 4s/epoch - 6ms/step\n#&gt; Epoch 4/10\n#&gt; 625/625 - 5s - loss: 0.3130 - accuracy: 0.8938 - val_loss: 0.3275 - val_accuracy: 0.8766 - 5s/epoch - 8ms/step\n#&gt; Epoch 5/10\n#&gt; 625/625 - 4s - loss: 0.2749 - accuracy: 0.9028 - val_loss: 0.3067 - val_accuracy: 0.8844 - 4s/epoch - 6ms/step\n#&gt; Epoch 6/10\n#&gt; 625/625 - 4s - loss: 0.2471 - accuracy: 0.9125 - val_loss: 0.2942 - val_accuracy: 0.8872 - 4s/epoch - 6ms/step\n#&gt; Epoch 7/10\n#&gt; 625/625 - 4s - loss: 0.2253 - accuracy: 0.9207 - val_loss: 0.2880 - val_accuracy: 0.8890 - 4s/epoch - 7ms/step\n#&gt; Epoch 8/10\n#&gt; 625/625 - 4s - loss: 0.2074 - accuracy: 0.9269 - val_loss: 0.2855 - val_accuracy: 0.8904 - 4s/epoch - 6ms/step\n#&gt; Epoch 9/10\n#&gt; 625/625 - 4s - loss: 0.1925 - accuracy: 0.9324 - val_loss: 0.2844 - val_accuracy: 0.8896 - 4s/epoch - 6ms/step\n#&gt; Epoch 10/10\n#&gt; 625/625 - 4s - loss: 0.1789 - accuracy: 0.9376 - val_loss: 0.2794 - val_accuracy: 0.8940 - 4s/epoch - 6ms/step\n\nOnce the model is trained, we can access the word embeddings from the embedding layer and visualise them using t-SNE or PCA:\n\nlibrary(tsne)\nlibrary(tidyverse)\n#&gt; ‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n#&gt; ‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n#&gt; ‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n#&gt; ‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n#&gt; ‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n#&gt; ‚úî purrr     1.0.2     \n#&gt; ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n#&gt; ‚úñ dplyr::filter() masks stats::filter()\n#&gt; ‚úñ dplyr::lag()    masks stats::lag()\n#&gt; ‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nword_embeddings &lt;- get_weights(model)[[1]]\n\n#Load the IMDB dataset:\nimdb &lt;- dataset_imdb(num_words = 5000)\nx_train &lt;- imdb$train$x\ny_train &lt;- imdb$train$y\nx_test &lt;- imdb$test$x\ny_test &lt;- imdb$test$y\n\n# word_index is a named list mapping words to an integer index\nword_index &lt;- dataset_imdb_word_index()\nindex_to_word &lt;- names(word_index)\nnames(index_to_word) &lt;- word_index\n\nwords &lt;- index_to_word[1:1000]\nembeddings &lt;- word_embeddings[1:1000, ]\n\nFinally, we‚Äôll use t-SNE to reduce the dimensionality of the embeddings and visualise them:\n\nlibrary(Rtsne)\nlibrary(plotly)\n#&gt; \n#&gt; Attaching package: 'plotly'\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt; \n#&gt;     last_plot\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n#&gt; The following object is masked from 'package:graphics':\n#&gt; \n#&gt;     layout\n\n# Run t-SNE on the embeddings\ntsne_model &lt;- Rtsne(embeddings, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)\n#&gt; Performing PCA\n#&gt; Read the 1000 x 50 data matrix successfully!\n#&gt; Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000\n#&gt; Computing input similarities...\n#&gt; Building tree...\n#&gt; Done in 0.09 seconds (sparsity = 0.103554)!\n#&gt; Learning embedding...\n#&gt; Iteration 50: error is 60.419095 (50 iterations in 0.10 seconds)\n#&gt; Iteration 100: error is 51.089836 (50 iterations in 0.11 seconds)\n#&gt; Iteration 150: error is 49.754795 (50 iterations in 0.11 seconds)\n#&gt; Iteration 200: error is 49.236931 (50 iterations in 0.11 seconds)\n#&gt; Iteration 250: error is 48.939243 (50 iterations in 0.12 seconds)\n#&gt; Iteration 300: error is 0.597436 (50 iterations in 0.11 seconds)\n#&gt; Iteration 350: error is 0.454116 (50 iterations in 0.10 seconds)\n#&gt; Iteration 400: error is 0.423469 (50 iterations in 0.09 seconds)\n#&gt; Iteration 450: error is 0.408928 (50 iterations in 0.09 seconds)\n#&gt; Iteration 500: error is 0.402160 (50 iterations in 0.08 seconds)\n#&gt; Fitting performed in 1.03 seconds.\n#Create a data frame with the t-SNE results and the corresponding words:\ntsne_data &lt;- data.frame(tsne_model$Y)\ncolnames(tsne_data) &lt;- c(\"X\", \"Y\")\ntsne_data$word &lt;- words        \n\n#Visualize the t-SNE results using ggplot2:\np &lt;- ggplot(tsne_data, aes(x = X, y = Y)) +\n  geom_text(aes(label = word), size = 3, alpha = 0.7, vjust = 1, hjust = 1, angle = 45) +\n  theme_minimal() +\n  labs(title = \"t-SNE Visualization of Word Embeddings\",\n       x = \"t-SNE Dimension 1\",\n       y = \"t-SNE Dimension 2\")\nggplotly(p)\n\n\n\n\n\nIn the context of word embeddings, t-SNE visualisation helps to understand the relationships between words and their meanings by representing them in a 2D space. It maintains the relative distances between high-dimensional data points as much as possible while mapping them onto a lower-dimensional space. As a result, words with similar meanings or usage tend to be placed close to each other in the 2D space, while words that are unrelated or have different meanings will be placed further apart.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Understand a simple NLP model</span>"
    ]
  },
  {
    "objectID": "understanding-nlp.html#pre-trained-word-embeddings",
    "href": "understanding-nlp.html#pre-trained-word-embeddings",
    "title": "23¬† Understand a simple NLP model",
    "section": "\n23.3 Pre-trained word embeddings",
    "text": "23.3 Pre-trained word embeddings\nPre-computed embeddings are designed to capture general aspects of language structure by considering the co-occurrence of words in sentences and documents within extensive corpora of text. These embeddings are beneficial because they already encapsulate semantic and syntactic information, which can be utilised for various natural language processing tasks.\nThere are two primary and widely-used pre-trained word embedding models: Word2Vec and GloVe. These models have been trained on massive datasets, enabling them to effectively represent words in a high-dimensional vector space, where semantically similar words are positioned close to one another.\n\nWord2Vec: Developed by Google, the Word2Vec model employs neural networks to learn word embeddings. It operates by predicting a word‚Äôs context (surrounding words) or by predicting a target word given its context. Two popular architectures for Word2Vec are Continuous Bag of Words (CBOW) and Skip-Gram.\nGloVe: An acronym for Global Vectors for Word Representation, GloVe is a model developed by researchers at Stanford University. It combines the benefits of both global matrix factorisation methods and local context window methods. GloVe generates word embeddings by leveraging the global co-occurrence matrix of words in a corpus, focusing on the word-word co-occurrence probability ratios.\n\nBoth Word2Vec and GloVe have proven their effectiveness in various NLP tasks and are widely adopted in the field, offering a solid starting point for leveraging pre-trained word embeddings.\n\n# Install and load the required packages\nlibrary(text)\n\n# Load the pre-trained Word2Vec model\nword2vec_model &lt;- textEmbed(\"path/to/word2vec_model.bin\")\n\n# Get the word embeddings for a specific word\nword_embedding &lt;- word2vec_model$wordEmbeddings[\"word\"]\n\n# Get the most similar words to a given word\nsimilar_words &lt;- word2vec_model$nearest_neighbors(words = \"word\", k = 5)\n\n# Perform word analogy tasks\nword_analogy &lt;- word2vec_model$analogies(\"king\", \"man\", \"woman\")\n\n# Calculate the cosine similarity between two words\nsimilarity_score &lt;- word2vec_model$similarity(\"word1\", \"word2\")\n\nIn the example, above you first need to install the text package if you haven‚Äôt already done so. You can install it using install.packages(\"text\").\nOnce the package is installed, you load it using library(text). Then, you can load the pre-trained Word2Vec model by providing the path to the binary file (.bin) of the Word2Vec model. Replace \"path/to/word2vec_model.bin\" with the actual path to your pre-trained Word2Vec model file.\nAfter loading the model, you can access various functionalities. For example, you can retrieve the word embeddings for a specific word using word2vec_model$wordEmbeddings[\"word\"], where \"word\" is the word you want to get the embeddings for.\nYou can also find the most similar words to a given word using word2vec_model$nearest_neighbors(words = \"word\", k = 5), where \"word\" is the word you want to find similar words for, and k is the number of similar words to retrieve.\nThe word2vec_model$analogies function allows you to perform word analogy tasks. In the example, it is used to find the word that is to ‚Äúking‚Äù what ‚Äúman‚Äù is to ‚Äúwoman‚Äù.\nFinally, you can calculate the cosine similarity between two words using word2vec_model$similarity(\"word1\", \"word2\"), where \"word1\" and \"word2\" are the words for which you want to calculate the similarity score.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Understand a simple NLP model</span>"
    ]
  },
  {
    "objectID": "apply-nlp.html",
    "href": "apply-nlp.html",
    "title": "24¬† Apply NLP Techniques to Real World Data",
    "section": "",
    "text": "24.1 Sentiment Analysis\nIn this example, we‚Äôll use the IMDB movie review dataset for text classification. We‚Äôll use the pre-trained word embeddings with a Keras model for sentiment analysis.\nlibrary(text2vec)\nlibrary(keras)\nlibrary(tidyverse)\n\n#Load the IMDB dataset:\nimdb &lt;- dataset_imdb(num_words = 5000)\nx_train &lt;- imdb$train$x\ny_train &lt;- imdb$train$y\nx_test &lt;- imdb$test$x\ny_test &lt;- imdb$test$y\n\n# word_index is a named list mapping words to an integer index\nword_index &lt;- dataset_imdb_word_index()\n\n# Reverses it, mapping integer indices to words\nreverse_word_index &lt;- names(word_index)                                    \nnames(reverse_word_index) &lt;- word_index\n\n# Decodes the 1st review. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for \"padding,\" \"start of sequence,\" and \"unknown.\"\ndecoded_review &lt;- sapply(x_train[[1]], function(index) {                \n  word &lt;- if (index &gt;= 3) reverse_word_index[[as.character(index - 3)]]\n  if (!is.null(word)) word else \"?\"\n})\n\npaste(decoded_review, collapse = \" \")\nAn example of decoded review:\n'? this film was just brilliant casting location scenery story direction everyone\\'s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don\\'t you think the whole story was so lovely because it was true and was someone\\'s life after all that was ? with us all'\n#&gt; [1] \"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\"\nIn order to feed the data into a neural network, we must first convert the lists of integers into an appropriate tensor format. In this case, we‚Äôll transform the training data into a matrix, where each row consists of 1D vectors.\nEach row in the matrix has 5,000 columns, which represent all possible words in a review. These words are then one-hot encoded (also known as categorical encoding), where a 0 or 1 is used to indicate if a specific word is present in the review. This process is performed for each of the 25,000 reviews, resulting in a sparse matrix containing 125,000,000 1s and 0s (occupying 1GB of data). The same transformation is also applied to the testing data, which has an identical size.\nlibrary(keras)\n\nvectorize_sequences &lt;- function(sequences, dimension = 5000) {\n  # Initialize a matrix with all zeroes\n  results &lt;- matrix(0, nrow = length(sequences), ncol = dimension)\n  # Replace 0 with a 1 for each column of the matrix given in the list\n  for (i in 1:length(sequences))\n    results[i, sequences[[i]]] &lt;- 1\n  results\n}\n\nimdb &lt;- dataset_imdb(num_words = 5000)\nx_train &lt;- imdb$train$x\ny_train &lt;- imdb$train$y\nx_test &lt;- imdb$test$x\ny_test &lt;- imdb$test$y\n\nx_train &lt;- vectorize_sequences(x_train)\nx_test &lt;- vectorize_sequences(x_test)\ny_train &lt;- as.numeric(y_train)\ny_test &lt;- as.numeric(y_test)\n\nstr(x_train[1,])\n#&gt;  num [1:5000] 1 1 0 1 1 1 1 1 1 0 ...",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Apply NLP Techniques to Real World Data</span>"
    ]
  },
  {
    "objectID": "apply-nlp.html#building-the-network",
    "href": "apply-nlp.html#building-the-network",
    "title": "24¬† Apply NLP Techniques to Real World Data",
    "section": "\n24.2 Building the Network",
    "text": "24.2 Building the Network\nA type of network that performs well on this kind of vector data is a simple stack of fully connected (dense) layers with ReLU activations.\nThere are two key architecture decisions to be made when designing such a stack of dense layers:\n\nThe number of layers to use\nThe number of hidden units to choose for each layer\n\nIncorporating more hidden units (a higher-dimensional representation space) enables your network to learn more complex representations, but it also increases the computational cost and may lead to learning unwanted patterns (patterns that enhance performance on the training data but not on the test data).\nThe chosen model has two intermediate dense layers with 16 hidden units each, and a third (sigmoid) layer that outputs the scalar prediction concerning the sentiment of the current review.\n\nlibrary(keras)\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 16, activation = \"relu\", input_shape = c(5000)) %&gt;%\n  layer_dense(units = 16, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n#prepare the validation dataset\nval_indices &lt;- 1:5000\n\nx_val &lt;- x_train[val_indices,]\npartial_x_train &lt;- x_train[-val_indices,]\ny_val &lt;- y_train[val_indices]\npartial_y_train &lt;- y_train[-val_indices]\n\n#fit the model\nhistory &lt;- model %&gt;% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(x_val, y_val)\n)\n#&gt; Epoch 1/20\n#&gt; 40/40 - 1s - loss: 0.4940 - accuracy: 0.7876 - val_loss: 0.3602 - val_accuracy: 0.8672 - 1s/epoch - 27ms/step\n#&gt; Epoch 2/20\n#&gt; 40/40 - 0s - loss: 0.3074 - accuracy: 0.8880 - val_loss: 0.3376 - val_accuracy: 0.8620 - 315ms/epoch - 8ms/step\n#&gt; Epoch 3/20\n#&gt; 40/40 - 0s - loss: 0.2475 - accuracy: 0.9079 - val_loss: 0.3100 - val_accuracy: 0.8758 - 278ms/epoch - 7ms/step\n#&gt; Epoch 4/20\n#&gt; 40/40 - 0s - loss: 0.2183 - accuracy: 0.9185 - val_loss: 0.2851 - val_accuracy: 0.8848 - 281ms/epoch - 7ms/step\n#&gt; Epoch 5/20\n#&gt; 40/40 - 0s - loss: 0.1978 - accuracy: 0.9269 - val_loss: 0.2995 - val_accuracy: 0.8842 - 278ms/epoch - 7ms/step\n#&gt; Epoch 6/20\n#&gt; 40/40 - 0s - loss: 0.1830 - accuracy: 0.9324 - val_loss: 0.3039 - val_accuracy: 0.8790 - 276ms/epoch - 7ms/step\n#&gt; Epoch 7/20\n#&gt; 40/40 - 0s - loss: 0.1695 - accuracy: 0.9362 - val_loss: 0.5102 - val_accuracy: 0.8122 - 290ms/epoch - 7ms/step\n#&gt; Epoch 8/20\n#&gt; 40/40 - 0s - loss: 0.1641 - accuracy: 0.9381 - val_loss: 0.3257 - val_accuracy: 0.8726 - 295ms/epoch - 7ms/step\n#&gt; Epoch 9/20\n#&gt; 40/40 - 0s - loss: 0.1517 - accuracy: 0.9445 - val_loss: 0.3681 - val_accuracy: 0.8624 - 293ms/epoch - 7ms/step\n#&gt; Epoch 10/20\n#&gt; 40/40 - 0s - loss: 0.1446 - accuracy: 0.9471 - val_loss: 0.3228 - val_accuracy: 0.8816 - 282ms/epoch - 7ms/step\n#&gt; Epoch 11/20\n#&gt; 40/40 - 0s - loss: 0.1359 - accuracy: 0.9508 - val_loss: 0.3462 - val_accuracy: 0.8756 - 290ms/epoch - 7ms/step\n#&gt; Epoch 12/20\n#&gt; 40/40 - 0s - loss: 0.1298 - accuracy: 0.9528 - val_loss: 0.3472 - val_accuracy: 0.8802 - 279ms/epoch - 7ms/step\n#&gt; Epoch 13/20\n#&gt; 40/40 - 0s - loss: 0.1205 - accuracy: 0.9553 - val_loss: 0.3712 - val_accuracy: 0.8750 - 275ms/epoch - 7ms/step\n#&gt; Epoch 14/20\n#&gt; 40/40 - 0s - loss: 0.1132 - accuracy: 0.9591 - val_loss: 0.3714 - val_accuracy: 0.8768 - 299ms/epoch - 7ms/step\n#&gt; Epoch 15/20\n#&gt; 40/40 - 0s - loss: 0.1077 - accuracy: 0.9605 - val_loss: 0.4143 - val_accuracy: 0.8672 - 283ms/epoch - 7ms/step\n#&gt; Epoch 16/20\n#&gt; 40/40 - 0s - loss: 0.0976 - accuracy: 0.9660 - val_loss: 0.4003 - val_accuracy: 0.8718 - 274ms/epoch - 7ms/step\n#&gt; Epoch 17/20\n#&gt; 40/40 - 0s - loss: 0.0914 - accuracy: 0.9679 - val_loss: 0.4088 - val_accuracy: 0.8738 - 280ms/epoch - 7ms/step\n#&gt; Epoch 18/20\n#&gt; 40/40 - 0s - loss: 0.0842 - accuracy: 0.9719 - val_loss: 0.4526 - val_accuracy: 0.8622 - 283ms/epoch - 7ms/step\n#&gt; Epoch 19/20\n#&gt; 40/40 - 0s - loss: 0.0795 - accuracy: 0.9732 - val_loss: 0.4694 - val_accuracy: 0.8668 - 360ms/epoch - 9ms/step\n#&gt; Epoch 20/20\n#&gt; 40/40 - 0s - loss: 0.0694 - accuracy: 0.9766 - val_loss: 0.4603 - val_accuracy: 0.8662 - 281ms/epoch - 7ms/step\nplot(history)\n\n\n\n\n\n\n\nThe validation loss and accuracy display signs of overfitting after approximately the 5th epoch, at which point the accuracy reaches its peak and the loss reaches its lowest point. To address this issue, we can limit the training to only 5 epochs.\n\nlibrary(keras)\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 16, activation = \"relu\", input_shape = c(5000)) %&gt;%\n  layer_dense(units = 16, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %&gt;% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nmodel %&gt;% fit(x_train, y_train, epochs = 5, batch_size = 512)\n#&gt; Epoch 1/5\n#&gt; 49/49 - 1s - loss: 0.4777 - accuracy: 0.7969 - 724ms/epoch - 15ms/step\n#&gt; Epoch 2/5\n#&gt; 49/49 - 0s - loss: 0.2888 - accuracy: 0.8938 - 276ms/epoch - 6ms/step\n#&gt; Epoch 3/5\n#&gt; 49/49 - 0s - loss: 0.2380 - accuracy: 0.9107 - 257ms/epoch - 5ms/step\n#&gt; Epoch 4/5\n#&gt; 49/49 - 0s - loss: 0.2137 - accuracy: 0.9184 - 255ms/epoch - 5ms/step\n#&gt; Epoch 5/5\n#&gt; 49/49 - 0s - loss: 0.1986 - accuracy: 0.9247 - 286ms/epoch - 6ms/step\n\nresults &lt;- model %&gt;% evaluate(x_test, y_test)\n#&gt; 782/782 - 1s - loss: 0.3074 - accuracy: 0.8779 - 761ms/epoch - 973us/step\nresults\n#&gt;     loss accuracy \n#&gt; 0.307369 0.877920\n\nCheck one review:\n\nlibrary(keras)\n\n#Load the IMDB dataset:\nimdb &lt;- dataset_imdb(num_words = 5000)\nx_train &lt;- imdb$train$x\ny_train &lt;- imdb$train$y\nx_test &lt;- imdb$test$x\ny_test &lt;- imdb$test$y\n\n# word_index is a named list mapping words to an integer index\nword_index &lt;- dataset_imdb_word_index()\n\n# Reverses it, mapping integer indices to words\nreverse_word_index &lt;- names(word_index)                                    \nnames(reverse_word_index) &lt;- word_index\n\nimdb &lt;- dataset_imdb(num_words = 5000)\ntest_text &lt;- imdb$test$x\ndecoded_review &lt;- sapply(test_text[[1]], function(index) {                \n  word &lt;- if (index &gt;= 3) reverse_word_index[[as.character(index - 3)]]\n  if (!is.null(word)) word else \"?\"\n})\n\npaste(decoded_review, collapse = \" \")\n#&gt; [1] \"? please give this one a miss br br ? ? and the rest of the cast ? terrible performances the show is flat flat flat br br i don't know how michael ? could have allowed this one on his ? he almost seemed to know this wasn't going to work out and his performance was quite ? so all you ? fans give this a miss\"\n\nThe prediction:\n\n  \nif (c(model %&gt;% predict(x_test[1:2,]))[1] &gt; 0.5) {\ncat('Positive')\n} else {\ncat('Negative')\n}\n\nOutput:\n\nprint('Negative')\n#&gt; [1] \"Negative\"",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Apply NLP Techniques to Real World Data</span>"
    ]
  },
  {
    "objectID": "img-proc.html",
    "href": "img-proc.html",
    "title": "Image Processing",
    "section": "",
    "text": "In this part of the book, our focus will be on the the following chapters:\n\nIn 25¬† Introduction to image processing we will get to understand the basics of image processing from a theoretical and practical standpoint.\nIn 26¬† Image segmentation and object detection we will dive into image segmentation from a theoretical and practical standpoint.",
    "crumbs": [
      "Image Processing"
    ]
  },
  {
    "objectID": "intro-img-proc.html",
    "href": "intro-img-proc.html",
    "title": "25¬† Introduction to image processing",
    "section": "",
    "text": "25.1 Overview\nImage classification is the process of categorising images into predefined classes or categories based on their visual features. It plays a crucial role in various applications such as medical diagnosis, object recognition, and autonomous driving. With the increasing availability of digital images, the need for accurate and efficient image classification has become more important than ever.\nMachine learning has revolutionised the field of image classification by enabling the development of automated classification systems that can learn from data. Machine learning algorithms can automatically learn the visual features that distinguish between different classes of images, and then use this knowledge to classify new images. With the development of deep learning algorithms, such as Convolutional Neural Networks (CNNs), image classification has achieved unprecedented levels of accuracy and performance.\nR also has a number of packages that can be used for image classification. These packages provide a range of machine learning and deep learning algorithms, as well as tools for image processing and feature extraction. In this table, we summarise some of the most popular R packages that can be used for image classification, along with their key features.",
    "crumbs": [
      "Image Processing",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Introduction to image processing</span>"
    ]
  },
  {
    "objectID": "intro-img-proc.html#am-r-example",
    "href": "intro-img-proc.html#am-r-example",
    "title": "25¬† Introduction to image processing",
    "section": "\n25.2 Am R Example",
    "text": "25.2 Am R Example\nThe MNIST dataset is a popular benchmark dataset for image classification tasks in computer vision. It consists of a large set of 70,000 grayscale images of handwritten digits (0-9), each of size 28x28 pixels. The images are split into a training set of 60,000 images and a test set of 10,000 images.\nThe MNIST dataset is considered a relatively easy dataset, with high-quality images and well-defined classes. However, it remains a challenging dataset for some machine learning models, especially those with limited capacity or prone to overfitting. As a result, the MNIST dataset has become a popular benchmark for evaluating the effectiveness of different data preprocessing techniques, regularisation methods, and model architectures.\nIn this example, we will use the dataset_mnist() function from TensorFlow to load the MNIST dataset.\n\nlibrary(keras)\nlibrary(tensorflow)\n# Load the data\nmnist &lt;- dataset_mnist()\nx_train &lt;- mnist$train$x / 255\ny_train &lt;- mnist$train$y\nx_test &lt;- mnist$test$x / 255\ny_test &lt;- mnist$test$y\n\n# Reshape the data\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))\nx_test &lt;- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))\n\n# One-hot encode the labels\ny_train &lt;- to_categorical(y_train, num_classes = 10)\ny_test &lt;- to_categorical(y_test, num_classes = 10)\n\n# Convert labels to categorical\nlabels &lt;- to_categorical(y_train, num_classes = 10)\n\n# Display the images\npar(mfcol=c(6,6))\npar(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')\nfor (idx in 1:36) { \n    im &lt;- x_train[idx,,,1]\n    im &lt;- t(apply(im, 2, rev)) \n    image(1:28, 1:28, im, col=gray((0:255)/255), \n          xaxt='n', main=paste(y_train[idx]))\n}\n\n\n\n\n\n\n\nData augmentation Data augmentation is a technique commonly used in image processing to increase the size of the training dataset by generating new, slightly modified versions of the original images. The goal of data augmentation is to reduce overfitting and improve the generalisation performance of machine learning models.\nData augmentation can be applied to images in many ways, including rotating, scaling, cropping, flipping, translating, or adding noise to the original images. By applying these transformations to the original images, the machine learning model is exposed to a larger set of training data that includes variations in lighting conditions, object orientation, and object position. As a result, the model becomes more robust to variations in the test data, which improves its ability to generalise to new, unseen examples.\nWe will use data augmentation techniques to generate additional training data and reduce overfitting. In this example, we will perform random rotations, random zooms, and random crops.\n\n# Set up the data augmentation generator:\ndatagen &lt;- image_data_generator(\n  rotation_range = 15,\n  zoom_range = 0.1,\n  width_shift_range = 0.1,\n  height_shift_range = 0.1\n)\ndatagen %&gt;% fit_image_data_generator(x_train)\n\nWe will use a simple Convolutional Neural Network (CNN) with 2 convolutional layers and 2 fully connected layers.\n\n# Build the model\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\", input_shape = c(28, 28, 1)) %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(units = 128, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %&gt;% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n\nTrain the model with data augmentation (this will take about 20min):\n\nbatch_size &lt;- 32\nepochs &lt;- 1\n\nhistory &lt;- model %&gt;% fit(\n  flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),\n  steps_per_epoch = as.integer(nrow(x_train) / batch_size),\n  epochs = epochs,\n  validation_data = list(x_test, y_test),\n  verbose = 1\n)\n\nPreprocess the test dataset (assuming it is not preprocessed yet):\n\nx_test &lt;- mnist$test$x / 255\ny_test &lt;- mnist$test$y\n\n# Reshape the data\nx_test &lt;- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))\n\n# One-hot encode the labels\ny_test_categorical &lt;- to_categorical(y_test, num_classes = 10)\n\nPerform predictions on the test dataset:\n\npredictions &lt;- model %&gt;% predict(x_test)\npredicted_labels &lt;- apply(predictions, 1, which.max) - 1\naccuracy &lt;- mean(predicted_labels == y_test)\nprint(paste(\"Test accuracy:\", accuracy))\n\nTest accuracy: 0.9876\nA random image from the test sample:\n\n\n\n\n\n\n\n\nIn this text classification example, we have successfully demonstrated the process of loading, preprocessing, and augmenting the MNIST dataset using R, Keras, and TensorFlow. We have implemented a simple Convolutional Neural Network (CNN) model for classifying handwritten digits from the MNIST dataset.\nWe utilised data augmentation techniques, such as random rotations, zooms, and translations, to improve the model‚Äôs performance on unseen data by generating additional training samples. This helps in mitigating overfitting and increasing the model‚Äôs generalisation capabilities.\nAfter training the model, we performed predictions on the test dataset and evaluated its accuracy. We also visualised some of the predictions to gain insights into the model‚Äôs performance.",
    "crumbs": [
      "Image Processing",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Introduction to image processing</span>"
    ]
  },
  {
    "objectID": "img-seg.html",
    "href": "img-seg.html",
    "title": "26¬† Image segmentation and object detection",
    "section": "",
    "text": "26.1 Overview\nImage segmentation is the process of dividing an image into multiple segments, or regions, each of which corresponds to a distinct object or part of the image. The goal of image segmentation is to simplify the representation of an image into meaningful and easy-to-understand parts, which can then be used for further analysis or manipulation.\nThere are several techniques for image segmentation, including thresholding, edge detection, region growing, and clustering.\nImage segmentation has many applications, such as object recognition, image editing, and medical imaging. For example, in object recognition, image segmentation can be used to identify the boundaries of objects in an image and extract features for use in a machine learning model. In image editing, image segmentation can be used to remove or replace specific objects or parts of an image. In medical imaging, image segmentation can be used to identify and segment different tissues or structures in an image, which can be used for diagnosis or treatment planning.\nFor efficient image segmentation with machine learning, Python with TensorFlow is recommended, and interested readers may find the reference page on Image segmentation with a U-Net-like architecture useful.\nObject detection\nObject detection based on segmentation is a technique that combines object detection and image segmentation to identify and localise objects in an image or video. This approach involves first segmenting the image to obtain a set of candidate regions that potentially contain objects. Each candidate region is then analysed to determine whether it contains an object, and if so, to classify the object.\nThere are several methods for object detection based on segmentation, including region-based convolutional neural networks (R-CNN), Mask R-CNN, and instance segmentation.\nRegion-based convolutional neural networks (R-CNN) first segment the image into regions of interest using a segmentation algorithm such as selective search. These regions are then passed through a convolutional neural network (CNN) to extract features, followed by a classifier to classify the objects contained within the regions.\nObject detection based on segmentation has numerous applications in various fields, including robotics, self-driving cars, and medical imaging. For example, in robotics, object detection based on segmentation can be used to identify and locate objects for manipulation or navigation tasks. In self-driving cars, object detection based on segmentation can be used to detect and track other vehicles, pedestrians, and traffic signs. In medical imaging, object detection based on segmentation can be used to detect and locate tumours or other abnormalities in medical scans.\nObject detection based on segmentation is a powerful technique that can accurately and efficiently detect and locate objects in images or videos, with numerous applications in various fields.\nPerforming object detection with region-based convolutional neural networks (R-CNN) in R typically involves using pre-trained models and specialized packages. One popular package for object detection in R that allows it is reticulate, which allows you to interface with Python libraries. Here‚Äôs an example of how you can use R to perform object detection using R-CNN:\n# Install and load the required packages\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# Load the Python libraries\ncv2 &lt;- import(\"cv2\")\nnumpy &lt;- import(\"numpy\")\ntensorflow &lt;- import(\"tensorflow\")\ntf &lt;- tensorflow$compat$v1\n\n# Load the pre-trained R-CNN model\nrcnn_model &lt;- tf$keras$models$load_model(\"path/to/rcnn_model.h5\")\n\n# Load and preprocess the image\nimage &lt;- cv2$imread(\"path/to/image.jpg\")\nimage &lt;- cv2$cvtColor(image, cv2$COLOR_BGR2RGB)\nimage &lt;- numpy$array(image, dtype=\"float32\")\nimage &lt;- numpy$expand_dims(image, axis=0)\n\n# Perform object detection\npreds &lt;- rcnn_model$predict(image)\nboxes &lt;- preds$reshape(-1, 4)\nscores &lt;- preds$reshape(-1)\n\n# Set the confidence threshold\nconfidence_threshold &lt;- 0.5\n\n# Filter the detected objects based on confidence threshold\nfiltered_boxes &lt;- boxes[scores &gt; confidence_threshold, ]\nfiltered_scores &lt;- scores[scores &gt; confidence_threshold]\n\n# Display the detected objects\nfor (i in 1:nrow(filtered_boxes)) {\n  box &lt;- filtered_boxes[i, ]\n  cv2$rectangle(image, tuple(box[1:2]), tuple(box[3:4]), (0, 255, 0), 2)\n}\n\n# Show the image with detected objects\ncv2$imshow(\"Object Detection\", image)\ncv2$waitKey(0)\ncv2$destroyAllWindows()",
    "crumbs": [
      "Image Processing",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Image segmentation and object detection</span>"
    ]
  },
  {
    "objectID": "img-seg.html#overview",
    "href": "img-seg.html#overview",
    "title": "26¬† Image segmentation and object detection",
    "section": "",
    "text": "Thresholding involves dividing the image into two or more parts based on a particular threshold value, such as pixel intensity or colour.\nEdge detection involves identifying the edges of objects in the image based on changes in intensity or colour.\nRegion growing involves grouping together pixels that have similar properties, such as intensity or texture.\nClustering involves grouping pixels based on similarity in features, such as colour, texture, or shape.",
    "crumbs": [
      "Image Processing",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Image segmentation and object detection</span>"
    ]
  }
]