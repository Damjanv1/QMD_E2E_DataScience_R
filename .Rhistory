# Create a sparse matrix of the categories
sparse_matrix <- sparse.model.matrix(~ category - 1, data = sparse_categories)
# Convert the sparse matrix to a regular matrix
sparse_encoded <- as.data.frame(as.matrix(sparse_matrix))
# Print the sparse encoded features
print(sparse_encoded)
# Create a sample data frame
data <- data.frame(
feature1 = c("A", "B", "C"),
feature2 = c("X", "Y", "Z"),
label = c(1, 0, 1)
)
# Create interaction feature
data$interaction_feature <- interaction(data$feature1, data$feature2, drop = TRUE)
# Print the data frame with the interaction feature
print(data)
# Load required package
library(dplyr)
# Create a sample data frame with missing values
data <- data.frame(
feature1 = c(1, 2, NA, 4),
feature2 = c("A", NA, "C", "D"),
feature3 = c(5, 6, 7, NA),
label = c(1, 0, 1, 0)
)
# Removing rows with missing values
data_without_missing_rows <- na.omit(data)
# Removing columns with too many missing values
data_without_missing_cols <- data %>%
select_if(function(x) sum(is.na(x)) < 0.5 * nrow(data))
# Imputation using mean, median, or mode
imputed_data <- data %>%
mutate(
feature1_imputed = ifelse(is.na(feature1), mean(feature1, na.rm = TRUE), feature1),
feature2_imputed = ifelse(is.na(feature2), median(feature2, na.rm = TRUE), feature2),
feature3_imputed = ifelse(is.na(feature3), as.character(mode(feature3, na.rm = TRUE)), feature3)
)
# Load required package
library(dplyr)
# Create a sample data frame with missing values
data <- data.frame(
feature1 = c(1, 2, NA, 4),
feature2 = c("A", NA, "C", "D"),
feature3 = c(5, 6, 7, NA),
label = c(1, 0, 1, 0)
)
# Removing rows with missing values
data_without_missing_rows <- na.omit(data)
# Removing columns with too many missing values
data_without_missing_cols <- data %>%
select_if(function(x) sum(is.na(x)) < 0.5 * nrow(data))
# Imputation using mean, median, or mode
imputed_data <- data %>%
mutate(
feature1_imputed = ifelse(is.na(feature1), mean(feature1, na.rm = TRUE), feature1),
feature2_imputed = ifelse(is.na(feature2), median(feature2, na.rm = TRUE), feature2),
feature3_imputed = ifelse(is.na(feature3), as.character(mode(feature3)), feature3)
)
# Imputation using model-based approach
library(mice)
install.packages('mice')
# Load required package
library(dplyr)
# Create a sample data frame with missing values
data <- data.frame(
feature1 = c(1, 2, NA, 4),
feature2 = c("A", NA, "C", "D"),
feature3 = c(5, 6, 7, NA),
label = c(1, 0, 1, 0)
)
# Removing rows with missing values
data_without_missing_rows <- na.omit(data)
# Removing columns with too many missing values
data_without_missing_cols <- data %>%
select_if(function(x) sum(is.na(x)) < 0.5 * nrow(data))
# Imputation using mean, median, or mode
imputed_data <- data %>%
mutate(
feature1_imputed = ifelse(is.na(feature1), mean(feature1, na.rm = TRUE), feature1),
feature2_imputed = ifelse(is.na(feature2), median(feature2, na.rm = TRUE), feature2),
feature3_imputed = ifelse(is.na(feature3), as.character(mode(feature3)), feature3)
)
# Imputation using model-based approach
library(mice)
imputed_data_model <- complete(mice(data))
# Creating a dummy variable for missing values
data_with_dummy <- data %>%
mutate(
feature1_missing = ifelse(is.na(feature1), 1, 0),
feature2_missing = ifelse(is.na(feature2), 1, 0),
feature3_missing = ifelse(is.na(feature3), 1, 0)
)
# Learning algorithms handling missing values
library(randomForest)
install.packages('randomForest')
# Load required package
library(dplyr)
# Create a sample data frame with missing values
data <- data.frame(
feature1 = c(1, 2, NA, 4),
feature2 = c("A", NA, "C", "D"),
feature3 = c(5, 6, 7, NA),
label = c(1, 0, 1, 0)
)
# Removing rows with missing values
data_without_missing_rows <- na.omit(data)
# Removing columns with too many missing values
data_without_missing_cols <- data %>%
select_if(function(x) sum(is.na(x)) < 0.5 * nrow(data))
# Imputation using mean, median, or mode
imputed_data <- data %>%
mutate(
feature1_imputed = ifelse(is.na(feature1), mean(feature1, na.rm = TRUE), feature1),
feature2_imputed = ifelse(is.na(feature2), median(feature2, na.rm = TRUE), feature2),
feature3_imputed = ifelse(is.na(feature3), as.character(mode(feature3)), feature3)
)
# Imputation using model-based approach
library(mice)
imputed_data_model <- complete(mice(data))
# Creating a dummy variable for missing values
data_with_dummy <- data %>%
mutate(
feature1_missing = ifelse(is.na(feature1), 1, 0),
feature2_missing = ifelse(is.na(feature2), 1, 0),
feature3_missing = ifelse(is.na(feature3), 1, 0)
)
# Learning algorithms handling missing values
library(randomForest)
rf_model <- randomForest(label ~ ., data = data, na.action = na.exclude)
# Print the modified data frames
print(data_without_missing_rows)
print(data_without_missing_cols)
print(imputed_data)
print(imputed_data_model)
print(data_with_dummy)
# Load required packages
library(dplyr)
library(car)
install.packages('car')
# Load required packages
library(dplyr)
library(car)
library(MASS)
# Create a sample data frame with outliers
data <- data.frame(
variable = c(1, 2, 3, 100, 5, 6, 200),
label = c(1, 0, 1, 0, 1, 0, 1)
)
# Avoid deleting outliers
# Outliers may contain valuable information, so we don't remove them
# Fix errors or treat outliers as missing values
data$variable_fixed <- ifelse(data$variable > 100, NA, data$variable)
# Censoring outliers
threshold <- 100
data$variable_censored <- ifelse(data$variable > threshold, threshold, data$variable)
# Transforming the variable
data$variable_log <- log(data$variable)
# Creating a dummy variable to indicate outliers
data$outlier_dummy <- ifelse(data$variable > 100, 1, 0)
# Using a robust learning algorithm
model <- rlms(formula = label ~ variable, data = data, method = "MM")
# Load required packages
library(dplyr)
library(car)
library(MASS)
# Create a sample data frame with outliers
data <- data.frame(
variable = c(1, 2, 3, 100, 5, 6, 200),
label = c(1, 0, 1, 0, 1, 0, 1)
)
# Avoid deleting outliers
# Outliers may contain valuable information, so we don't remove them
# Fix errors or treat outliers as missing values
data$variable_fixed <- ifelse(data$variable > 100, NA, data$variable)
# Censoring outliers
threshold <- 100
data$variable_censored <- ifelse(data$variable > threshold, threshold, data$variable)
# Transforming the variable
data$variable_log <- log(data$variable)
# Creating a dummy variable to indicate outliers
data$outlier_dummy <- ifelse(data$variable > 100, 1, 0)
# Using a robust learning algorithm
model <- rlm(formula = label ~ variable, data = data, method = "MM")
# Print the modified data frame
print(data)
library(ggplot2)
library(plotly)
# Create a scatter plot
p <- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +
geom_point() +
labs(title = "Scatter Plot of Sepal Length vs. Sepal Width",
x = "Sepal Length", y = "Sepal Width")
plotly::ggplotly(p)
p <- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
geom_point() +
labs(title = "Scatter Plot with Colored Points",
x = "Sepal Length", y = "Sepal Width")
plotly::ggplotly(p)
p <- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +
geom_point() +
labs(title = "Scatter Plot with Adjusted Axis Limits",
x = "Sepal Length", y = "Sepal Width") +
xlim(4, 8) +
ylim(2, 4.5)
ggplotly(p)
p <- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +
geom_point() +
labs(title = "Scatter Plot with Grouping and Faceting",
x = "Sepal Length", y = "Sepal Width") +
facet_wrap(~ Species)
ggplotly(p)
df1 <- data.frame(x = 1:10, y = 1:10)
df2 <- data.frame(x = 1:10, y = 10:1)
p <- ggplot() +
geom_point(data = df1, aes(x, y)) +
geom_line(data = df2, aes(x, y)) +
labs(title = "Overlaying Multiple Data Sources")
ggplotly(p)
# Load the ggplot2 library
library(ggplot2)
# Create a dataset
data <- data.frame(
Group = rep(c("A", "B", "C"), each = 100),
Value = rnorm(300)
)
# Create a violin plot
p <- ggplot(data, aes(x = Group, y = Value)) +
geom_violin(fill = "#FF6666", color = "#990000", alpha = 0.8) +
theme_minimal()  # Apply a minimal theme
# Display the plot
ggplotly(p)
library(ggplot2)
library(plotly)
n <- 100
x1 <- rnorm(n); x2 <- rnorm(n)
y1 <- 2 * x1 + rnorm(n)
y2 <- 3 * x2 + (2 + rnorm(n))
A <- as.factor(rep(c(1, 2), each = n))
df <- data.frame(x = c(x1, x2), y = c(y1, y2), A = A)
fm <- lm(y ~ x + A, data = df)
p <- ggplot(data = cbind(df, pred = predict(fm)), aes(x = x, y = y, color = A))
p <- p + geom_point() + geom_line(aes(y = pred))
ggplotly(p)
library(ggplot2)
library(plotly)
n <- 1000
x1 <- rnorm(n)
y1 <- 3 * x2 + (2 + rnorm(n))
df <- data.frame(x1, y1)
fm <- lm(y1 ~ x1, data = df)
p <- ggplot(data = cbind(df, pred = predict(fm)), aes(x = x1, y = y1)
p <- p + geom_point() + geom_line(aes(y = pred))
library(ggplot2)
library(plotly)
n <- 1000
x1 <- rnorm(n)
y1 <- 3 * x2 + (2 + rnorm(n))
df <- data.frame(x1, y1)
fm <- lm(y1 ~ x1, data = df)
p <- ggplot(data = cbind(df, pred = predict(fm)), aes(x = x1, y = y1))
p <- p + geom_point() + geom_line(aes(y = pred))
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_smooth(method = "lm", se = FALSE)
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["x1"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red')
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["cyl"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red')
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = mpg, y = hp)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["mpg"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red')
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ cyl, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = cyl, y = mpg)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["cyl"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red')
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ wt, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = wt, y = mpg)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["wt"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red')
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ wt, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = wt, y = mpg)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["wt"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red') +
labs(title="Miles per gallon \n according to the weight",
x="Weight (lb/1000)", y = "Miles/(US) gallon")
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ wt, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = wt, y = mpg)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["wt"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red') +
labs(title="Miles per gallon",
x="Weight (lb/1000)", y = "Miles/(US) gallon")
ggplotly(p)
library(ggplot2)
library(plotly)
# read dataset
df = mtcars
# create multiple linear model
lm_fit <- lm(mpg ~ wt, data=df)
summary(lm_fit)
# this is predicted line comparing only chosen variables
p <- ggplot(data = df, aes(x = wt, y = mpg)) +
geom_point(color='blue') +
geom_abline(slope = coef(lm_fit)[["wt"]],
intercept = coef(lm_fit)[["(Intercept)"]],
color = 'red') +
labs(title="Miles per gallon",
x="Weight (lb/1000)", y = "Miles/(US) gallon") + theme_linedraw()
ggplotly(p)
# Create a sample dataset
x <- seq(1, 10, by = 0.5)
y <- sin(x) + rnorm(length(x), mean = 0, sd = 0.3)
data <- data.frame(x, y)
# Define a function for KNN regression
knn_regression <- function(train_data, test_point, k) {
distances <- sqrt((train_data$x - test_point$x)^2)
sorted_indices <- order(distances)
neighbors <- train_data$y[sorted_indices[1:k]]
predicted_value <- mean(neighbors)
return(predicted_value)
}
# Predict the value using KNN regression
test_point <- data.frame(x = 8.5)
k <- 3
predicted_value <- knn_regression(data, test_point, k)
# Print the predicted value
print(predicted_value)
mtmod <- glm(am ~ hp + wt, data = mtcars, family="binomial")
head(predict(mtmod, mtcars))
head(predict(mtmod, mtcars, type = "response"))
logmod <- glm(am ~ mpg, data = mtcars, family = binomial)
summary(logmod)
library(ggplot2)
library(plotly)
# Load mtcars dataset
data(mtcars)
p <- ggplot(mtcars, aes(mpg, am)) +
geom_point() +
geom_smooth(method = "lm", se = F)
ggplotly(p)
p <- ggplot(mtcars, aes(mpg, am)) +
geom_point() +
geom_smooth(method = "loess", se = F)
ggplotly(p)
summary(glm(am ~ hp + wt, data = mtcars, family="binomial"))
mtmod <- glm(am ~ hp + wt, data = mtcars, family="binomial")
head(predict(mtmod, mtcars))
head(predict(mtmod, mtcars, type = "response"))
#| echo: false
source("_common.R")
# Load required package
library(dplyr)
# Load iris dataset
data(iris)
# Select numerical features
numerical_features <- iris %>% select_if(is.numeric)
# Perform feature scaling using the scale() function
scaled_features <- scale(numerical_features)
# Convert scaled features back to a data frame
scaled_df <- as.data.frame(scaled_features)
# Print the first few rows of the scaled dataset
head(scaled_df)
# Load required packages
library(dplyr)
library(caret)
library(Matrix)
# Create a sample data frame
dataf <- data.frame(
category = c("A", "B", "C", "A", "B", "D", "C"),
label = c(1, 0, 1, 0, 1, 0, 1)
)
# One-hot encoding using caret package
dmy <- caret::dummyVars(" ~ .", data = dataf)
one_hot_encoded <- data.frame(predict(dmy, newdata = dataf))
print(one_hot_encoded)
# Dummy encoding using dplyr package
dummy_encoded <- data %>%
mutate(category = as.factor(category)) %>%
bind_cols(model.matrix(~ category - 1, data = .))
# Load required packages
library(dplyr)
library(caret)
library(Matrix)
# Create a sample data frame
dataf <- data.frame(
category = c("A", "B", "C", "A", "B", "D", "C"),
label = c(1, 0, 1, 0, 1, 0, 1)
)
# One-hot encoding using caret package
dmy <- caret::dummyVars(" ~ .", data = dataf)
one_hot_encoded <- data.frame(predict(dmy, newdata = dataf))
print(one_hot_encoded)
# Dummy encoding using dplyr package
dummy_encoded <- dataf %>%
mutate(category = as.factor(category)) %>%
bind_cols(model.matrix(~ category - 1, data = .))
# Print the dummy encoded features
print(dummy_encoded)
# Handling sparse categories using Matrix package
sparse_categories <- data %>%
mutate(category = as.factor(category)) %>%
mutate(category = as.character(category))
# Load required packages
library(dplyr)
library(caret)
library(Matrix)
# Create a sample data frame
dataf <- data.frame(
category = c("A", "B", "C", "A", "B", "D", "C"),
label = c(1, 0, 1, 0, 1, 0, 1)
)
# One-hot encoding using caret package
dmy <- caret::dummyVars(" ~ .", data = dataf)
one_hot_encoded <- data.frame(predict(dmy, newdata = dataf))
print(one_hot_encoded)
# Dummy encoding using dplyr package
dummy_encoded <- dataf %>%
mutate(category = as.factor(category)) %>%
bind_cols(model.matrix(~ category - 1, data = .))
# Print the dummy encoded features
print(dummy_encoded)
# Handling sparse categories using Matrix package
sparse_categories <- dataf %>%
mutate(category = as.factor(category)) %>%
mutate(category = as.character(category))
# Create a sparse matrix of the categories
sparse_matrix <- sparse.model.matrix(~ category - 1, data = sparse_categories)
# Convert the sparse matrix to a regular matrix
sparse_encoded <- as.data.frame(as.matrix(sparse_matrix))
# Print the sparse encoded features
print(sparse_encoded)
