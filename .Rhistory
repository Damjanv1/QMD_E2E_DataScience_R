library(ggplot2)
library(plotly)
rf <- randomForest(Species ~ Petal.Length + Petal.Width, data = iris,
proximity = TRUE)
rf_df <- expand.grid(Petal.Width = seq(0, 3, length.out = 100),
Petal.Length = seq(0, 7, length.out = 100))
rf_df$Species <- predict(rf, rf_df)
p <- ggplot(iris, aes(Petal.Width, Petal.Length, fill = Species)) +
geom_raster(data = df, alpha = 0.5) +
geom_point(shape = 21, size = 3) +
theme_minimal()
ggplotly(p)
rf_df
library(randomForest)
library(ggplot2)
library(plotly)
rf <- randomForest(Species ~ Petal.Length + Petal.Width, data = iris,
proximity = TRUE)
rf_df <- expand.grid(Petal.Width = seq(0, 3, length.out = 100),
Petal.Length = seq(0, 7, length.out = 100))
rf_df$Species <- predict(rf, rf_df)
p <- ggplot(iris, aes(Petal.Width, Petal.Length, fill = Species)) +
geom_raster(data = rf_df, alpha = 0.5) +
geom_point(shape = 21, size = 3) +
theme_minimal()
ggplotly(p)
library(randomForest)
library(ggplot2)
library(plotly)
rf <- randomForest(Species ~ Petal.Length + Petal.Width, data = iris,
proximity = TRUE)
rf_df <- expand.grid(Petal.Width = seq(0, 3, length.out = 100),
Petal.Length = seq(0, 7, length.out = 100))
rf_df$Species <- predict(rf, rf_df)
p <- ggplot(iris, aes(Petal.Width, Petal.Length, fill = Species)) +
geom_raster(data = rf_df, alpha = 0.5) +
geom_point(shape = 21, size = 3) +
theme_minimal()
p
library(adabag)
#Load the iris dataset and split it into training and testing sets:
data(iris)
set.seed(42)
indices <- sample(1:nrow(iris), size = 0.7 * nrow(iris))
train_set <- iris[indices,]
test_set <- iris[-indices,]
#Train the AdaBoost model:
iris_boost <- boosting(Species ~ ., data = train_set, boos = TRUE, mfinal = 100)
#Make predictions using the trained model:
predictions <- predict(iris_boost, newdata = test_set)
#Evaluate the model's performance:
confusion_matrix <- table(predictions$class, test_set$Species)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)
install.packages(c('NeuralNetTools', 'nnet', 'deepnet'))
library(nnet)
library(deepnet)
library(NeuralNetTools)
# Load data
data(iris)
# Normalize the numeric features
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
iris[,1:4] <- lapply(iris[,1:4], normalize)
# Convert the target variable into a factor
iris$Species <- as.factor(iris$Species)
set.seed(42)
# Define the neural network architecture
nn <- nnet(
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
data = iris,
size = 5,
act.fct = "logistic",
linout = FALSE,
maxit = 1e+05
)
# Visualise
plotnet(nn)
install.packages('keras')
#Install and load the required libraries:
library(keras)
#Load the iris dataset and pre-process it:
data(iris)
# Normalize the numeric features
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
iris[,1:4] <- lapply(iris[,1:4], normalize)
# Convert the target variable into a one-hot encoded matrix
iris$Species <- as.factor(iris$Species)
y <- to_categorical(as.numeric(iris$Species) - 1)
X <- as.matrix(iris[, 1:4])
# Set the seed for reproducibility
set.seed(42)
# Define the L2 regularization term
l2_regularizer <- regularizer_l2(l = 0.01)
# Define the neural network architecture
#
model <- keras_model_sequential() %>%
layer_dense(units = 5, activation = "relu", input_shape = ncol(X),
kernel_regularizer = l2_regularizer) %>%
layer_dense(units = 3, activation = "softmax", kernel_regularizer = l2_regularizer)
# Compile the model
model %>% compile(
optimizer = "adam",
loss = "categorical_crossentropy",
metrics = "accuracy"
)
# Train the model
history <- model %>% fit(
X, y,
epochs = 100,
batch_size = 16,
validation_split = 0.2
)
#Plot the training and validation accuracy:
plot(history)
library(tokenizers)
install.packages('tokenizers')
library(tokenizers)
#let's create a simple text data example to tokenize:
text <- "Natural Language Processing with R is an exciting journey!"
#Now, we will tokenize the text at the word level using the tokenize_words() function:
word_tokens <- tokenize_words(text)
print(word_tokens)
library(tidytext)
install.packages('tidytext')
library(tidytext)
library(dplyr)
#let's create a simple text data example with some common stop words:
text <- "The quick brown fox jumps over the lazy dog."
#Now, we will tokenize the text into words using the unnest_tokens() function from the tidytext package:
text_df <- tibble(line = 1, text = text) %>%
unnest_tokens(word, text)
print(text_df)
text_no_stop_words <- text_df %>%
anti_join(stop_words)
print(text_no_stop_words)
text_df <- tibble(line = 1, text = text) %>%
unnest_tokens(word, text)
word_frequencies <- text_df %>%
count(word, sort = TRUE)
print(word_frequencies)
library(keras)
imdb <- dataset_imdb(num_words = 10000) #will take around 30 seconds to download
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
# Pad sequences to the same length
maxlen <- 500
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
vocab_size <- 10000
embedding_dim <- 50
embedding_layer <- layer_embedding(input_dim = vocab_size,
output_dim = embedding_dim,
input_length = maxlen)
model <- keras_model_sequential() %>%
embedding_layer %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "adam",
loss = "binary_crossentropy",
metrics = c("accuracy"))
history <- model %>% fit(x_train, y_train,
epochs = 10,
batch_size = 32,
validation_split = 0.2)
library(tsne)
install.packages('tsne')
library(tsne)
library(tidyverse)
install.packages('tidyverse')
library(tsne)
library(tidyverse)
word_embeddings <- get_weights(model)[[1]]
word_index <- imdb$word_index
index_to_word <- names(word_index)
names(index_to_word) <- word_index
words <- index_to_word[1:1000]
embeddings <- word_embeddings[1:1000, ]
library(Rtsne)
library(plotly)
# Run t-SNE on the embeddings
tsne_model <- Rtsne(embeddings, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)
#Create a data frame with the t-SNE results and the corresponding words:
tsne_data <- data.frame(tsne_model$Y)
colnames(tsne_data) <- c("X", "Y")
tsne_data$word <- words
#Visualize the t-SNE results using ggplot2:
p <- ggplot(tsne_data, aes(x = X, y = Y)) +
geom_text(aes(label = word), size = 3, alpha = 0.7, vjust = 1, hjust = 1, angle = 45) +
theme_minimal() +
labs(title = "t-SNE Visualization of Word Embeddings",
x = "t-SNE Dimension 1",
y = "t-SNE Dimension 2")
ggplotly(p)
library(Rtsne)
library(plotly)
# Run t-SNE on the embeddings
tsne_model <- Rtsne(embeddings, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)
#Create a data frame with the t-SNE results and the corresponding words:
tsne_data <- data.frame(tsne_model$Y)
colnames(tsne_data) <- c("X", "Y")
tsne_data$word <- words
#Visualize the t-SNE results using ggplot2:
p <- ggplot(tsne_data, aes(x = X, y = Y)) +
geom_text(aes(label = word), size = 3, alpha = 0.7, vjust = 1, hjust = 1, angle = 45) +
theme_minimal() +
labs(title = "t-SNE Visualization of Word Embeddings",
x = "t-SNE Dimension 1",
y = "t-SNE Dimension 2")
p
tsne_data
words
index_to_word
imdb$train
library(tsne)
library(tidyverse)
word_embeddings <- get_weights(model)[[1]]
#Load the IMDB dataset:
imdb <- dataset_imdb(num_words = 5000)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
# word_index is a named list mapping words to an integer index
word_index <- dataset_imdb_word_index()
index_to_word <- names(word_index)
names(index_to_word) <- word_index
words <- index_to_word[1:1000]
embeddings <- word_embeddings[1:1000, ]
library(Rtsne)
library(plotly)
# Run t-SNE on the embeddings
tsne_model <- Rtsne(embeddings, dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)
#Create a data frame with the t-SNE results and the corresponding words:
tsne_data <- data.frame(tsne_model$Y)
colnames(tsne_data) <- c("X", "Y")
tsne_data$word <- words
#Visualize the t-SNE results using ggplot2:
p <- ggplot(tsne_data, aes(x = X, y = Y)) +
geom_text(aes(label = word), size = 3, alpha = 0.7, vjust = 1, hjust = 1, angle = 45) +
theme_minimal() +
labs(title = "t-SNE Visualization of Word Embeddings",
x = "t-SNE Dimension 1",
y = "t-SNE Dimension 2")
ggplotly(p)
install.packages("text")
# Install and load the required packages
library(text)
# Load the pre-trained Word2Vec model
word2vec_model <- textEmbed("path/to/word2vec_model.bin")
library(text2vec)
install.packages('text2vec')
library(text2vec)
install.packages('text2vec')
install.packages('text2vec')
install.packages('text2vec')
install.packages("text2vec", dependencies = FALSE)
'? this film was just brilliant casting location scenery story direction everyone\'s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don\'t you think the whole story was so lovely because it was true and was someone\'s life after all that was ? with us all'
vectorize_sequences <- function(sequences, dimension = 5000) {
# Initialize a matrix with all zeroes
results <- matrix(0, nrow = length(sequences), ncol = dimension)
# Replace 0 with a 1 for each column of the matrix given in the list
for (i in 1:length(sequences))
results[i, sequences[[i]]] <- 1
results
}
x_train <- vectorize_sequences(x_train)
x_test <- vectorize_sequences(x_test)
y_train <- as.numeric(y_train)
y_test <- as.numeric(y_test)
str(x_train[1,])
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = c(5000)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("accuracy")
)
#prepare the validation dataset
val_indices <- 1:5000
x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]
y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
#fit the model
history <- model %>% fit(
partial_x_train,
partial_y_train,
epochs = 20,
batch_size = 512,
validation_data = list(x_val, y_val)
)
plot(history)
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = c(5000)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("accuracy")
)
model %>% fit(x_train, y_train, epochs = 5, batch_size = 512)
results <- model %>% evaluate(x_test, y_test)
results
imdb <- dataset_imdb(num_words = 5000)
test_text <- imdb$test$x
decoded_review <- sapply(test_text[[1]], function(index) {
word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
if (!is.null(word)) word else "?"
})
#Load the IMDB dataset:
imdb <- dataset_imdb(num_words = 5000)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
# word_index is a named list mapping words to an integer index
word_index <- dataset_imdb_word_index()
# Reverses it, mapping integer indices to words
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
imdb <- dataset_imdb(num_words = 5000)
test_text <- imdb$test$x
decoded_review <- sapply(test_text[[1]], function(index) {
word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
if (!is.null(word)) word else "?"
})
paste(decoded_review, collapse = " ")
if (c(model %>% predict(x_test[1:2,]))[1] > 0.5) {
cat('Positive')
} else {
cat('Negative')
}
library(keras)
library(tensorflow)
# Load the data
mnist <- dataset_mnist()
x_train <- mnist$train$x / 255
y_train <- mnist$train$y
x_test <- mnist$test$x / 255
y_test <- mnist$test$y
# Reshape the data
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
# One-hot encode the labels
y_train <- to_categorical(y_train, num_classes = 10)
y_test <- to_categorical(y_test, num_classes = 10)
# Convert labels to categorical
labels <- to_categorical(labels, num_classes = 10)
mnist$train
library(keras)
library(tensorflow)
# Load the data
mnist <- dataset_mnist()
x_train <- mnist$train$x / 255
y_train <- mnist$train$y
x_test <- mnist$test$x / 255
y_test <- mnist$test$y
# Reshape the data
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
# One-hot encode the labels
y_train <- to_categorical(y_train, num_classes = 10)
y_test <- to_categorical(y_test, num_classes = 10)
# Convert labels to categorical
labels <- to_categorical(y_train, num_classes = 10)
# Display the images
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) {
im <- x_train[idx,,,1]
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255),
xaxt='n', main=paste(y_train[idx]))
}
# Set up the data augmentation generator:
datagen <- image_data_generator(
rotation_range = 15,
zoom_range = 0.1,
width_shift_range = 0.1,
height_shift_range = 0.1
)
datagen %>% fit_image_data_generator(x_train)
# Build the model
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(lr = 0.001),
metrics = c("accuracy")
)
# Build the model
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
batch_size <- 32
epochs <- 1
history <- model %>% fit(
flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),
steps_per_epoch = as.integer(nrow(x_train) / batch_size),
epochs = epochs,
validation_data = list(x_test, y_test),
verbose = 1
)
#| label: pred-6
#| echo: false
#| out.width: NULL
knitr::include_graphics("images/pred_6.png", dpi = 150)
#| label: pred-6
#| echo: false
#| out.width: NULL
knitr::include_graphics("images/segmentation.png", dpi = 150)
# Install and load the required packages
install.packages("reticulate")
library(reticulate)
# Load the Python libraries
cv2 <- import("cv2")
#| label: t-sne1
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/t-sne.png", dpi = 170)
#| label: t-sne2
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/t-sne_2.png", dpi = 170)
#| label: t-sne2
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/t-sne_2.png", dpi = 170)
#| label: t-sne3
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/t-sne_3.png", dpi = 170)
#| label: t-sne4
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/t-sne_4.png", dpi = 170)
#| label: t-sne5
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/t-sne_5.png", dpi = 170)
#| label: t-sne1
#| echo: false
#| fig-align: 'center'
#| fig-cap: |
#| fig-alt: |
#| out.width: NULL
knitr::include_graphics("images/Screenshot 2024-05-13 at 9.31.17 pm.png", dpi = 170)
vectorize_sequences <- function(sequences, dimension = 5000) {
# Initialize a matrix with all zeroes
results <- matrix(0, nrow = length(sequences), ncol = dimension)
# Replace 0 with a 1 for each column of the matrix given in the list
for (i in 1:length(sequences))
results[i, sequences[[i]]] <- 1
results
}
imdb <- dataset_imdb(num_words = 5000)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
x_train <- vectorize_sequences(x_train)
x_test <- vectorize_sequences(x_test)
y_train <- as.numeric(y_train)
y_test <- as.numeric(y_test)
str(x_train[1,])
